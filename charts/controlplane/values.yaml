# ----------------------------------------------------------------------------
# Global Configuration
# ----------------------------------------------------------------------------
# Global values are accessible to both the chart and all subcharts.
# Override these in deployment-specific values files:
#   - values.aws.yaml: AWS-specific configuration (RDS, S3, IAM roles)
#   - values.aws.selfhosted-intracluster.yaml: Self-contained intra-cluster configuration
#
# Usage:
#   helm install -f values.aws.yaml [...]
#   helm install -f values.aws.selfhosted-intracluster.yaml [...]

global:

  # Control plane hostname (FQDN)
  # Example: "mycompany.union.ai" or "union.example.com"
  UNION_HOST: ""

  # AWS region for all resources
  # TODO: Will eventually be moved to values.aws.yaml
  # Example: "us-west-2", "us-east-1", "eu-west-1"
  AWS_REGION: ""

  # PostgreSQL RDS endpoint
  # Example: "mydb.abc123.us-west-2.rds.amazonaws.com"
  DB_HOST: ""

  # PostgreSQL database name
  # Example: "unionai" or "union_controlplane"
  DB_NAME: ""

  # PostgreSQL username
  # Example: "unionai" or "admin"
  DB_USER: ""

  # S3 bucket for control plane metadata
  # Example: "union-controlplane-metadata"
  BUCKET_NAME: ""

  # S3 bucket for artifacts storage
  # Example: "union-controlplane-artifacts"
  ARTIFACTS_BUCKET_NAME: ""

  # Name of Kubernetes secret containing the DB password and other service specific secrets.
  # The secret can be created and set through databaseSecret.secretManifest and dbPass below.
  # Example: "union-controlplane-secrets"
  # Note: Secret must contain "pass.txt" key
  KUBERNETES_SECRET_NAME: ""

  # Union Private ECR repository prefix for control plane images
  # Contact Union for controlplane access and distribution.
  IMAGE_REPOSITORY_PREFIX: "643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp"

# ----------------------------------------------------------------------------
# Additional Configuration
# ----------------------------------------------------------------------------
# Note: Most configuration now comes from globals above.
# Database host, name, user, buckets, and region are all configured via globals.

# It's recommended to create a Kubernetes secret and reference it via
# globals.KUBERNETES_SECRET_NAME rather than storing the password here
dbPass: ""
# -- Set the image used for control plane services
image:
  repository: "{{ .Values.global.IMAGE_REPOSITORY_PREFIX }}/services"
  pullPolicy: IfNotPresent
  tag: "{{ .Chart.AppVersion }}"

# -- Default config for Union services (excluding flyte subchart)
defaults:
  # Defaults to assume secrets and db credentials are stored in the same Kubernetes secret.
  #
  # Kubernetes secret names for service credentials
  secretName: "{{ .Values.global.KUBERNETES_SECRET_NAME }}"
  # Database secret name for database password
  dbSecretName: "{{ .Values.global.KUBERNETES_SECRET_NAME }}"

controlplane:
  enabled: true

replicaCount: 1
imagePullSecrets:
nameOverride: ""
# Configuring the fullnameOverride ensures that all deployed resources
# have predictable names that do not depend on the release name.
# This can be important in the context of self-hosted deployments
# where dataplanes interact through well-known service names. (e.g. ingress-nginx controller)
fullnameOverride: "controlplane"
serviceAccount:
  create: true
  annotations: {}
podAnnotations:
  linkerd.io/inject: disabled
  prometheus.io/path: /metrics
  prometheus.io/port: "10254"
service:
  type: ClusterIP
  grpcport: 80
  httpport: 81
  debugport: 82
  connectport: 83
resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 250m
    memory: 250Mi
env:
- name: GOMEMLIMIT
  valueFrom:
    resourceFieldRef:
      resource: limits.memory
      divisor: "1"
- name: GOMAXPROCS
  valueFrom:
    resourceFieldRef:
      resource: limits.cpu
      divisor: "1"
autoscaling:
  enabled: true
  minReplicas: 1
  maxReplicas: 1
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80
# Union autoscaling configuration.
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 1
    maxSurge: 1
secrets: {}
podMonitor:
  custom:
    enabled: false
    metricRelabelings: []
probe:
  enabled: false
serviceProfile:
  enabled: false
spreadConstraints:
  enabled: false
argo:
  enabled: false
externalSecrets:
  refreshInterval: 1h
  refs: {}
postgres-exporter:
  enabled: false
  fullnameOverride: postgres-exporter
  serviceMonitor:
    enabled: false
    labels:
      instance: union

ingress:

  # Enable Secret Service ingress routes
  secretService: true

  # Default to the ingress class being created in nginx-ingress chart.
  className: "controlplane"

  # TLS to apply to all ingress objects.
  # tls:

  # Host to use for all ingress objects.
  host:  "" # Set the DNS name of the ingress host used by you control plane

  # Whether to create separate ingress objects for gRPC and HTTP traffic.
  separateGrpcIngress: true

  # Universal ingress annotations.
  # Currently biased toward nginx-ingress controller
  annotations:
    # You can switch to v1 if needed using /console instead of /v2 which defaults to v2 unionconsole
    nginx.ingress.kubernetes.io/app-root: /v2
    nginx.ingress.kubernetes.io/service-upstream: "true"

    # Set RPS (requests per second) to 100, Burst is automatically computed to be 5x that number. If we decide to
    # bump rps, we should consider setting Burst separately through `limit-burst-multiplier`
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;

  separateGrpcIngressAnnotations:
    # Must annotate gRPC ingress with this to ensure proper handling of gRPC traffic.
    nginx.ingress.kubernetes.io/backend-protocol: "GRPC"
  protectedIngressAnnotations:
    nginx.org/websocket-services: dataproxy-service
  enableProtectedConsoleIngress: false
  protectedIngressAnnotationsWithoutSignin:
    nginx.org/websocket-services: dataproxy-service
  protectedConsoleIngressAnnotations:
    # Ensure this cache key resolves as non-empty to avoid an unintended hit on a header-less request.
    # Specifically, the cache key should include the configurable authorization header and cookie header.
    # Make sure to update the key if a different authorization header is used.
    nginx.ingress.kubernetes.io/auth-cache-key: $http_flyte_authorization$http_cookie
    nginx.org/websocket-services: dataproxy-service
  protectedIngressAnnotationsGrpc:

configMap:
  authorizer:
    type: "Noop"
    internalCommunicationConfig:
      enabled: false
  cache:
    identity:
      enabled: false
  connection:
    environment: "staging"
    region: '{{ .Values.global.AWS_REGION }}'
    rootTenantURLPattern: 'dns:///{{ .Values.global.UNION_HOST }}'
  logger:
    level: 6
  otel:  # Ref: https://github.com/flyteorg/flyte/blob/master/flytestdlib/otelutils/config.go
    type: noop
  union:
    internalConnectionConfig:
      enabled: true
      urlPattern: "_SERVICE_.{{ .Release.Namespace }}.svc.cluster.local:80"
    auth:
      enable: false

services:
  artifacts:

    # Artifacts service migrations conflict with Executions services. In a subsequent PR we will force database
    # migrations to create separate databases.
    disabled: true

    fullnameOverride: "artifacts"
    initContainers:
      - name: migrate
        args:
          - artifacts
          - migrate
          - --config
          - "/etc/config/*.yaml"
    args:
      - artifacts
      - serve
      - --config
      - /etc/config/*.yaml
    configMap:
      sharedService:
        metrics:
          scope: "artifacts:"
      db:
        dbname: '{{ .Values.global.DB_NAME }}'
        host: '{{ .Values.global.DB_HOST }}'
        options: sslmode=disable
        username: '{{ .Values.global.DB_USER }}'
        port: 5432
        passwordPath: /etc/db/pass.txt
        connectionPool:
          maxIdleConnections: 20
          maxOpenConnections: 20
          maxConnectionLifetime: 1m
      artifactsConfig:
        app:
          adminClient:
            hackFlagUntilCellIsolation: true
          artifactBlobStoreConfig:
            container: '{{ .Values.global.ARTIFACTS_BUCKET_NAME }}'
            stow:
              config:
                auth_type: iam
                region: '{{ .Values.global.AWS_REGION }}'
              kind: s3
            type: stow
          artifactDatabaseConfig:
            connMaxLifeTime: 1m
            maxIdleConnections: 20
            maxOpenConnections: 30
            # Duplicate for application postgres reference
            postgres:
              dbname: '{{ .Values.global.DB_NAME }}'
              host: '{{ .Values.global.DB_HOST }}'
              options: sslmode=disable
              passwordPath: /etc/db/pass.txt
              port: 5432
              # Default to the same host as the primary DB.
              readReplicaHost: '{{ .Values.global.DB_HOST }}'
              username: '{{ .Values.global.DB_USER }}'
          artifactServerConfig:
            httpPort: 8089
            port: 8080
            respectUserOrgsForServerless: true
          artifactTriggerConfig:
            executionMaxRetryCount: 3
            executionSchedulerQuerySize: 20
            executionSchedulers: 1
            executionSchedulersWait: 10
            triggerProcessorQuerySize: 100
            triggerProcessors: 1
            triggerProcessorsWait: 10
  authorizer:
    fullnameOverride: "authorizer"
    args:
      - authorizer
      - serve
      - --config
      - /etc/config/*.yaml
    configMap:
      sharedService:
        connectPort: 8081
        metrics:
          scope: "authorizer:"
  cluster:
    fullnameOverride: "cluster"
    initContainers:
      - name: migrate
        args:
          - cloudcluster
          - migrate
          - --config
          - "/etc/config/*.yaml"
    args:
      - cloudcluster
      - serve
      - --config
      - /etc/config/*.yaml
    configMap:
      sharedService:
        metrics:
          scope: "cluster:"
      cloudProvider:
        provider: Mock
      db:
        dbname: '{{ .Values.global.DB_NAME }}'
        host: '{{ .Values.global.DB_HOST }}'
        username:  '{{ .Values.global.DB_USER }}'
        passwordPath: /etc/db/pass.txt
        port: 5432
        connectionPool:
          maxIdleConnections: 20
          maxOpenConnections: 20
          maxConnectionLifetime: 1m
      cluster:
        cloudflare:
          active: false
  dataproxy:
    fullnameOverride: "dataproxy"
    args:
      - dataproxy
      - serve
      - --config
      - /etc/config/*.yaml
    configMap:
      sharedService:
        metrics:
          scope: "dataproxy:"
      dataproxy:
        secureTunnelTenantURLPattern: http://ingress-nginx-internal.ingress-nginx.svc.cluster.local:80 # http://ingress-nginx-internal.ingress-nginx.svc.cluster.local
        clusterSelector:
          type: local
  executions:
    fullnameOverride: "executions"
    initContainers:
      - name: migrate
        args:
          - cloudpropeller
          - migrate
          - --config
          - "/etc/config/*.yaml"
    args:
      - cloudpropeller
      - serve
      - --config
      - /etc/config/*.yaml
    configMap:
      workspace:
        enable: false
      sharedService:
        metrics:
          scope: "executions:"
      db:
        dbname: '{{ .Values.global.DB_NAME }}'
        host:  '{{ .Values.global.DB_HOST }}'
        username:  '{{ .Values.global.DB_USER }}'
        passwordPath: /etc/db/pass.txt
        port: 5432
        connectionPool:
          maxIdleConnections: 20
          maxOpenConnections: 20
          maxConnectionLifetime: 1m
      cloudEventsProcessor:
        cloudProvider: Local
      executions:

        # app:
          # adminClient:
            # connection:
              # -- Override rootTenantURLPattern for adminClient to point to control plane service.
              # endpoint: ""

              # -- Insecure should be true only for local testing with self-signed certs.
              # insecure: true|false

              # -- Skip TLS verification for self-signed certs. Should be true only for local testing.
              # insecureSkipVerify: true|false

        apps:
          # Identity service is disabled by default. Enriching identities can be enabled here.
          enrichIdentities: false
          # App serving URL pattern (work in progress for self-hosted)
          publicURLPattern: 'https://%s.apps.{{ .Values.global.UNION_HOST }}'

        # Disable LLM assistant features (requires Redis and OpenAI API key)
        llm:
          enabled: false

        # Enable V2 task/run/trigger endpoints
        task:
          enabled: true
          # Identity service is disabled by default. Enriching identities can be enabled here.
          enrichIdentities: false
  usage:
    fullnameOverride: "usage"
    args:
      - usage
      - serve
      - --config
      - /etc/config/*.yaml
    configMap:
      sharedService:
        metrics:
          scope: "usage:"
      cloudProvider:
        provider: Mock
      billing:
        enable: false
      usage:
        workers: 10
        taskMetrics:
          metricDelayToleranceDuration: 0s
          promQuery:
            queries:
              EXECUTION_METRIC_APP_REQUESTS: |
                sum(rate((
                  envoy_cluster_upstream_rq_xx{
                    job="serving-envoy",
                    project=~"{{ "{{" }}.Project{{ "}}" }}",
                    domain=~"{{ "{{" }}.Domain{{ "}}" }}",
                    name=~"{{ "{{" }}.AppName{{ "}}" }}",
                    name!=""}
                )[5m:])) by (project, domain, name, envoy_response_code_class)
              EXECUTION_METRIC_APP_RESPONSE_TIME_P50: |
                histogram_quantile(0.5, sum(rate((
                  envoy_cluster_upstream_rq_time_bucket{
                    job="serving-envoy",
                    project=~"${{ "{{" }}.Project{{ "}}" }}",
                    domain=~"{{ "{{" }}.Domain{{ "}}" }}",
                    name=~"{{ "{{" }}.AppName{{ "}}" }}",
                    name!=""}
                )[5m:])) by (project, domain, name, le))
              EXECUTION_METRIC_APP_RESPONSE_TIME_P90: |
                histogram_quantile(0.90, sum(rate((
                  envoy_cluster_upstream_rq_time_bucket{
                    job="serving-envoy",
                    project=~"${{ "{{" }}.Project{{ "}}" }}",
                    domain=~"{{ "{{" }}.Domain{{ "}}" }}",
                    name=~"{{ "{{" }}.AppName{{ "}}" }}",
                    name!=""}
                )[5m:])) by (project, domain, name, le))
              EXECUTION_METRIC_APP_RESPONSE_TIME_P95: |
                histogram_quantile(0.95, sum(rate((
                  envoy_cluster_upstream_rq_time_bucket{
                    job="serving-envoy",
                    project=~"${{ "{{" }}.Project{{ "}}" }}",
                    domain=~"{{ "{{" }}.Domain{{ "}}" }}",
                    name=~"{{ "{{" }}.AppName{{ "}}" }}",
                    name!=""}
                )[5m:])) by (project, domain, name, le))
              EXECUTION_METRIC_APP_REPLICA_COUNT: |
                sum (kube_pod_status_phase{phase=~"Running|Pending", namespace="{{ "{{" }}.Namespace{{ "}}" }}", pod=~"{{ "{{" }}.AppName{{ "}}" }}.*"} == 1) or vector(0)
              EXECUTION_METRIC_ALLOCATED_CPU_AVG: |
                max by (namespace, pod) (
                  (
                    sum by (namespace, pod) (irate(container_cpu_usage_seconds_total{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",image!=""}[5m])) >
                    sum by (namespace, pod) (kube_pod_container_resource_requests{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",resource="cpu"})
                  )
                  or
                  sum by (namespace, pod) (kube_pod_container_resource_requests{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",resource="cpu"})
                ) *
                on (namespace, pod) group_left max by (namespace, pod) (kube_pod_status_phase{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",phase=~"Pending|Running"} == 1)
              EXECUTION_METRIC_ALLOCATED_MEMORY_BYTES_AVG: |
                max by (namespace, pod) (
                  (
                    sum by (namespace, pod) (container_memory_working_set_bytes{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",image!=""}) >
                    sum by (namespace, pod) (kube_pod_container_resource_requests{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",resource="memory"})
                  )
                  or
                  sum by (namespace, pod) (kube_pod_container_resource_requests{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",resource="memory"})
                ) *
                on (namespace, pod) group_left max by (namespace, pod) (kube_pod_status_phase{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",phase=~"Pending|Running"} == 1)
              EXECUTION_METRIC_CPU_UTILIZATION: |
                (sum by (namespace, pod) (irate(container_cpu_usage_seconds_total{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",image!=""}[5m])) /
                  sum by (namespace, pod) (kube_pod_container_resource_requests{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",resource="cpu"})) *
                on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",phase=~"Pending|Running"} == 1)
              EXECUTION_METRIC_GPU_FRAME_BUFFER_UTILIZATION: |
                (sum by (namespace, pod, gpu) (DCGM_FI_DEV_FB_USED{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}"}) /
                  sum by (namespace, pod, gpu) (DCGM_FI_DEV_FB_USED{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}"} + DCGM_FI_DEV_FB_FREE{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}"})) *
                on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",phase=~"Pending|Running"} == 1)
              EXECUTION_METRIC_GPU_MEMORY_UTILIZATION: |
                sum by (gpu) (DCGM_FI_DEV_MEM_COPY_UTIL{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}"} *
                on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",phase=~"Pending|Running"} == 1)) / 100.0
              EXECUTION_METRIC_GPU_UTILIZATION: |
                sum by (gpu) (DCGM_FI_DEV_GPU_UTIL{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}"} *
                on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",phase=~"Pending|Running"} == 1)) / 100.0
              EXECUTION_METRIC_GPU_SM_ACTIVE: |
                sum by (gpu) (DCGM_FI_PROF_SM_ACTIVE{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}"} *
                on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",phase=~"Pending|Running"} == 1))
              EXECUTION_METRIC_GPU_SM_OCCUPANCY: |
                sum by (gpu) (DCGM_FI_PROF_SM_OCCUPANCY{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}"} *
                on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",phase=~"Pending|Running"} == 1))
              EXECUTION_METRIC_LIMIT_CPU: |
                sum by (namespace, pod) (kube_pod_container_resource_limits{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",resource="cpu"} *
                on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",phase=~"Pending|Running"} == 1))
              EXECUTION_METRIC_LIMIT_MEMORY_BYTES: |
                sum by (namespace, pod) (kube_pod_container_resource_limits{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",resource="memory"} *
                on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",phase=~"Pending|Running"} == 1))
              EXECUTION_METRIC_MEMORY_UTILIZATION: |
                (sum by (namespace, pod) (container_memory_working_set_bytes{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",image!=""}) /
                  sum by (namespace, pod) (kube_pod_container_resource_requests{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",resource="memory"})) *
                on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",phase=~"Pending|Running"} == 1)
              EXECUTION_METRIC_REQUEST_CPU: |
                sum by (namespace, pod) (kube_pod_container_resource_requests{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",resource="cpu"} *
                on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",phase=~"Pending|Running"} == 1))
              EXECUTION_METRIC_REQUEST_MEMORY_BYTES: |
                sum by (namespace, pod) (kube_pod_container_resource_requests{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",resource="memory"} *
                on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",phase=~"Pending|Running"} == 1))
              EXECUTION_METRIC_USED_CPU_AVG: |
                sum by (namespace, pod) (irate(container_cpu_usage_seconds_total{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",image!=""}[5m]) *
                on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",phase=~"Pending|Running"} == 1))
              EXECUTION_METRIC_USED_MEMORY_BYTES_AVG: |
                sum by (namespace, pod) (container_memory_working_set_bytes{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",image!=""} *
                on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{ "{{" }}.Namespace{{ "}}" }}",pod=~"{{ "{{" }}.PodName{{ "}}" }}",phase=~"Pending|Running"} == 1))
          agentQuery:
            mappings:
              dgx_job:
                queries:
                  EXECUTION_METRIC_ALLOCATED_CPU_AVG: "CPU_ALLOCATION:MEAN"
                  EXECUTION_METRIC_ALLOCATED_MEMORY_BYTES_AVG: "MEM_ALLOCATION:MEAN"
                  EXECUTION_METRIC_CPU_UTILIZATION: "CPU_UTILIZATION:MEAN"
                  EXECUTION_METRIC_MEMORY_UTILIZATION: "MEM_UTILIZATION:MEAN"
                  EXECUTION_METRIC_GPU_UTILIZATION: "GPU_UTILIZATION:MEAN"
    resources:
      limits:
        cpu: 3
        memory: 512Mi
      requests:
        cpu: 500m
        memory: 250Mi

  run-scheduler:
    fullnameOverride: "run-scheduler"
    args:
      - cloudpropeller
      - scheduler
      - start
      - --config
      - "/etc/config/*.yaml"
    initContainers:
      - name: migrate
        args:
          - cloudpropeller
          - migrate
          - --config
          - "/etc/config/*.yaml"
    configMap:
      sharedService:
        metrics:
          scope: "run-scheduler:"
      db:
        dbname: '{{ .Values.global.DB_NAME }}'
        host:  '{{ .Values.global.DB_HOST }}'
        username:  '{{ .Values.global.DB_USER }}'
        passwordPath: /etc/db/pass.txt
        port: 5432
        connectionPool:
          maxIdleConnections: 20
          maxOpenConnections: 20
          maxConnectionLifetime: 1m

  queue:
    fullnameOverride: "queue"

    # Queue service must only run a single replica.
    replicaCount: 1
    autoscaling:
      enabled: false

    args:
      - queue
      - serve
      - '--config'
      - /etc/config/*.yaml
    initContainers:
      - name: migrate
        args:
          - queue
          - migrate
          - --config
          - "/etc/config/*.yaml"
    configMap:
      sharedService:
        metrics:
          scope: "queue:"
      queue:
        db:
          hosts:
            - "{{ .Values.scylla.fullnameOverride }}-client.{{ .Release.Namespace }}.svc.cluster.local"
          threadCount: 64
          type: cql
        eventer:
          recordActionThreadCount: 16
          type: runservice
          updateActionStatusThreadCount: 16


# Union console configuration.
console:
  replicaCount: 1

  image:
    repository: "{{ .Values.global.IMAGE_REPOSITORY_PREFIX }}/unionconsole"
    pullPolicy: IfNotPresent
    tag: "{{ .Chart.AppVersion }}"

  nameOverride: ""
  # More readable resource names.
  fullnameOverride: "unionconsole"

  serviceAccount:
    create: true
    annotations: {}
    name: ""

  podAnnotations:
    kubectl.kubernetes.io/default-container: unionconsole

  podLabels: {}

  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroupChangePolicy: OnRootMismatch
    seLinuxOptions:
      type: spc_t

  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL

  service:
    type: ClusterIP
    port: 80
    targetPort: 8080
    metricsPort: 8081
    annotations: {}

  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 250m
      memory: 250Mi

  # Environment variables
  env: []

  # Additional environment variables from ConfigMap
  envFrom: []

  nodeSelector: {}

  tolerations: []

  affinity: {}

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1

flyte:
  # Database configuration (references globals)
  dbHost: '{{ .Values.global.DB_HOST }}'
  dbName: '{{ .Values.global.DB_NAME }}'
  dbUser: '{{ .Values.global.DB_USER }}'

  # Storage configuration (references globals)
  bucketName: '{{ .Values.global.BUCKET_NAME }}'
  region: '{{ .Values.global.AWS_REGION }}'

  flyteadmin:
    # -- Set the image used for flyteadmin
    image:
      # TODO flyte-core does not render templated repository and tag correctly.
      # PR: https://github.com/flyteorg/flyte/pull/6711
      repository: "643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services"
      pullPolicy: IfNotPresent
      tag:  ""

    # Replica count to be used as long as autoscaling is disabled
    replicaCount: 1

    # Autoscaling configuration for flyteadmin
    autoscaling:
       # Enable autoscaling for flyteadmin
      enabled: true

    strategy:
      type: RollingUpdate
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1

    serviceAccount:
      # -- Set the role arn for the flyteadmin service account which has access to the S3 bucket
      annotations: {}
    podAnnotations:
      kubectl.kubernetes.io/default-container: flyteadmin
    initialProjects:
      - union-health-monitoring
      - flytesnacks
    readinessProbe: |-
      httpGet:
        path: /healthcheck
        port: 8088
      initialDelaySeconds: 15
      timeoutSeconds: 1
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 3

    livenessProbe: |-
      httpGet:
        path: /healthcheck
        port: 8088
      initialDelaySeconds: 20
      timeoutSeconds: 1
      periodSeconds: 5
      successThreshold: 1
      failureThreshold: 3
    resources:
      limits:
        cpu: 2
        ephemeral-storage: 500Mi
        memory: 3Gi
      requests:
        cpu: 50m
        ephemeral-storage: 200Mi
        memory: 500Mi

  flytescheduler:
    image:
      # TODO flyte-core does not render templated repository and tag correctly.
      # PR: https://github.com/flyteorg/flyte/pull/6711
      repository: "643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/flytescheduler"
      pullPolicy: IfNotPresent
      tag:  ""

  workflow_scheduler:
    enabled: false
    type: native

  datacatalog:
    # Enable datacatalog service
    enabled: false
    replicaCount: 1
    image:
      # TODO flyte-core does not render templated repository and tag correctly.
      # PR: https://github.com/flyteorg/flyte/pull/6711
      repository: "643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/datacatalog" # -- Set the repository for the public datacatalog image"
      tag: "" # -- Set the tag for the public datacatalog image
      pullPolicy: "IfNotPresent" # -- Set the pull policy for the datacatalog image

    autoscaling:
       # -- Enable autoscaling for datacatalog
      enabled: true

    strategy:
      type: RollingUpdate
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1

    resources:
      limits:
        cpu: 1
        ephemeral-storage: 500Mi
        memory: 1Gi
      requests:
        cpu: 10m
        ephemeral-storage: 50Mi
        memory: 50Mi
    serviceAccount:
      # -- Set the role arn for the datacatlog service account which has access to the S3 bucket
      annotations: {}
    service:
      type: ClusterIP
  flytepropeller:
    enabled: false
  flyteconsole:
    enabled: true
    podEnv: {}
    replicaCount: 1
    image:
      # TODO flyte-core does not render templated repository and tag correctly.
      # PR: https://github.com/flyteorg/flyte/pull/6711
      repository: "643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/flyteconsole" # -- Set the repository for the console image"
      tag: "" # -- Set the tag for the console image
    resources:
      # flyteconsole service has less memory footprint but cpu goes up depends on traffic, If CPU use is high then we will use HPA
      limits:
        cpu: 250m
        memory: 250Mi
        ephemeral-storage: 200Mi # Pre-liminary limit to avoid runaway disk usage
      requests:
        cpu: 10m
        memory: 50Mi
        ephemeral-storage: 20Mi
    serviceAccount:
      create: true
    podAnnotations:
      linkerd.io/inject: disabled
      prometheus.io/scrape: "false"
    ga:
      enabled: true
      tracking_id: ""
    service:
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "600"
        external-dns.alpha.kubernetes.io/hostname: "flyte.example.com"
      type: ClusterIP
    serviceMonitor:
      enabled: false
  webhook:
    enabled: false
  cluster_resource_manager:
    enabled: false
  cacheservice:
    enabled: true
    replicaCount: 1
    image:
      # TODO flyte-core does not render templated repository and tag correctly.
      # PR: https://github.com/flyteorg/flyte/pull/6711
      repository: "643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services" # -- Set the repository for the private cacheservice image"
      tag: "" # -- Set the tag for the private cacheservice datacatalog image
      pullPolicy: "IfNotPresent" # -- Set the pull policy for the private cacheservice  image
    serviceAccount:
      # -- If the service account is created by you, make this false
      create: true
      # -- Set the role arn for the cacheservice service account which has access to the S3 bucket
      annotations: {}
    resources:
      limits:
        cpu: 1
        ephemeral-storage: 200Mi
      requests:
        cpu: 500m
        ephemeral-storage: 200Mi
        memory: 200Mi
    configPath: /etc/cacheservice/config/*.yaml
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/name: cacheservice
            topologyKey: kubernetes.io/hostname

    service:
      annotations: { }
      type: ClusterIP
    # -- Annotations for Cacheservice pods
    podAnnotations: { }
    # -- Additional Cacheservice container environment variables
    podEnv: { }
    # -- Labels for Cacheservice pods
    podLabels: { }
    # -- nodeSelector for Cacheservice deployment
    nodeSelector: { }
    # -- tolerations for Cacheservice deployment
    tolerations: [ ]
    # -- Appends additional volumes to the deployment spec. May include template values.
    additionalVolumes: [ ]
    # -- Appends additional volume mounts to the main container's spec. May include template values.
    additionalVolumeMounts: [ ]
    # -- Appends additional containers to the deployment spec. May include template values.
    additionalContainers: [ ]
    # -- Appends extra command line arguments to the main command
    extraArgs: { }
    # -- Sets priorityClassName for cacheservice pod(s).
    priorityClassName: ""
    # -- Sets securityContext for cacheservice pod(s).
    securityContext:
      runAsNonRoot: true
      fsGroup: 1001
      runAsUser: 1001
      fsGroupChangePolicy: "OnRootMismatch"
      seLinuxOptions:
        type: spc_t
  common:
    databaseSecret:
      # -- Specify name of K8s Secret which contains Database password. Leave it empty if you don't need this Secret
      # flyte-org/flyte flyte-core helm chart _helpers.tpl does not render templates.
      # Therefore we have to explicitly set the value here.
      # Ref: https://github.com/flyteorg/flyte/pull/6711
      # TODO (DIRECTLY CONFIGURE): Match value to global.KUBERNETES_SECRET_NAME
      name: "union-controlplane-secrets"
      # -- Leave it empty if your secret already exists
      secretManifest: {}

        # Else you can create your own secret object. You can use Kubernetes secrets, else you can configure external secrets
        # For external secrets please install Necessary dependencies, like, of your choice
        # - https://github.com/hashicorp/vault
        # - https://github.com/godaddy/kubernetes-external-secrets
        # apiVersion: v1
        # kind: Secret
        # metadata:
        #   name: db-pass
        # type: Opaque
        # stringData:
        #   # -- If using plain text you can provide the password here
        #   pass.txt: '{{ .Values.dbPass }}'
    ingress:
      # Disables flyte subchart ingresses.
      enabled: false

  storage:
    # -- Sets the storage type. Supported values are sandbox, s3, gcs and custom.
    type: s3
    # -- bucketName defines the storage bucket flyte will use. Required for all types except for sandbox.
    bucketName: '{{ .Values.global.BUCKET_NAME }}'
    s3:
      region: "{{ .Values.global.AWS_REGION }}"
    cache:
      maxSizeMBs: 1024
  db:
    checks: false
    datacatalog:
      database:
        port: 5432
        # -- Create a user called flyteadmin
        username: "{{ .Values.global.DB_USER }}"
        host: "{{ .Values.global.DB_HOST }}"
        # -- Use a specific DB name for datacatalog
        dbname: datacatalog
        passwordPath: /etc/db/pass.txt
        maxIdleConnections: 10
        maxOpenConnections: 20
        connMaxLifeTime: 120s
    admin:
      database:
        port: 5432
        # -- Create a user called flyteadmin
        username: "{{ .Values.global.DB_USER }}"
        host: "{{ .Values.global.DB_HOST }}"
        # -- Use a specific DB name for flyteadmin
        dbname: flyteadmin
        passwordPath: /etc/db/pass.txt
        maxIdleConnections: 10
        maxOpenConnections: 80
        connMaxLifeTime: 120s
    cacheservice:
      database:
        port: 5432
        # -- Create a user called flyteadmin
        username: "{{ .Values.global.DB_USER }}"
        host: "{{ .Values.global.DB_HOST }}"
        # -- Use a specific DB name for cacheservice
        dbname: cacheservice
        passwordPath: /etc/db/pass.txt
        maxIdleConnections: 10
        maxOpenConnections: 20
        connMaxLifeTime: 120s
  configmap:
    adminServer:
      admin:
        endpoint: 'dns:///{{ .Values.global.UNION_HOST }}'
        insecure: false
      authorizer:
        type: "Noop"
        internalCommunicationConfig:
          enabled: false
      cloudEvents:
        enable: false
      connection:
        environment: "staging"
        region: '{{ .Values.global.AWS_REGION }}'
        rootTenantURLPattern: 'dns:///{{ .Values.global.UNION_HOST }}'
      flyteadmin:
        metricsKeys:
          - phase
        useOffloadedInputs: true
        useOffloadedWorkflowClosure: true
        # See: https://github.com/unionai/flyte/pull/733/files
        testing: null
      otel:  # Ref: https://github.com/flyteorg/flyte/blob/master/flytestdlib/otelutils/config.go
        type: noop
      private:
        app:
          cacheProviderConfig:
            kind: bypass
          # Don't rely on user API to exists
          populateUserFields: false
      server:
        security:
          useAuth: false
      union:
        internalConnectionConfig:
          enabled: true
          urlPattern: '{{ printf "_SERVICE_.%s.svc.cluster.local:80" .Release.Namespace }}'
      sharedService:
        # Flyteadmin container and service parts hardcoded and should not change.
        # https://github.com/flyteorg/flyte/blob/v1.16.0-b2/charts/flyte-core/templates/admin/service.yaml
        connectPort: 8089
        httpPort: 8088
        port: 8089
    cacheserviceServer:
      union:
        internalConnectionConfig:
          enabled: true
          urlPattern: '{{ printf "_SERVICE_.%s.svc.cluster.local:80" .Release.Namespace }}'
      authorizer:
        type: "Noop"
        internalCommunicationConfig:
          enabled: false
      cacheservice:
        storage-prefix: cached_outputs
        metrics-scope: flyte
        profiler-port: 10254
        heartbeat-grace-period-multiplier: 3
        max-reservation-heartbeat: 30s
      cache-server:
        grpcPort: 8089
        httpPort: 8080
        grpcServerReflection: true
      otel:  # Ref: https://github.com/flyteorg/flyte/blob/master/flytestdlib/otelutils/config.go
        type: noop
      private:
        app:
          cacheProviderConfig:
            kind: bypass
    logger:
      # This level is incorrectly positioned and needs to be removed in underlying subchart
      level: null

      # In order to set the log level, this is the correct structure.
      # Log levels range from 1 (least verbose) to 5 ("DEBUG" most verbose)
      # logger:
      #   level: 5
    sharedService:
      connectPort: 8089
      httpPort: 8088
      port: 8089
    admin:
      # Hack: Initializing clients triggers a code path in AdminService where it attempt to extract
      # the org from the host. This constraint breaks in situations where we are using a DNS entry that
      # does not have atleast one "." in it. The default flyte-core helm chart defaults to access flyteadmin
      # at "flyteadmin:81". This breaks the Union specific code path. To unblock testing, we are setting
      # the endpoint to the full DNS entry of the flyteadmin service in the cluster.
      endpoint: flyteadmin.{{ .Release.Namespace }}.svc.cluster.local:81
      insecure: true
      clientId: null
      clientSecretLocation: null
  cloudEvents:
    enable: false
  workflow_notifications:
    enabled: false

unionv2:
  enabled: false

dataproxy:
  prometheus:
    enabled: false

artifacts:
  enabled: false

objectstore:
  controlPlane:
    enabled: false

# Disable by default, but can be explicityl enabled if needed.
ingress-nginx:
  enabled: false

  controller:

    # Disable admission webhooks for simplicity
    admissionWebhooks:
      enabled: true

    # Snippet annotations are required
    allowSnippetAnnotations: true
    config:
      annotations-risk-level: "Critical"
      grpc-connect-timeout: "1200"
      grpc-read-timeout: "604800"
      grpc-send-timeout: "604800"

      # HTTP proxy timeouts (in seconds) - for long-running Watch streams
      proxy-read-timeout: "3600"   # 1 hour
      proxy-send-timeout: "3600"   # 1 hour
      proxy-connect-timeout: "60"  # 60 seconds

    ingressClassResource:
      enabled: true
      default: false # Assume shared with dataplane and require explicit ingress class assignment.

      name: controlplane
      controllerValue: union.ai/controlplane

# ScyllaDB configuration
# REQUIRED: ScyllaDB is used exclusively by the queue service for high-performance message queueing
# Postgres (configured above) is used by all other control plane services
#
# When enabled, deploys ScyllaDB Operator and Cluster as the queue service backend
#
# IMPORTANT: Before enabling ScyllaDB, you MUST manually install the CRDs:
#   Run the helper script: ./scripts/install-scylla-crds.sh
#
#   Or manually:
#   helm repo add scylla-operator https://scylla-operator-charts.storage.googleapis.com/stable
#   tmpdir=$(mktemp -d) \
#   && helm pull scylla-operator/scylla-operator --version v1.18.1 --untar --untardir "${tmpdir}" \
#   && kubectl apply --server-side -f "${tmpdir}"/scylla-operator/crds/ \
#   && rm -rf "${tmpdir}"
#
# This is required because Helm does not properly manage CRD lifecycle updates.
# See: https://operator.docs.scylladb.com/stable/installation/helm.html
scylla:
  enabled: true

  # ScyllaDB cluster configuration
  fullnameOverride: scylla

  # Storage class configuration
  storageClass:
    create: true # TODO set to false if you have an existing storage class
    name: "scylladb" # arbitrary name for the storage class
    provisioner: "ebs.csi.eks.amazonaws.com"
    parameters:
      type: gp2
      fsType: ext4
    reclaimPolicy: Delete
    volumeBindingMode: WaitForFirstConsumer
    allowVolumeExpansion: true

  # Datacenter configuration
  datacenter: dc1

  # Rack configuration
  racks:
    - name: rack1
      agentResources:
        requests:
          cpu: 50m
          memory: 10M
      members: 3
      storage:
        capacity: 100Gi
        storageClassName: "scylladb"  # Reference the storage class created above
      resources:
        limits:
          cpu: 2
          memory: 4Gi
        requests:
          cpu: 1
          memory: 2Gi
      # Explicitly disable placement constraints (operator adds defaults otherwise)
      # For production with dedicated nodes, configure nodeAffinity and tolerations here
      placement:
        nodeAffinity: {}
        tolerations: []

  # ScyllaDB version
  scyllaImage:
    tag: 2025.1.5

  # Developer mode (disable for production)
  developerMode: true

  # Sysctl properties
  sysctls:
    - fs.aio-max-nr=30000000

  # Additional ScyllaDB configuration can be added here
  # See https://operator.docs.scylladb.com/stable/generic for more options

# ScyllaDB Operator configuration
# The operator manages ScyllaDB clusters in Kubernetes
scylla-operator:

  fullnameOverride: scylla-operator

  # Webhook configuration
  webhook:
    # Set to false if you don't need admission webhooks
    enabled: true

  # Additional operator configuration
  # See https://operator.docs.scylladb.com/stable/installation for more options

# NOTE: ScyllaDB Manager is NOT included as a subchart.
# For backup/repair functionality, install scylla-manager separately.
# See README.md for installation instructions.

# This section is for additional objects that can be added to the Helm chart.
extraObjects: []

# ----------------------------------------------------------------------------
# Monitoring & Observability
# ----------------------------------------------------------------------------
# Optional kube-prometheus-stack instance for monitoring controlplane services.
# This deploys Prometheus, Grafana, and supporting exporters for general
# observability of the controlplane.
#
# CRDs are NOT installed by this chart. They must be installed separately,
# either by the dataplane chart or manually:
#   helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
#   helm install prometheus-crds prometheus-community/prometheus-operator-crds
monitoring:
  enabled: false

  # Override the name to distinguish from dataplane prometheus instances.
  nameOverride: "union-cp-monitoring"
  fullnameOverride: "union-cp-monitoring"

  prometheusOperator:
    enabled: true

  # CRDs should be installed by the dataplane chart or separately.
  # Do NOT install CRDs from the controlplane to avoid conflicts.
  crds:
    enabled: false

  # Alertmanager disabled by default.
  alertmanager:
    enabled: false

  grafana:
    enabled: true
    fullNameOverride: "union-cp-monitoring-grafana"
    adminPassword: admin

  # Default monitoring for K8s components that impact controlplane performance.
  coreDns:
    enabled: true
  defaultRules:
    create: true
  kubeApiServer:
    enabled: true
  kubeControllerManager:
    enabled: true
  kubeEtcd:
    enabled: true
  kubeProxy:
    enabled: true
  kubeScheduler:
    enabled: true
  nodeExporter:
    enabled: true
  kubelet:
    enabled: true
  kubeStateMetrics:
    enabled: true

  kube-state-metrics:
    nameOverride: "union-cp-monitoring-kube-state-metrics"
    fullnameOverride: "union-cp-monitoring-kube-state-metrics"

  prometheus:
    enabled: true

    # Ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/platform/prometheus-agent.md
    agentMode: false

    service:
      port: 80
    prometheusSpec:
      maximumStartupDurationSeconds: 600
      retention: 7d

      # Use NotIn to pick up everything EXCEPT union-features CRDs.
      # This matches CRDs with no label, with "union-services", or any other value,
      # but excludes CRDs explicitly labeled "union-features" (from the dataplane).
      ruleSelectorNilUsesHelmValues: false
      ruleSelector:
        matchExpressions:
        - key: platform.union.ai/prometheus-group
          operator: NotIn
          values: ["union-features"]

      serviceMonitorSelectorNilUsesHelmValues: false
      serviceMonitorSelector:
        matchExpressions:
        - key: platform.union.ai/prometheus-group
          operator: NotIn
          values: ["union-features"]

      podMonitorSelectorNilUsesHelmValues: false
      podMonitorSelector:
        matchExpressions:
        - key: platform.union.ai/prometheus-group
          operator: NotIn
          values: ["union-features"]

      resources:
        limits:
          cpu: "2"
          memory: "4Gi"
        requests:
          cpu: "500m"
          memory: "1Gi"
