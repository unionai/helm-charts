---
# Source: dataplane/templates/common/namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: flytesnacks-development
---
# Source: dataplane/templates/common/namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: flytesnacks-staging
---
# Source: dataplane/templates/common/namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: flytesnacks-production
---
# Source: dataplane/templates/common/namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: union-health-monitoring-development
---
# Source: dataplane/templates/common/namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: union-health-monitoring-staging
---
# Source: dataplane/templates/common/namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: union-health-monitoring-production
---
# Source: dataplane/charts/dcgm-exporter/templates/serviceaccount.yaml
# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-dcgm-exporter
  namespace: union
  labels:
    helm.sh/chart: dcgm-exporter-4.1.0
    app.kubernetes.io/name: dcgm-exporter
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "4.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "dcgm-exporter"
---
# Source: dataplane/charts/fluentbit/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentbit-system
  namespace: union
  labels:
    helm.sh/chart: fluentbit-0.48.9
    app.kubernetes.io/name: fluentbit
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "3.2.8"
    app.kubernetes.io/managed-by: Helm
---
# Source: dataplane/charts/opencost/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: opencost
  namespace: union
  labels:
    helm.sh/chart: opencost-1.42.0
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.111.0"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: dataplane/charts/prometheus/charts/kube-state-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-5.33.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.15.0"
    release: release-name
  name: release-name-kube-state-metrics
  namespace: kube-system
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus-operator
  namespace: union
  labels:
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app: prometheus-operator
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator
automountServiceAccountToken: true
---
# Source: dataplane/charts/prometheus/templates/prometheus/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: union-operator-prometheus
  namespace: union
  labels:
    app: prometheus-prometheus
    app.kubernetes.io/name: prometheus-prometheus
    app.kubernetes.io/component: prometheus
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
automountServiceAccountToken: true
---
# Source: dataplane/templates/clusterresourcesync/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: clustersync-system
  namespace: union
---
# Source: dataplane/templates/nodeexecutor/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: executor
  namespace: union
  labels:
    app: executor
---
# Source: dataplane/templates/operator/serviceaccount-proxy.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: proxy-system
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: dataplane/templates/operator/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: operator-system
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: dataplane/templates/propeller/serviceaccount-webhook.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flytepropeller-webhook-system
  namespace: union
---
# Source: dataplane/templates/propeller/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flytepropeller-system
  namespace: union
---
# Source: dataplane/templates/common/cluster-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: operator-cluster-name
type: Opaque
data:
  cluster_name:
---
# Source: dataplane/templates/propeller/deployment-webhook.yaml
# Create an empty secret that the first propeller pod will populate
apiVersion: v1
kind: Secret
metadata:
  name: flyte-pod-webhook
  namespace: union
type: Opaque
---
# Source: dataplane/charts/dcgm-exporter/templates/metrics-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: exporter-metrics-config-map
  namespace: union
data:
  metrics: |
      # Format
      # If line starts with a '#' it is considered a comment
      # DCGM FIELD, Prometheus metric type, help message
      
      # Clocks
      DCGM_FI_DEV_SM_CLOCK,  gauge, SM clock frequency (in MHz).
      DCGM_FI_DEV_MEM_CLOCK, gauge, Memory clock frequency (in MHz).
      
      # Temperature
      DCGM_FI_DEV_MEMORY_TEMP, gauge, Memory temperature (in C).
      DCGM_FI_DEV_GPU_TEMP,    gauge, GPU temperature (in C).
      
      # Power
      DCGM_FI_DEV_POWER_USAGE,              gauge, Power draw (in W).
      DCGM_FI_DEV_TOTAL_ENERGY_CONSUMPTION, counter, Total energy consumption since boot (in mJ).
      
      # PCIE
      # DCGM_FI_PROF_PCIE_TX_BYTES,  counter, Total number of bytes transmitted through PCIe TX via NVML.
      # DCGM_FI_PROF_PCIE_RX_BYTES,  counter, Total number of bytes received through PCIe RX via NVML.
      DCGM_FI_DEV_PCIE_REPLAY_COUNTER, counter, Total number of PCIe retries.
      
      # Utilization (the sample period varies depending on the product)
      DCGM_FI_DEV_GPU_UTIL,      gauge, GPU utilization (in %).
      DCGM_FI_DEV_MEM_COPY_UTIL, gauge, Memory utilization (in %).
      DCGM_FI_DEV_ENC_UTIL,      gauge, Encoder utilization (in %).
      DCGM_FI_DEV_DEC_UTIL ,     gauge, Decoder utilization (in %).
      
      # Errors and violations
      DCGM_FI_DEV_XID_ERRORS,            gauge,   Value of the last XID error encountered.
      # DCGM_FI_DEV_POWER_VIOLATION,       counter, Throttling duration due to power constraints (in us).
      # DCGM_FI_DEV_THERMAL_VIOLATION,     counter, Throttling duration due to thermal constraints (in us).
      # DCGM_FI_DEV_SYNC_BOOST_VIOLATION,  counter, Throttling duration due to sync-boost constraints (in us).
      # DCGM_FI_DEV_BOARD_LIMIT_VIOLATION, counter, Throttling duration due to board limit constraints (in us).
      # DCGM_FI_DEV_LOW_UTIL_VIOLATION,    counter, Throttling duration due to low utilization (in us).
      # DCGM_FI_DEV_RELIABILITY_VIOLATION, counter, Throttling duration due to reliability constraints (in us).
      
      # Memory usage
      DCGM_FI_DEV_FB_FREE, gauge, Framebuffer memory free (in MiB).
      DCGM_FI_DEV_FB_USED, gauge, Framebuffer memory used (in MiB).
      
      # ECC
      # DCGM_FI_DEV_ECC_SBE_VOL_TOTAL, counter, Total number of single-bit volatile ECC errors.
      # DCGM_FI_DEV_ECC_DBE_VOL_TOTAL, counter, Total number of double-bit volatile ECC errors.
      # DCGM_FI_DEV_ECC_SBE_AGG_TOTAL, counter, Total number of single-bit persistent ECC errors.
      # DCGM_FI_DEV_ECC_DBE_AGG_TOTAL, counter, Total number of double-bit persistent ECC errors.
      
      # Retired pages
      # DCGM_FI_DEV_RETIRED_SBE,     counter, Total number of retired pages due to single-bit errors.
      # DCGM_FI_DEV_RETIRED_DBE,     counter, Total number of retired pages due to double-bit errors.
      # DCGM_FI_DEV_RETIRED_PENDING, counter, Total number of pages pending retirement.
      
      # NVLink
      # DCGM_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_TOTAL, counter, Total number of NVLink flow-control CRC errors.
      # DCGM_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_TOTAL, counter, Total number of NVLink data CRC errors.
      # DCGM_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_TOTAL,   counter, Total number of NVLink retries.
      # DCGM_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_TOTAL, counter, Total number of NVLink recovery errors.
      DCGM_FI_DEV_NVLINK_BANDWIDTH_TOTAL,            counter, Total number of NVLink bandwidth counters for all lanes.
      # DCGM_FI_DEV_NVLINK_BANDWIDTH_L0,               counter, The number of bytes of active NVLink rx or tx data including both header and payload.
      
      # VGPU License status
      DCGM_FI_DEV_VGPU_LICENSE_STATUS, gauge, vGPU License status
      
      # Remapped rows
      DCGM_FI_DEV_UNCORRECTABLE_REMAPPED_ROWS, counter, Number of remapped rows for uncorrectable errors
      DCGM_FI_DEV_CORRECTABLE_REMAPPED_ROWS,   counter, Number of remapped rows for correctable errors
      DCGM_FI_DEV_ROW_REMAP_FAILURE,           gauge,   Whether remapping of rows has failed
      
      # DCP metrics
      DCGM_FI_PROF_GR_ENGINE_ACTIVE,   gauge, Ratio of time the graphics engine is active.
      # DCGM_FI_PROF_SM_ACTIVE,          gauge, The ratio of cycles an SM has at least 1 warp assigned.
      # DCGM_FI_PROF_SM_OCCUPANCY,       gauge, The ratio of number of warps resident on an SM.
      DCGM_FI_PROF_PIPE_TENSOR_ACTIVE, gauge, Ratio of cycles the tensor (HMMA) pipe is active.
      DCGM_FI_PROF_DRAM_ACTIVE,        gauge, Ratio of cycles the device memory interface is active sending or receiving data.
      # DCGM_FI_PROF_PIPE_FP64_ACTIVE,   gauge, Ratio of cycles the fp64 pipes are active.
      # DCGM_FI_PROF_PIPE_FP32_ACTIVE,   gauge, Ratio of cycles the fp32 pipes are active.
      # DCGM_FI_PROF_PIPE_FP16_ACTIVE,   gauge, Ratio of cycles the fp16 pipes are active.
      DCGM_FI_PROF_PCIE_TX_BYTES,      counter, The number of bytes of active pcie tx data including both header and payload.
      DCGM_FI_PROF_PCIE_RX_BYTES,      counter, The number of bytes of active pcie rx data including both header and payload.
---
# Source: dataplane/templates/clusterresourcesync/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: flyte-clusterresourcesync-config
  namespace: union
  labels: 
    app.kubernetes.io/name: clusterresourcesync
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
data:
  cluster_resources.yaml: | 
    cluster_resources:
      clusterName: ''
      customData:
      - production:
        - projectQuotaCpu:
            value: "4096"
        - projectQuotaMemory:
            value: 2Ti
        - projectQuotaNvidiaGpu:
            value: "256"
        - defaultUserRoleKey:
            value: 'eks.amazonaws.com/role-arn'
        - defaultUserRoleValue:
            value: 'arn:aws:iam::ACCOUNT_ID:role/flyte_project_role'
      - staging:
        - projectQuotaCpu:
            value: "4096"
        - projectQuotaMemory:
            value: 2Ti
        - projectQuotaNvidiaGpu:
            value: "256"
        - defaultUserRoleKey:
            value: 'eks.amazonaws.com/role-arn'
        - defaultUserRoleValue:
            value: 'arn:aws:iam::ACCOUNT_ID:role/flyte_project_role'
      - development:
        - projectQuotaCpu:
            value: "4096"
        - projectQuotaMemory:
            value: 2Ti
        - projectQuotaNvidiaGpu:
            value: "256"
        - defaultUserRoleKey:
            value: 'eks.amazonaws.com/role-arn'
        - defaultUserRoleValue:
            value: 'arn:aws:iam::ACCOUNT_ID:role/flyte_project_role'
      refreshInterval: 5m
      standaloneDeployment: true
      templatePath: /etc/flyte/clusterresource/templates
    clusterResourcesPrivate:
      app:
        isServerless: false
    union:
      auth:
        authorizationMetadataKey: flyte-authorization
        clientId: 'dataplane-operator'
        clientSecretLocation: /etc/union/secret/client_secret
        tokenRefreshWindow: 5m
        type: ClientSecret
      connection:
        host: dns:///
  admin.yaml: | 
    admin:
      clientId: 'dataplane-operator'
      clientSecretLocation: /etc/union/secret/client_secret
      endpoint: dns:///
      insecure: false
    event:
      capacity: 1000
      rate: 500
      type: admin
  domain.yaml: | 
    domains:
    - id: development
      name: development
    - id: staging
      name: staging
    - id: production
      name: production
  clusters.yaml: |
    clusters:
      clusterConfigs: []
      labelClusterMap: {}
---
# Source: dataplane/templates/clusterresourcesync/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: clusterresource-template
  namespace: union
  labels: 
    app.kubernetes.io/name: clusterresourcesync
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
data:
  a_namespace.yaml: | 
    apiVersion: v1
    kind: Namespace
    metadata:
      name: {{ namespace }}
      labels:
        union.ai/namespace-type: flyte
    spec:
      finalizers:
      - kubernetes
    
  b_default_service_account.yaml: | 
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: default
      namespace: {{ namespace }}
      annotations:
        {{ defaultUserRoleKey }}: {{ defaultUserRoleValue }}
    
  c_project_resource_quota.yaml: | 
    apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: project-quota
      namespace: {{ namespace }}
    spec:
      hard:
        limits.cpu: {{ projectQuotaCpu }}
        limits.memory: {{ projectQuotaMemory }}
        requests.nvidia.com/gpu: {{ projectQuotaNvidiaGpu }}
---
# Source: dataplane/templates/fluent-bit/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentbit-system
  namespace: union
  labels:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  custom_parsers.conf: |
    [PARSER]
        Name docker_no_time
        Format json
        Time_Keep Off
        Time_Key time
        Time_Format %Y-%m-%dT%H:%M:%S.%L
  fluent-bit.conf: |
    [SERVICE]
        Parsers_File /fluent-bit/etc/parsers.conf
        Parsers_File /fluent-bit/etc/conf/custom_parsers.conf
        HTTP_Server On
        HTTP_Listen 0.0.0.0
        Health_Check On
    [INPUT]
        Name                tail
        Tag                 namespace-<namespace_name>.pod-<pod_name>.cont-<container_name>
        Tag_Regex           (?<pod_name>[a-z0-9](?:[-a-z0-9]*[a-z0-9])?(?:\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*)_(?<namespace_name>[^_]+)_(?<container_name>.+)-
        Path                /var/log/containers/*.log
        DB                  /var/log/flb_kube.db
        multiline.parser    docker, cri
        Mem_Buf_Limit       5MB
        Skip_Long_Lines     On
        Refresh_Interval    10
    
    
    [OUTPUT]
        Name s3
        Match *
        upload_timeout 1m
        s3_key_format /persisted-logs/$TAG
        static_file_path true
        json_date_key false
        region us-east-1
        bucket
---
# Source: dataplane/templates/nodeexecutor/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: executor
  namespace: union
  labels:
    app: executor
data:
  task_logs.yaml: | 
    plugins:
      logs:
        cloudwatch-enabled: false
        kubernetes-enabled: true
  enabled_plugins.yaml: |
    tasks:
      task-plugins:
        default-for-task-types:
          container: container
          container_array: k8s-array
          sidecar: sidecar
        enabled-plugins:
        - container
        - sidecar
        - k8s-array
        - agent-service
        - echo
  config.yaml: |
    executor:
      cluster: ''
      evaluatorCount: 64
      maxActions: 2000
      organization: ''
      unionAuth:
        injectSecret: true
        secretName: EAGER_API_KEY
      workerName: worker1
      task_resources:
        defaults:
          cpu: 100m
          memory: 500Mi
        limits:
          cpu: 2
          gpu: 1
          memory: 1Gi
    union:
      connection:
        host: dns:///
      auth:
        authorizationMetadataKey: flyte-authorization
        clientId: 'dataplane-operator'
        clientSecretLocation: /etc/union/secret/client_secret
        tokenRefreshWindow: 5m
        type: ClientSecret
    admin:
      clientId: 'dataplane-operator'
      clientSecretLocation: /etc/union/secret/client_secret
      endpoint: dns:///
      insecure: false
    authorizer:
      type: noop
    catalog-cache:
      cache-endpoint: dns:///
      endpoint: dns:///
      insecure: false
      type: fallback
      use-admin-auth: true
    logger:
      level: 4
      show-source: true
    sharedService:
      metrics:
        scope: 'executor:'
      security:
        allowCors: true
        allowLocalhostAccess: true
        allowedHeaders:
        - Content-Type
        allowedOrigins:
        - '*'
        secure: false
        useAuth: false
    propeller:
      node-config:
        disable-input-file-writes: true
    plugins:
      fasttask:
        additional-worker-args:
        - --last-ack-grace-period-seconds
        - "120"
        callback-uri: http://unionai-dataplane-executor.union.svc.cluster.local:15605
        grace-period-status-not-found: 2m
      ioutils:
        remoteFileOutputPaths:
          deckFilename: report.html
      k8s:
        disable-inject-owner-references: true
        default-cpus: 100m
        default-env-vars: []
        default-memory: 100Mi
        co-pilot:
          image: 'cr.flyte.org/flyteorg/flytecopilot:v1.14.1'
          name: flyte-copilot-
          start-timeout: 30s
    storage:
      container: 
      type: stow
      stow:
        kind: s3
        config:
          auth_type: accesskey
          access_key_id: 
          secret_key: 
          disable_ssl: false
          endpoint: 
          region: us-east-1
      enable-multicontainer: false
      limits:
        maxDownloadMBs: 1024
      cache:
        max_size_mbs: 0
        target_gc_percent: 70
---
# Source: dataplane/templates/operator/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: union-operator
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
data:
  k8s.yaml: | 
    plugins:
      k8s:
        default-cpus: 100m
        default-env-vars: []
        default-memory: 100Mi
  config.yaml: |
    union:
      connection: 
        host: dns:///
      auth: 
        authorizationMetadataKey: flyte-authorization
        clientId: 'dataplane-operator'
        clientSecretLocation: /etc/union/secret/client_secret
        tokenRefreshWindow: 5m
        type: ClientSecret
    sharedService: 
      features:
        gatewayV2: true
      port: 8081
    authorizer: 
      type: noop
    operator:
      enabled: true
      enableTunnelService: true
      tunnel:
        enableDirectToAppIngress: false
      apps: 
        enabled: 'false'
      syncClusterConfig: 
        enabled: false
      clusterId: 
        organization: ''
      clusterData:
        appId: 'dataplane-operator'
        bucketName: ''
        bucketRegion: 'us-east-1'
        cloudHostName: ''
        gcpProjectId: ''
        metadataBucketPrefix: s3://
        userRole: 'arn:aws:iam::ACCOUNT_ID:role/flyte_project_role'
        userRoleKey: 'eks.amazonaws.com/role-arn'
      collectUsages: 
        enabled: true
      billableUsageCollector: 
        enabled: true
      dependenciesHeartbeat: 
        prometheus:
          endpoint: 'http://union-operator-prometheus:80/-/healthy'
        propeller:
          endpoint: 'http://flytepropeller:10254'
        proxy:
          endpoint: 'http://union-operator-proxy:10254'
    proxy: 
      imageBuilderConfig:
        authenticationType: 'noop'
        defaultRepository: ''
      persistedLogs:
        objectStore:
          pathTemplate: namespace-{{.KubernetesNamespace}}.pod-{{.KubernetesPodName}}.cont-{{.KubernetesContainerName}}
          prefix: persisted-logs
        sourceType: ObjectStore
      smConfig:
        enabled: 'true'
        k8sConfig:
          namespace: 'union'
        type: 'K8s'
  logger.yaml: |
    logger: 
      level: 4
      show-source: true
  config-overrides.yaml: | 
    cache:
      identity:
        enabled: false
  storage.yaml: | 
    storage:
      container: 
      type: stow
      stow:
        kind: s3
        config:
          auth_type: accesskey
          access_key_id: 
          secret_key: 
          disable_ssl: false
          endpoint: 
          region: us-east-1
      enable-multicontainer: false
      limits:
        maxDownloadMBs: 1024
      cache:
        max_size_mbs: 0
        target_gc_percent: 70
  fast_registration_storage.yaml: | 
    fastRegistrationStorage:
      container: ""
      type: stow
      stow:
        kind: s3
        config:
          auth_type: accesskey
          access_key_id: 
          secret_key: 
          disable_ssl: false
          endpoint: 
          region: us-east-1
---
# Source: dataplane/templates/propeller/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: flyte-propeller-config
  namespace: union
data:
  admin.yaml: | 
    admin:
      clientId: 'dataplane-operator'
      clientSecretLocation: /etc/union/secret/client_secret
      endpoint: dns:///
      insecure: false
    event:
      capacity: 1000
      rate: 500
      type: admin
  catalog.yaml: | 
    catalog-cache:
      cache-endpoint: dns:///
      endpoint: dns:///
      insecure: false
      type: fallback
      use-admin-auth: true
  copilot.yaml: | 
    plugins:
      k8s:
        co-pilot:
          image: 'cr.flyte.org/flyteorg/flytecopilot:v1.14.1'
          name: flyte-copilot-
          start-timeout: 30s
  core.yaml: | 
    propeller:
      downstream-eval-duration: 30s
      enable-admin-launcher: true
      leader-election:
        enabled: true
        lease-duration: 15s
        lock-config-map:
          name: propeller-leader
          namespace: union
        renew-deadline: 10s
        retry-period: 2s
      limit-namespace: all
      literal-offloading-config:
        enabled: true
      max-workflow-retries: 30
      metadata-prefix: metadata/propeller
      metrics-prefix: flyte
      prof-port: 10254
      queue:
        batch-size: -1
        batching-interval: 2s
        queue:
          base-delay: 5s
          capacity: 1000
          max-delay: 120s
          rate: 100
          type: maxof
        sub-queue:
          capacity: 100
          rate: 10
          type: bucket
        type: batch
      rawoutput-prefix: s3://
      workers: 4
      workflow-reeval-duration: 30s
    webhook:
      certDir: /etc/webhook/certs
      embeddedSecretManagerConfig:
        imagePullSecrets:
          enabled: true
        k8sConfig:
          namespace: 'union'
        type: 'K8s'
      secretManagerTypes:
      - Embedded
      - K8s
      serviceName: flyte-pod-webhook
  enabled_plugins.yaml: | 
    tasks:
      task-plugins:
        default-for-task-types:
          container: container
          container_array: k8s-array
          sidecar: sidecar
        enabled-plugins:
        - container
        - sidecar
        - k8s-array
        - agent-service
        - echo
  k8s.yaml: | 
    plugins:
      k8s:
        default-cpus: 100m
        default-env-vars: []
        default-memory: 100Mi
  logger.yaml: |
    logger: 
      level: 4
      show-source: true
  resource_manager.yaml: | 
    propeller:
      resourcemanager:
        type: noop
  task_logs.yaml: | 
    plugins:
      logs:
        cloudwatch-enabled: false
        dynamic-log-links:
        - vscode:
            displayName: VS Code Debugger
            templateUris:
            - /dataplane/pod/v1/generated_name/task/{{.executionProject}}/{{.executionDomain}}/{{.executionName}}/{{.nodeID}}/{{.taskRetryAttempt}}/{{.taskProject}}/{{.taskDomain}}/{{.taskID}}/{{.taskVersion}}/
        kubernetes-enabled: false
        templates:
        - displayName: Task Logs
          scheme: TaskExecution
          templateUris:
          - /console/projects/{{.executionProject}}/domains/{{.executionDomain}}/executions/{{.executionName}}/nodeId/{{.nodeID}}/taskId/{{.taskID}}/attempt/{{.taskRetryAttempt}}/view/logs?duration=all&fromExecutionNav=true
  storage.yaml: | 
    storage:
      container: 
      type: stow
      stow:
        kind: s3
        config:
          auth_type: accesskey
          access_key_id: 
          secret_key: 
          disable_ssl: false
          endpoint: 
          region: us-east-1
      enable-multicontainer: false
      limits:
        maxDownloadMBs: 1024
      cache:
        max_size_mbs: 0
        target_gc_percent: 70
---
# Source: dataplane/charts/fluentbit/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-fluentbit
  labels:
    helm.sh/chart: fluentbit-0.48.9
    app.kubernetes.io/name: fluentbit
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "3.2.8"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - namespaces
      - pods
    verbs:
      - get
      - list
      - watch
---
# Source: dataplane/charts/opencost/templates/clusterrole.yaml
# Cluster role giving opencost to get, list, watch required resources
# No write permissions are required
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: opencost
  labels:
    helm.sh/chart: opencost-1.42.0
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.111.0"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups: [""]
    resources:
      - configmaps
      - deployments
      - nodes
      - pods
      - services
      - resourcequotas
      - replicationcontrollers
      - limitranges
      - persistentvolumeclaims
      - persistentvolumes
      - namespaces
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
    resources:
      - daemonsets
      - deployments
      - replicasets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - statefulsets
      - deployments
      - daemonsets
      - replicasets
    verbs:
      - list
      - watch
  - apiGroups:
      - batch
    resources:
      - cronjobs
      - jobs
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - autoscaling
    resources:
      - horizontalpodautoscalers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - storage.k8s.io
    resources:
      - storageclasses
    verbs:
      - get
      - list
      - watch
---
# Source: dataplane/charts/prometheus/charts/kube-state-metrics/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-5.33.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.15.0"
    release: release-name
  name: release-name-kube-state-metrics
rules:

- apiGroups: ["certificates.k8s.io"]
  resources:
  - certificatesigningrequests
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - cronjobs
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - daemonsets
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - deployments
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - endpoints
  verbs: ["list", "watch"]

- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "networking.k8s.io"]
  resources:
  - ingresses
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - jobs
  verbs: ["list", "watch"]

- apiGroups: ["coordination.k8s.io"]
  resources:
  - leases
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - limitranges
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - mutatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - namespaces
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - networkpolicies
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - nodes
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumeclaims
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumes
  verbs: ["list", "watch"]

- apiGroups: ["policy"]
  resources:
    - poddisruptionbudgets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - pods
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - replicasets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - replicationcontrollers
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - resourcequotas
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - secrets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - services
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - statefulsets
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - storageclasses
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - validatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - volumeattachments
  verbs: ["list", "watch"]
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus-operator
  labels:
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app: prometheus-operator
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator
rules:
- apiGroups:
  - monitoring.coreos.com
  resources:
  - alertmanagers
  - alertmanagers/finalizers
  - alertmanagers/status
  - alertmanagerconfigs
  - prometheuses
  - prometheuses/finalizers
  - prometheuses/status
  - prometheusagents
  - prometheusagents/finalizers
  - prometheusagents/status
  - thanosrulers
  - thanosrulers/finalizers
  - thanosrulers/status
  - scrapeconfigs
  - servicemonitors
  - podmonitors
  - probes
  - prometheusrules
  verbs:
  - '*'
- apiGroups:
  - apps
  resources:
  - statefulsets
  verbs:
  - '*'
- apiGroups:
  - ""
  resources:
  - configmaps
  - secrets
  verbs:
  - '*'
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - delete
- apiGroups:
  - ""
  resources:
  - services
  - services/finalizers
  - endpoints
  verbs:
  - get
  - create
  - update
  - delete
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - patch
  - create
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - storage.k8s.io
  resources:
  - storageclasses
  verbs:
  - get
---
# Source: dataplane/charts/prometheus/templates/prometheus/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: union-operator-prometheus
  labels:
    app: prometheus-prometheus
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
rules:
# This permission are not in the kube-prometheus repo
# they're grabbed from https://github.com/prometheus/prometheus/blob/master/documentation/examples/rbac-setup.yml
- apiGroups: [""]
  resources:
  - nodes
  - nodes/metrics
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: ["discovery.k8s.io"]
  resources:
  - endpointslices
  verbs: ["get", "list", "watch"]
- apiGroups:
  - "networking.k8s.io"
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics", "/metrics/cadvisor"]
  verbs: ["get"]
---
# Source: dataplane/templates/clusterresourcesync/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: clustersync-resource
rules:
  - apiGroups:
      - ""
      - rbac.authorization.k8s.io
    resources:
      - configmaps
      - namespaces
      - pods
      - resourcequotas
      - roles
      - rolebindings
      - secrets
      - services
      - serviceaccounts
      - clusterrolebindings
    verbs:
      - '*'
---
# Source: dataplane/templates/operator/serviceaccount-proxy.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: proxy-system
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - '*'
    resources:
      - events
      - flyteworkflows
      - pods/log
      - pods
      - rayjobs
      - resourcequotas
    verbs:
      - get
      - list
      - watch
---
# Source: dataplane/templates/operator/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: operator-system
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
rules:
  # Allow Access to all resources under flyte.lyft.com
  - apiGroups:
      - flyte.lyft.com
    resources:
      - flyteworkflows
      - flyteworkflows/finalizers
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - patch
      - post
      - deletecollection
  - apiGroups:
      - '*'
    resources:
      - resourcequotas
      - pods
      - configmaps
      - podtemplates
      - secrets
      - namespaces
      - nodes
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
---
# Source: dataplane/templates/propeller/serviceaccount-webhook.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: flytepropeller-webhook-role
  namespace: union
rules:
  - apiGroups:
      - "*"
    resources:
      - mutatingwebhookconfigurations
      - secrets
      - pods
      - replicasets/finalizers
    verbs:
      - get
      - create
      - update
      - patch
---
# Source: dataplane/templates/propeller/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: flytepropeller-role
rules:
  # Allow RO access to PODS
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
  # Allow Event recording access
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - update
      - delete
      - patch
  # Allow Access All plugin objects
  - apiGroups:
      - '*'
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - patch
  # Allow Access to CRD
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - update
  # Allow Access to all resources under flyte.lyft.com
  - apiGroups:
      - flyte.lyft.com
    resources:
      - flyteworkflows
      - flyteworkflows/finalizers
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - patch
      - post
      - deletecollection
---
# Source: dataplane/charts/fluentbit/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-fluentbit
  labels:
    helm.sh/chart: fluentbit-0.48.9
    app.kubernetes.io/name: fluentbit
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "3.2.8"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-fluentbit
subjects:
  - kind: ServiceAccount
    name: fluentbit-system
    namespace: union
---
# Source: dataplane/charts/opencost/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: opencost
  labels:
    helm.sh/chart: opencost-1.42.0
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.111.0"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: opencost
subjects:
  - kind: ServiceAccount
    name: opencost
    namespace: union
---
# Source: dataplane/charts/prometheus/charts/kube-state-metrics/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-5.33.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.15.0"
    release: release-name
  name: release-name-kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-kube-state-metrics
subjects:
- kind: ServiceAccount
  name: release-name-kube-state-metrics
  namespace: kube-system
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus-operator
  labels:
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app: prometheus-operator
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus-operator
subjects:
- kind: ServiceAccount
  name: prometheus-operator
  namespace: union
---
# Source: dataplane/charts/prometheus/templates/prometheus/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: union-operator-prometheus
  labels:
    app: prometheus-prometheus
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: union-operator-prometheus
subjects:
  - kind: ServiceAccount
    name: union-operator-prometheus
    namespace: union
---
# Source: dataplane/templates/clusterresourcesync/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: clustersync-resource
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: clustersync-resource
subjects:
  - kind: ServiceAccount
    name: clustersync-system
    namespace: union
---
# Source: dataplane/templates/clusterresourcesync/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: clustersync-auth-delegator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
  - kind: ServiceAccount
    name: clustersync-system
    namespace: union
---
# Source: dataplane/templates/operator/serviceaccount-proxy.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: proxy-system
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: proxy-system
subjects:
  - kind: ServiceAccount
    name: proxy-system
    namespace: union
---
# Source: dataplane/templates/operator/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: operator-system
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: operator-system
subjects:
  - kind: ServiceAccount
    name: operator-system
    namespace: union
---
# Source: dataplane/templates/propeller/serviceaccount-webhook.yaml
# Create a binding from Role -> ServiceAccount
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: flytepropeller-webhook-binding
  namespace: union
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flytepropeller-webhook-role
subjects:
  - kind: ServiceAccount
    name: flytepropeller-webhook-system
    namespace: union
---
# Source: dataplane/templates/propeller/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: flytepropeller-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flytepropeller-role
subjects:
  - kind: ServiceAccount
    name: flytepropeller-system
    namespace: union
---
# Source: dataplane/charts/dcgm-exporter/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: dcgm-exporter-read-cm
  namespace: union
  labels:
    helm.sh/chart: dcgm-exporter-4.1.0
    app.kubernetes.io/name: dcgm-exporter
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "4.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "dcgm-exporter"
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["exporter-metrics-config-map"]
  verbs: ["get"]
---
# Source: dataplane/templates/operator/serviceaccount-proxy-secret.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: proxy-system-secret
  namespace: union
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - '*'
    resources:
      - secrets
    verbs:
      - get
      - list
      - create
      - update
      - delete
---
# Source: dataplane/templates/operator/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: operator-system
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - '*'
    resources:
      - secrets
      - deployments
    verbs:
      - get
      - list
      - watch
      - create
      - update
---
# Source: dataplane/charts/dcgm-exporter/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-dcgm-exporter
  namespace: union
  labels:
    helm.sh/chart: dcgm-exporter-4.1.0
    app.kubernetes.io/name: dcgm-exporter
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "4.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "dcgm-exporter"
subjects:
- kind: ServiceAccount
  name: release-name-dcgm-exporter
  namespace: union
roleRef:
  kind: Role 
  name: dcgm-exporter-read-cm
  apiGroup: rbac.authorization.k8s.io
---
# Source: dataplane/templates/operator/serviceaccount-proxy-secret.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: proxy-system-secret
  namespace: union
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: proxy-system-secret
subjects:
  - kind: ServiceAccount
    name: proxy-system
    namespace: union
---
# Source: dataplane/templates/operator/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: operator-system
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: operator-system
subjects:
  - kind: ServiceAccount
    name: operator-system
    namespace: union
---
# Source: dataplane/charts/dcgm-exporter/templates/service.yaml
# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: v1
kind: Service
metadata:
  name: release-name-dcgm-exporter
  namespace: union
  labels:
    helm.sh/chart: dcgm-exporter-4.1.0
    app.kubernetes.io/name: dcgm-exporter
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "4.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "dcgm-exporter"
spec:
  type: ClusterIP
  ports:
  - name: "metrics"
    port: 9400
    targetPort: 9400
    protocol: TCP
  selector:
    app.kubernetes.io/name: dcgm-exporter
    app.kubernetes.io/instance: release-name
---
# Source: dataplane/charts/fluentbit/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-fluentbit
  namespace: union
  labels:
    helm.sh/chart: fluentbit-0.48.9
    app.kubernetes.io/name: fluentbit
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "3.2.8"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 2020
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: fluentbit
    app.kubernetes.io/instance: release-name
---
# Source: dataplane/charts/opencost/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: opencost
  namespace: union
  labels:
    helm.sh/chart: opencost-1.42.0
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.111.0"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: release-name
  type: "ClusterIP"
  ports:
    - name: http
      port: 9003
      targetPort: 9003
---
# Source: dataplane/charts/prometheus/charts/kube-state-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-kube-state-metrics
  namespace: kube-system
  labels:    
    helm.sh/chart: kube-state-metrics-5.33.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.15.0"
    release: release-name
  annotations:
spec:
  type: "ClusterIP"
  ports:
  - name: "http"
    protocol: TCP
    port: 8080
    targetPort: 8080
  
  selector:    
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
---
# Source: dataplane/charts/prometheus/templates/exporters/core-dns/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator-coredns
  labels:
    app: prometheus-coredns
    jobLabel: coredns
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
  namespace: kube-system
spec:
  clusterIP: None
  ports:
    - name: http-metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
  selector:
    k8s-app: kube-dns
---
# Source: dataplane/charts/prometheus/templates/exporters/kube-controller-manager/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator-kube-controller-manager
  labels:
    app: prometheus-kube-controller-manager
    jobLabel: kube-controller-manager
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
  namespace: kube-system
spec:
  clusterIP: None
  ports:
    - name: http-metrics
      port: 10257
      protocol: TCP
      targetPort: 10257
  selector:
    component: kube-controller-manager
  type: ClusterIP
---
# Source: dataplane/charts/prometheus/templates/exporters/kube-etcd/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator-kube-etcd
  labels:
    app: prometheus-kube-etcd
    jobLabel: kube-etcd
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
  namespace: kube-system
spec:
  clusterIP: None
  ports:
    - name: http-metrics
      port: 2381
      protocol: TCP
      targetPort: 2381
  selector:
    component: etcd
  type: ClusterIP
---
# Source: dataplane/charts/prometheus/templates/exporters/kube-proxy/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator-kube-proxy
  labels:
    app: prometheus-kube-proxy
    jobLabel: kube-proxy
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
  namespace: kube-system
spec:
  clusterIP: None
  ports:
    - name: http-metrics
      port: 10249
      protocol: TCP
      targetPort: 10249
  selector:
    k8s-app: kube-proxy
  type: ClusterIP
---
# Source: dataplane/charts/prometheus/templates/exporters/kube-scheduler/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator-kube-scheduler
  labels:
    app: prometheus-kube-scheduler
    jobLabel: kube-scheduler
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
  namespace: kube-system
spec:
  clusterIP: None
  ports:
    - name: http-metrics
      port: 10259
      protocol: TCP
      targetPort: 10259
  selector:
    component: kube-scheduler
  type: ClusterIP
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: prometheus-operator
  namespace: union
  labels:
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app: prometheus-operator
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator
spec:
  ports:
  - name: https
    port: 443
    targetPort: https
  selector:
    app: prometheus-operator
    release: "release-name"
  type: "ClusterIP"
---
# Source: dataplane/charts/prometheus/templates/prometheus/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator-prometheus
  namespace: union
  labels:
    app: prometheus-prometheus
    self-monitor: "true"
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
spec:
  ports:
  - name: http-web
    port: 80
    targetPort: 9090
  - name: reloader-web
    appProtocol: http
    port: 8080
    targetPort: reloader-web
  publishNotReadyAddresses: false
  selector:
    app.kubernetes.io/name: prometheus
    operator.prometheus.io/name: union-operator-prometheus
  sessionAffinity: None
  type: "ClusterIP"
---
# Source: dataplane/templates/nodeexecutor/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-dataplane-executor
  labels:
    app: executor
spec:
  type: ClusterIP
  ports:
    - port: 15605
      targetPort: 15605
      protocol: TCP
      name: fasttask
  selector:
    app: executor
---
# Source: dataplane/templates/operator/service-proxy.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator-proxy
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
    - port: 10254
      targetPort: debug
      protocol: TCP
      name: debug
  selector:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
---
# Source: dataplane/templates/operator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: debug
      protocol: TCP
      name: debug
  selector:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
---
# Source: dataplane/templates/propeller/service-webhook.yaml
apiVersion: v1
kind: Service
metadata:
  name: flyte-pod-webhook
  namespace: union
  labels: 
    app.kubernetes.io/name: flyte-pod-webhook
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
  annotations: 
    projectcontour.io/upstream-protocol.h2c: grpc
spec:
  selector: 
    app.kubernetes.io/name: flyte-pod-webhook
    app.kubernetes.io/instance: release-name
  ports:
    - name: https
      protocol: TCP
      port: 443
      targetPort: 9443
    - name: debug
      protocol: TCP
      port: 10254
      targetPort: 10254
---
# Source: dataplane/templates/propeller/service.yaml
apiVersion: v1
kind: Service
metadata:
  namespace: union
  name: flytepropeller
  labels: 
    app.kubernetes.io/name: flytepropeller
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: debug
      protocol: TCP
      port: 10254
    - name: fasttask
      port: 15605
      protocol: TCP
      targetPort: 15605
  selector: 
    app.kubernetes.io/name: flytepropeller
    app.kubernetes.io/instance: release-name
---
# Source: dataplane/charts/dcgm-exporter/templates/daemonset.yaml
# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-dcgm-exporter
  namespace: union
  labels:
    helm.sh/chart: dcgm-exporter-4.1.0
    app.kubernetes.io/name: dcgm-exporter
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "4.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "dcgm-exporter"
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: dcgm-exporter
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: "dcgm-exporter"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: dcgm-exporter
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/component: "dcgm-exporter"
    spec:
      priorityClassName: system-node-critical
      serviceAccountName: release-name-dcgm-exporter
      volumes:
      - name: "pod-gpu-resources"
        hostPath:
          path: /var/lib/kubelet/pod-resources
      - configMap:
          items:
          - key: metrics
            path: default-counters.csv
          name: exporter-metrics-config-map
        name: exporter-metrics-volume
      containers:
      - name: exporter
        securityContext:
          capabilities:
            add:
            - SYS_ADMIN
          runAsNonRoot: false
          runAsUser: 0
        image: "nvcr.io/nvidia/k8s/dcgm-exporter:4.2.0-4.1.0-ubuntu22.04"
        imagePullPolicy: "IfNotPresent"
        args:
        - -f
        - /etc/dcgm-exporter/default-counters.csv
        env:
        - name: "DCGM_EXPORTER_KUBERNETES"
          value: "true"
        - name: "DCGM_EXPORTER_LISTEN"
          value: ":9400"
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        ports:
        - name: "metrics"
          containerPort: 9400
        volumeMounts:
        - name: "pod-gpu-resources"
          readOnly: true
          mountPath: "/var/lib/kubelet/pod-resources"
        - mountPath: /etc/dcgm-exporter/default-counters.csv
          name: exporter-metrics-volume
          subPath: default-counters.csv
        livenessProbe:
          httpGet:
            path: /health
            port: 9400
            scheme: HTTP
          initialDelaySeconds: 45
          periodSeconds: 5
        readinessProbe:
          httpGet:
            path: /health
            port: 9400
            scheme: HTTP
          initialDelaySeconds: 45
---
# Source: dataplane/charts/fluentbit/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-fluentbit
  namespace: union
  labels:
    helm.sh/chart: fluentbit-0.48.9
    app.kubernetes.io/name: fluentbit
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "3.2.8"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: fluentbit
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: fluentbit
        app.kubernetes.io/instance: release-name
      annotations:
        checksum/config: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
    spec:
      serviceAccountName: fluentbit-system
      hostNetwork: false
      dnsPolicy: ClusterFirst
      containers:
        - name: fluentbit
          image: "cr.fluentbit.io/fluent/fluent-bit:3.2.8"
          imagePullPolicy: IfNotPresent
          command:
            - /fluent-bit/bin/fluent-bit
          args:
            - --workdir=/fluent-bit/etc
            - --config=/fluent-bit/etc/conf/fluent-bit.conf
          ports:
            - name: http
              containerPort: 2020
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /api/v1/health
              port: http
          volumeMounts:
            - name: config
              mountPath: /fluent-bit/etc/conf
            - mountPath: /var/log
              name: varlog
            - mountPath: /var/lib/docker/containers
              name: varlibdockercontainers
              readOnly: true
            - mountPath: /etc/machine-id
              name: etcmachineid
              readOnly: true
      volumes:
        - name: config
          configMap:
            name: fluentbit-system
        - hostPath:
            path: /var/log
          name: varlog
        - hostPath:
            path: /var/lib/docker/containers
          name: varlibdockercontainers
        - hostPath:
            path: /etc/machine-id
            type: File
          name: etcmachineid
      tolerations:
        - operator: Exists
---
# Source: dataplane/charts/opencost/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opencost
  namespace: union
  labels:
    helm.sh/chart: opencost-1.42.0
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.111.0"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: opencost
      app.kubernetes.io/instance: release-name
  strategy: 
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: opencost
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: opencost
      containers:
        - name: opencost
          image: ghcr.io/opencost/opencost:1.111.0@sha256:6aa68e52a24b14ba41f23db08d1b9db1429a1c0300f4c0381ecc2c61fc311a97
          imagePullPolicy: IfNotPresent
          args:
          ports:
            - containerPort: 9003
              name: http
          resources:
            limits:
              cpu: 1000m
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 1Gi
          startupProbe:
            httpGet:
              path: /healthz
              port: 9003
            initialDelaySeconds: 10
            periodSeconds: 5
            failureThreshold: 30
          livenessProbe:
            httpGet:
              path: /healthz
              port: 9003
            initialDelaySeconds: 10
            periodSeconds: 20
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /healthz
              port: 9003
            initialDelaySeconds: 10
            periodSeconds: 10
            failureThreshold: 3
          env:
            - name: LOG_LEVEL
              value: info
            - name: CUSTOM_COST_ENABLED
              value: "false"
            - name: KUBECOST_NAMESPACE
              value: union
            - name: API_PORT
              value: "9003"
            - name: PROMETHEUS_SERVER_ENDPOINT
              value: "http://union-operator-prometheus.union.svc:80/prometheus"
            - name: CLUSTER_ID
              value: "default-cluster"
            - name: DATA_RETENTION_DAILY_RESOLUTION_DAYS
              value: "15"
            - name: CLOUD_COST_ENABLED
              value: "false"
            - name: CLOUD_COST_MONTH_TO_DATE_INTERVAL
              value: "6"
            - name: CLOUD_COST_REFRESH_RATE_HOURS
              value: "6"
            - name: CLOUD_COST_QUERY_WINDOW_DAYS
              value: "7"
            - name: CLOUD_COST_RUN_WINDOW_DAYS
              value: "3"
            # Add any additional provided variables
---
# Source: dataplane/charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-kube-state-metrics
  namespace: kube-system
  labels:    
    helm.sh/chart: kube-state-metrics-5.33.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.15.0"
    release: release-name
spec:
  selector:
    matchLabels:      
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/instance: release-name
  replicas: 1
  strategy:
    type: RollingUpdate
  revisionHistoryLimit: 10
  template:
    metadata:
      labels:        
        helm.sh/chart: kube-state-metrics-5.33.2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: kube-state-metrics
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "2.15.0"
        release: release-name
    spec:
      automountServiceAccountToken: true
      hostNetwork: false
      serviceAccountName: release-name-kube-state-metrics
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      dnsPolicy: ClusterFirst
      containers:
      - name: kube-state-metrics
        args:
        - --port=8080
        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
        imagePullPolicy: IfNotPresent
        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.15.0
        ports:
        - containerPort: 8080
          name: "http"
        livenessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders:
            path: /livez
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders:
            path: /readyz
            port: 8081
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          {}
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-operator
  namespace: union
  labels:
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app: prometheus-operator
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: prometheus-operator
      release: "release-name"
  template:
    metadata:
      labels:
        
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "72.9.1"
        app.kubernetes.io/part-of: prometheus
        chart: prometheus-72.9.1
        release: "release-name"
        heritage: "Helm"
        app: prometheus-operator
        app.kubernetes.io/name: prometheus-prometheus-operator
        app.kubernetes.io/component: prometheus-operator
    spec:
      containers:
        - name: prometheus
          image: "quay.io/prometheus-operator/prometheus-operator:v0.82.2"
          imagePullPolicy: "IfNotPresent"
          args:
            - --kubelet-service=kube-system/union-operator-kubelet
            - --kubelet-endpoints=true
            - --kubelet-endpointslice=false
            - --localhost=127.0.0.1
            - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.82.2
            - --config-reloader-cpu-request=0
            - --config-reloader-cpu-limit=0
            - --config-reloader-memory-request=0
            - --config-reloader-memory-limit=0
            - --thanos-default-base-image=quay.io/thanos/thanos:v0.38.0
            - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
            - --web.enable-tls=true
            - --web.cert-file=/cert/cert
            - --web.key-file=/cert/key
            - --web.listen-address=:10250
            - --web.tls-min-version=VersionTLS13
          ports:
            - containerPort: 10250
              name: https
          env:
          - name: GOGC
            value: "30"
          resources:
            {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          volumeMounts:
            - name: tls-secret
              mountPath: /cert
              readOnly: true
          readinessProbe:
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
          livenessProbe:
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
      volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: union-operator-admission
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: prometheus-operator
      automountServiceAccountToken: true
      terminationGracePeriodSeconds: 30
---
# Source: dataplane/templates/clusterresourcesync/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: syncresources
  namespace: union
  labels:
    app.kubernetes.io/name: clusterresourcesync
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: clusterresourcesync
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        configChecksum: "3ff04d7a349613c7b6bb961f5dcf5b7bc8a4af57a81392e4bc1ac23aa3ee69d"
        
      labels:
        app.kubernetes.io/name: clusterresourcesync
        app.kubernetes.io/instance: release-name
        platform.union.ai/service-group: release-name
        app.kubernetes.io/managed-by: Helm
    spec:
      containers:
        - command:
            - clusterresource
            - --config
            - /etc/flyte/config/*.yaml
            - clusterresource
            - run
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.cpu
          - name: CLUSTER_NAME
            valueFrom:
              secretKeyRef:
                name: operator-cluster-name
                key: cluster_name
          - name: DEPLOYMENT_NAME
            value: operator
          - name: PROXY_SERVICE_URL
            value: http://union-operator-proxy:8080
          - name: PROMETHEUS_SERVICE_URL
            value: http://union-operator-prometheus:80
          - name: KNATIVE_PROXY_SERVICE_URL
            value: http://kourier-internal
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2025.10.2"
          imagePullPolicy: "IfNotPresent"
          name: sync-cluster-resources
          resources:
            limits:
              cpu: "1"
              memory: 500Mi
            requests:
              cpu: 500m
              memory: 100Mi
          volumeMounts:
            - name: auth
              mountPath: /etc/union/secret
            - name: resource-templates
              mountPath: /etc/flyte/clusterresource/templates
            - name: config-volume
              mountPath: /etc/flyte/config
          ports:
            - containerPort: 10254
      serviceAccountName: clustersync-system
      volumes:
        - configMap:
            name: clusterresource-template
          name: resource-templates
        - configMap:
            name: flyte-clusterresourcesync-config
          name: config-volume
        - name: auth
          secret:
            secretName: union-secret-auth
---
# Source: dataplane/templates/nodeexecutor/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: executor
  namespace: union
  labels:
    app: executor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: executor
  template:
    metadata:
      labels:
        app: executor
    spec:
      securityContext:
        fsGroup: 1337
      serviceAccountName: flytepropeller-system # TODO: update `executor` ServiceAccount to have required permissions
      volumes:
        - name: config-volume
          configMap:
            name: executor
        - name: secret-volume
          secret:
            secretName: union-secret-auth
        - name: auth
          secret:
            secretName: union-secret-auth
      containers:
        - name: executor
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2025.10.2"
          imagePullPolicy: IfNotPresent
          command:
            - executor
            - serve
            - --config
            - /etc/config/*.yaml
          ports:
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: metrics
              containerPort: 10254
              protocol: TCP
          env:
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.memory
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.cpu
          resources:
            limits:
              cpu:    "4"
              memory: "8Gi"
            requests:
              cpu:    "2"
              memory: "4Gi"
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: secret-volume
              mountPath: /etc/union/secret
            - name: auth
              mountPath: /etc/secrets/
---
# Source: dataplane/templates/operator/deployment-proxy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: union-operator-proxy
  namespace: union
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: operator-proxy
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        configChecksum: "23d0ab6c00d43ff00e1ba4a4127e29a5ade34bd4033838cfe231788d81787e0"
        
      labels:
        app.kubernetes.io/name: operator-proxy
        app.kubernetes.io/instance: release-name
        platform.union.ai/service-group: release-name
        app.kubernetes.io/managed-by: Helm
    spec:
      volumes:
        - name: config-volume
          projected:
            sources:
            - configMap:
                name: union-operator
            - configMap:
                name: flyte-clusterresourcesync-config
        - name: secret-volume
          secret:
            secretName: union-secret-auth
      priorityClassName: 
      serviceAccountName: proxy-system
      securityContext:
        {}
      containers:
        - name: operator-proxy
          securityContext:
            {}
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2025.10.2"
          imagePullPolicy: IfNotPresent
          terminationMessagePolicy: FallbackToLogsOnError
          resources:
            limits:
              cpu: "4"
              memory: 8Gi
            requests:
              cpu: "4"
              memory: 8Gi
          volumeMounts:
            - mountPath: /etc/union/config
              name: config-volume
            - mountPath: /etc/union/secret
              name: secret-volume
          args:
            - operator
            - proxy
            - --config
            - /etc/union/config/*.yaml
          ports:
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: connect
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 8081
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.cpu
          - name: CLUSTER_NAME
            valueFrom:
              secretKeyRef:
                name: operator-cluster-name
                key: cluster_name
          - name: DEPLOYMENT_NAME
            value: operator
          - name: PROXY_SERVICE_URL
            value: http://union-operator-proxy:8080
          - name: PROMETHEUS_SERVICE_URL
            value: http://union-operator-prometheus:80
          - name: KNATIVE_PROXY_SERVICE_URL
            value: http://kourier-internal
        - name: "tunnel"
          securityContext:
            {}
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2025.10.2"
          imagePullPolicy: IfNotPresent
          args:
            - cloudflared
            - tunnel
            - --no-autoupdate
            - run
            - --token
            - $(TUNNEL_TOKEN)
          env:
            - name: TUNNEL_TOKEN
              valueFrom:
                secretKeyRef:
                  name: union-secret-auth
                  key: tunnel_token
                  optional: true
---
# Source: dataplane/templates/operator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: union-operator
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: union-operator
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        configChecksum: "23d0ab6c00d43ff00e1ba4a4127e29a5ade34bd4033838cfe231788d81787e0"
        
      labels:
        
        app.kubernetes.io/name: union-operator
        app.kubernetes.io/instance: release-name
        platform.union.ai/service-group: release-name
        app.kubernetes.io/managed-by: Helm
    spec:
      priorityClassName: 
      serviceAccountName: operator-system
      securityContext:
        {}
      volumes:
        - name: config-volume
          configMap:
            name: union-operator
        - name: secret-volume
          secret:
            secretName: union-secret-auth
      containers:
        - name: operator
          securityContext:
            {}
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2025.10.2"
          imagePullPolicy: IfNotPresent
          terminationMessagePolicy: FallbackToLogsOnError
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "2"
              memory: 4Gi
          volumeMounts:
            - mountPath: /etc/union/config
              name: config-volume
            - mountPath: /etc/union/secret
              name: secret-volume
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.cpu
          - name: CLUSTER_NAME
            valueFrom:
              secretKeyRef:
                name: operator-cluster-name
                key: cluster_name
          - name: DEPLOYMENT_NAME
            value: operator
          - name: PROXY_SERVICE_URL
            value: http://union-operator-proxy:8080
          - name: PROMETHEUS_SERVICE_URL
            value: http://union-operator-prometheus:80
          - name: KNATIVE_PROXY_SERVICE_URL
            value: http://kourier-internal
          args:
            - operator
            - serve
            - --config
            - /etc/union/config/*.yaml
            - --operator.clusterId.name
            - "$(CLUSTER_NAME)"
            - --operator.tunnel.k8sSecretName
            - union-secret-auth
          ports:
            - name: grpc
              containerPort: 8080
              protocol: TCP
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
---
# Source: dataplane/templates/propeller/deployment-webhook.yaml
# Create the actual deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flytepropeller-webhook
  namespace: union
  labels:
    app.kubernetes.io/name: flyte-pod-webhook
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flyte-pod-webhook
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        
        app.kubernetes.io/name: flyte-pod-webhook
        app.kubernetes.io/instance: release-name
        platform.union.ai/service-group: release-name
        app.kubernetes.io/managed-by: Helm
      annotations:
        configChecksum: "eac9f00cc5e216ddc3a6320a2986531658244860886ec6ab4da5fd95fe4c394"
        
    spec:
      securityContext: 
        fsGroup: 65534
        fsGroupChangePolicy: Always
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions:
          type: spc_t
      serviceAccountName: flytepropeller-webhook-system
      initContainers:
        - name: generate-secrets
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2025.10.2"
          imagePullPolicy: "IfNotPresent"
          command:
            - flytepropeller
          args:
            - webhook
            - init-certs
            - --config
            - /etc/flyte/config/*.yaml
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.cpu
          - name: CLUSTER_NAME
            valueFrom:
              secretKeyRef:
                name: operator-cluster-name
                key: cluster_name
          - name: DEPLOYMENT_NAME
            value: operator
          - name: PROXY_SERVICE_URL
            value: http://union-operator-proxy:8080
          - name: PROMETHEUS_SERVICE_URL
            value: http://union-operator-prometheus:80
          - name: KNATIVE_PROXY_SERVICE_URL
            value: http://kourier-internal
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
          volumeMounts:
            - name: config-volume
              mountPath: /etc/flyte/config
      containers:
        - name: webhook
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2025.10.2"
          imagePullPolicy: "IfNotPresent"
          command:
            - flytepropeller
          args:
            - webhook
            - --config
            - /etc/flyte/config/*.yaml
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.cpu
          - name: CLUSTER_NAME
            valueFrom:
              secretKeyRef:
                name: operator-cluster-name
                key: cluster_name
          - name: DEPLOYMENT_NAME
            value: operator
          - name: PROXY_SERVICE_URL
            value: http://union-operator-proxy:8080
          - name: PROMETHEUS_SERVICE_URL
            value: http://union-operator-prometheus:80
          - name: KNATIVE_PROXY_SERVICE_URL
            value: http://kourier-internal
          ports:
            - containerPort: 9443
            - containerPort: 10254
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
          resources:
            requests:
              cpu: 200m
              ephemeral-storage: 500Mi
              memory: 500Mi
          volumeMounts:
            - name: config-volume
              mountPath: /etc/flyte/config
              readOnly: true
            - name: webhook-certs
              mountPath: /etc/webhook/certs
              readOnly: true
      volumes:
        - name: config-volume
          configMap:
            name: flyte-propeller-config
        - name: webhook-certs
          secret:
            secretName: flyte-pod-webhook
---
# Source: dataplane/templates/propeller/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: union
  name: flytepropeller
  labels:
    app.kubernetes.io/name: flytepropeller
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flytepropeller
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        configChecksum: "eac9f00cc5e216ddc3a6320a2986531658244860886ec6ab4da5fd95fe4c394"
        
      labels:
        
        
        app.kubernetes.io/name: flytepropeller
        app.kubernetes.io/instance: release-name
        platform.union.ai/service-group: release-name
        app.kubernetes.io/managed-by: Helm
    spec:
      priorityClassName: system-cluster-critical
      containers:
        - command:
            - flytepropeller
            - --config
            - /etc/flyte/config/*.yaml
            - --propeller.cluster-id
            - ''
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.cpu
          - name: CLUSTER_NAME
            valueFrom:
              secretKeyRef:
                name: operator-cluster-name
                key: cluster_name
          - name: DEPLOYMENT_NAME
            value: operator
          - name: PROXY_SERVICE_URL
            value: http://union-operator-proxy:8080
          - name: PROMETHEUS_SERVICE_URL
            value: http://union-operator-prometheus:80
          - name: KNATIVE_PROXY_SERVICE_URL
            value: http://kourier-internal
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2025.10.2"
          imagePullPolicy: "IfNotPresent"
          name: flytepropeller
          ports:
            - containerPort: 10254
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "2"
              memory: 4Gi
          volumeMounts:
            - name: config-volume
              mountPath: /etc/flyte/config
            - name: auth
              mountPath: /etc/union/secret
      serviceAccountName: flytepropeller-system
      volumes:
        - configMap:
            name: flyte-propeller-config
          name: config-volume
        - name: auth
          secret:
            secretName: union-secret-auth
---
# Source: dataplane/charts/dcgm-exporter/templates/tls-secret.yaml
# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
# Source: dataplane/charts/dcgm-exporter/templates/web-config-configmap.yaml
# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/admission-webhooks/mutatingWebhookConfiguration.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name:  union-operator-admission
  annotations:
    
  labels:
    app: prometheus-admission
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator-webhook
webhooks:
  - name: prometheusrulemutate.monitoring.coreos.com
    failurePolicy: Ignore
    rules:
      - apiGroups:
          - monitoring.coreos.com
        apiVersions:
          - "*"
        resources:
          - prometheusrules
        operations:
          - CREATE
          - UPDATE
    clientConfig:
      service:
        namespace: union
        name: prometheus-operator
        path: /admission-prometheusrules/mutate
    timeoutSeconds: 10
    admissionReviewVersions: ["v1", "v1beta1"]
    sideEffects: None
---
# Source: dataplane/charts/prometheus/templates/prometheus/prometheus.yaml
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: union-operator-prometheus
  namespace: union
  labels:
    app: prometheus-prometheus
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
spec:
  automountServiceAccountToken: true
  image: "quay.io/prometheus/prometheus:v3.4.1"
  version: v3.4.1
  externalUrl: http://union-operator-prometheus.union:80
  paused: false
  replicas: 1
  shards: 1
  logLevel:  info
  logFormat:  logfmt
  listenLocal: false
  enableAdminAPI: false
  resources:
    limits:
      cpu: "4"
      memory: 8Gi
    requests:
      cpu: "4"
      memory: 8Gi
  retention: "3d"
  tsdb:
    outOfOrderTimeWindow: 0s
  walCompression: true
  routePrefix: "/prometheus/"
  serviceAccountName: union-operator-prometheus
  serviceMonitorSelector: {}
  serviceMonitorNamespaceSelector: {}
  podMonitorSelector: {}
  podMonitorNamespaceSelector: {}
  probeSelector:
    matchLabels:
      release: "release-name"

  probeNamespaceSelector: {}
  securityContext:
    fsGroup: 2000
    runAsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
    seccompProfile:
      type: RuntimeDefault
  ruleNamespaceSelector: {}
  ruleSelector: {}
  scrapeConfigSelector: {}
  scrapeConfigNamespaceSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          topologyKey: kubernetes.io/hostname
          labelSelector:
            matchExpressions:            
            - {key: app.kubernetes.io/name, operator: In, values: [prometheus]}
            - {key: app.kubernetes.io/instance, operator: In, values: [union-operator-prometheus]}
  portName: http-web
  maximumStartupDurationSeconds: 900
  hostNetwork: false
---
# Source: dataplane/templates/monitoring/prometheusrule.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: union-opencost-rules
  namespace: union
  labels:
    release: release-name
spec:
  groups:
    - name: cost_calculations_15s
      interval: 15s
      rules:
        - record: pod_gpu_allocation
          expr: |
            sum by (namespace, pod) (DCGM_FI_DEV_GPU_UTIL >= bool 0) * on (namespace, pod) group_left() (max by (namespace, pod) (kube_pod_status_phase{phase=~"Running|Pending"} == 1))
        - record: execution_info # A join metric to look up execution-level info. Used to disambiguate workflow/task executions, apps, and workspaces.
          expr: |
            max by (label_domain, label_project, label_entity_name, label_execution_id, label_entity_id)(
              label_replace(
                label_replace(
                  label_replace(
                    label_replace(
                      label_replace(
                        flyte:propeller:all:round:execution_info{domain!="", project!="", workflow_name!="", execution_id!=""}, # filter for workflow/task executions
                        "label_entity_id", "$1", "execution_id", "(.*)" # join key
                      ), "label_entity_name", "$1", "workflow_name", "(.*)" # set label_entity_name to the workflow/task name from the workflow_execution_id
                    ),
                    "label_execution_id", "$1", "execution_id", "(.*)"
                  ),
                  "label_project", "$1", "project", "(.*)" # project
                ),
                "label_domain", "$1", "domain", "(.*)" # domain
              )
            )
        - record: app_info # A join metric to look up app-level info. Used to disambiguate workflow/task executions, apps, and workspaces.
          expr: |
            max by (label_domain, label_project, label_app_name, label_app_version, label_entity_id)(
              label_replace(
                label_replace(
                  label_replace(
                    kube_pod_labels{
                      label_domain!="",
                      label_project!="",
                      label_serving_unionai_dev_app_name!="",
                      label_serving_knative_dev_revision!=""
                    }, # this filters for apps
                    "label_app_name", "$1", "label_serving_unionai_dev_app_name", "(.*)" # rename to cleanup
                  ),
                  "label_app_version", "$1", "label_serving_knative_dev_revision", "(.*)" # the app_version is equivalent to an execution_id for workflows (lowest level of granularity)
                ),
                "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # join key
              )
            )
        - record: workspace_info # A join metric to look up workspace info. Used to disambiguate workflow/task executions, apps, and workspaces.
          expr: |
            max by (label_domain, label_project, label_workspace_name, label_entity_id)(
              label_replace(
                label_replace(
                  kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # filter for workspaces
                  "label_entity_id", "$1", "label_node_id", "(.*)" # join key
                ), "label_workspace_name", "$1", "label_node_id", "(.*)" # set label_workspace_name to the workspace name from the kube_pod_labels
              )
            )
        - record: entity_id:mem_usage_bytes_total_per_node:sum # Allocated memory (max(requested, consumed)) aggregated per node and entity, where entity is either a task/workflow execution or an app.
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, node) ( # aggregate up to entity
              # First, calculate the allocated memory for each pod
              max by (namespace, pod) ( # this is the case where consumed (the memory working set) exceeds requested memory
                (
                  sum by (namespace, pod) (
                    container_memory_working_set_bytes{namespace!="",pod!="",image!=""}
                  )
                  > sum by (namespace, pod) (
                    kube_pod_container_resource_requests{namespace!="", pod!="", node!="", resource="memory"}
                  )
                )
                or sum by (namespace, pod) ( # this is the case where memory requests are <= consumed memory
                  kube_pod_container_resource_requests{namespace!="", pod!="", node!="", resource="memory"} # needed to add node!="" to dedupe
                )
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but we do not want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
              # Now join in node identifiers which are used for subsequent overhead calculations
              * on (namespace, pod) group_left(node) (
                max by (namespace, pod, node) (kube_pod_info{node!=""}) # needed to add node!="" to dedupe
              )
            )
        - record: entity_id:cpu_usage_per_node:sum # Allocated cpu (max(requested, consumed)) aggregated per node and entity, where entity is either a task/workflow execution or an app.
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, node) (
              # First, calculate the allocated cpu for each pod
              max by (namespace, pod) ( # this is the case where consumed (the cpu usage seconds total) exceeds requested cpu
                (
                  sum by (namespace, pod) (
                    irate(container_cpu_usage_seconds_total{namespace!="",pod!="",image!=""}[5m])
                  )
                  > sum by (namespace, pod) (
                    kube_pod_container_resource_requests{namespace!="", pod!="", node!="", resource="cpu"}
                  )
                )
                or sum by (namespace, pod) ( # this is the case where cpu requests are <= consumed cpu
                    kube_pod_container_resource_requests{namespace!="", pod!="", node!="", resource="cpu"}
                )
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but I didn't want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
              # Now join in node identifiers which are used for subsequent overhead calculations
              * on (namespace, pod) group_left(node) (
                max by (namespace, pod, node) (kube_pod_info{node!=""}) # needed to add node!="" to dedupe
              )
            )
        - record: entity_id:gpu_usage_per_node:sum # Allocated gpu aggregated per node and entity, where entity is either a task/workflow execution or an app.
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, node) (
              # First, grab the allocated gpu for each pod (which is always either 1 or zero, since k8s can't split gpus the way it can with cpu/memory)
              max by (namespace, pod) (
                pod_gpu_allocation
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but I didn't want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
              # Now join in node identifiers which are used for subsequent overhead calculations
              * on (namespace, pod) group_left(node) (
                max by (namespace, pod, node) (kube_pod_info{node!=""}) # needed to add node!="" to dedupe
              )
            )
        - record: entity_id:used_mem_bytes:sum # the sum of used memory across all containers in an entity (numerator for aggregate utilization calculations)
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id) ( # aggregate up to entity
              # First, calculate the used memory for each pod
              sum by (namespace, pod) (
                container_memory_working_set_bytes{namespace!="",pod!="",image!=""}
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but we do not want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
            )
        - record: entity_id:allocated_mem_bytes:sum # the sum of allocated memory across all containers in an entity (denominator for aggregate utilization calculations)
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id) ( # aggregate up to entity (remove node)
              entity_id:mem_usage_bytes_total_per_node:sum
            )
        - record: entity_id:used_cpu:sum # the sum of used cpu across all containers in an entity (numerator for aggregate utilization calculations)
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id) (
              sum by (namespace, pod) (
                irate(container_cpu_usage_seconds_total{namespace!="",pod!="",image!=""}[5m])
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but I didn't want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
            )
        - record: entity_id:allocated_cpu:sum # the sum of allocated cpu across all containers in an entity (denominator for aggregate utilization calculations)
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id) ( # aggregate up to entity (remove node)
              entity_id:cpu_usage_per_node:sum
            )
        - record: entity_id:sm_occupancy:avg # the simple average of SM occupancy (a good generic measure of GPU utilization) per entity
          expr: |
            avg by (label_entity_type, label_domain, label_project, label_entity_id) (
              # First, grab the SM occupancy for each pod
              max by (namespace, pod) (
                DCGM_FI_PROF_SM_OCCUPANCY # SM occupancy is a good proxy for actual GPU usage
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but I didn't want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
            )
        - record: entity_id:gpu_count:sum # the count of running gpu pods per entity (need this to weight the gpu utilization when aggregating upwards - i.e. project-level)
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, node) (
              # First, grab the allocated gpu for each pod (which is always either 1 or zero, since k8s can't split gpus the way it can with cpu/memory)
              max by (namespace, pod) (
                pod_gpu_allocation
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but I didn't want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
            )
        - record: entity_id:weighted_sm_occupancy:sum # product of SM occupancy and allocated GPU count (something like "used memory", numerator of weighted calcs)
          expr: |
            entity_id:sm_occupancy:avg
            * on (label_domain, label_project, label_entity_type, label_entity_id) entity_id:gpu_count:sum
        - record: entity_id:allocated_mem_cost:sum # Allocated cost of memory for each workflow/task execution and app.
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, type) (
              entity_id:mem_usage_bytes_total_per_node:sum / (1024 * 1024 * 1024) # convert bytes to GB
              * on (node) group_left(type) label_replace(avg by (node) (node_ram_hourly_cost * (15 / 3600)), "type", "mem", "", "") # convert hourly cost to 15-secondly cost and add type
            )
        - record: entity_id:allocated_cpu_cost:sum # Allocated cost of cpu for each workflow/task execution and app.
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, type)(
              entity_id:cpu_usage_per_node:sum
              * on (node) group_left(type) label_replace(avg by (node) (node_cpu_hourly_cost * (15 / 3600)), "type", "cpu", "", "") # convert hourly cost to 15-secondly cost and add type
            )
        - record: entity_id:allocated_gpu_cost:sum # Allocated cost of gpu for each workflow/task execution and app.
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, type)(
              entity_id:gpu_usage_per_node:sum
              * on (node) group_left(type) label_replace(avg by (node) (node_gpu_hourly_cost * (15 / 3600)), "type", "gpu", "", "") # convert hourly cost to 15-secondly cost and add type
            )
        - record: entity_id:allocated_cost:sum # Allocated cost of memory, cpu, and gpu for each workflow/task execution and app.
          expr: |
            label_replace(
              sum by (label_entity_type, label_domain, label_project, label_entity_id) ( # for the sum to work, the labels need to be different on each "or" element (type label)
                entity_id:allocated_mem_cost:sum
                or
                entity_id:allocated_cpu_cost:sum
                or
                entity_id:allocated_gpu_cost:sum
              ),
              "type", "allocated", "", "" # add type info
            )
        - record: entity_id:overhead_cost:sum # The amount of overhead costs (node costs that we can't allocate with container resources) to allocate to each entity (workflow/task execution or app)
          expr: |
            label_replace(
              sum by (label_entity_type, label_entity_id, label_domain, label_project)( # Aggregate the per-node metrics up to workflow/task execution or app (label_entity_id)
                # Start with each execution's and app's allocated cost per node
                sum by (label_entity_type, label_domain, label_project, label_entity_id, node) ( # for the sum to work, the labels need to be different on each "or" element (type label)
                  entity_id:mem_usage_bytes_total_per_node:sum / (1024 * 1024 * 1024) # convert bytes to GB
                  * on (node) group_left(type) label_replace(avg by (node) (node_ram_hourly_cost * (15 / 3600)), "type", "mem", "", "") # convert hourly cost to 15-secondly cost and add type
                  or
                  entity_id:cpu_usage_per_node:sum
                  * on (node) group_left(type) label_replace(avg by (node) (node_cpu_hourly_cost * (15 / 3600)), "type", "cpu", "", "") # convert hourly cost to 15-secondly cost and add type
                  or
                  entity_id:gpu_usage_per_node:sum
                  * on (node) group_left(type) label_replace(avg by (node) (node_gpu_hourly_cost * (15 / 3600)), "type", "gpu", "", "") # convert hourly cost to 15-secondly cost and add type
                )
                # Then divide out the total allocated cost per node to get the proportion of allocated cost associated with each entity
                / on (node) group_left()(
                  sum by (node) ( # for the sum to work, the labels need to be different on each "or" element (type label)
                    entity_id:mem_usage_bytes_total_per_node:sum / (1024 * 1024 * 1024) # convert bytes to GB
                    * on (node) group_left(type) label_replace(avg by (node) (node_ram_hourly_cost * (15 / 3600)), "type", "mem", "", "") # convert hourly cost to 15-secondly cost and add type
                    or
                    entity_id:cpu_usage_per_node:sum
                    * on (node) group_left(type) label_replace(avg by (node) (node_cpu_hourly_cost * (15 / 3600)), "type", "cpu", "", "") # convert hourly cost to 15-secondly cost and add type
                    or
                    entity_id:gpu_usage_per_node:sum
                    * on (node) group_left(type) label_replace(avg by (node) (node_gpu_hourly_cost * (15 / 3600)), "type", "gpu", "", "") # convert hourly cost to 15-secondly cost and add type
                  )
                  > 0 # need to avoid dividing by zero, or gaps in the data can cause NaNs to proliferate, borking all charts
                )
                # Then multiply by the overhead cost per node
                * on (node) group_left() (
                  # To calculate overhead, start with the true cost of running each node
                  avg by (node)(kube_node_labels{label_flyte_org_node_role="worker"}) # only look at worker nodes
                  * on (node) max by (node) (
                    node_total_hourly_cost{instance_type!=""} # sometimes, the instance_type can be null, causing an unlabeled label to show up in the Compute Costs dashboard charts
                  ) * (15 / 3600) # convert hourly cost to 15-secondly cost
                  # Then subtract out the total allocated cost on each node
                  - on (node) group_left()(
                    sum by (node) ( # for the sum to work, the labels need to be different on each "or" element (type label)
                      entity_id:mem_usage_bytes_total_per_node:sum / (1024 * 1024 * 1024) # convert bytes to GB
                      * on (node) group_left(type) label_replace(avg by (node) (node_ram_hourly_cost * (15 / 3600)), "type", "mem", "", "") # convert hourly cost to 15-secondly cost and add type
                      or
                      entity_id:cpu_usage_per_node:sum
                      * on (node) group_left(type) label_replace(avg by (node) (node_cpu_hourly_cost * (15 / 3600)), "type", "cpu", "", "") # convert hourly cost to 15-secondly cost and add type
                      or
                      entity_id:gpu_usage_per_node:sum
                      * on (node) group_left(type) label_replace(avg by (node) (node_gpu_hourly_cost * (15 / 3600)), "type", "gpu", "", "") # convert hourly cost to 15-secondly cost and add type
                    )
                  )
                )
              ),
              "type", "overhead", "", "" # add type info
            )
        - record: entity_id:total_cost:sum # Total cost of each entity (workflow/task execution or app), including allocated (from container resources) and overhead (proportion of unallocated node costs)
          expr: |
            label_replace(
              sum by (label_domain, label_project, label_entity_id, label_entity_type) (
                entity_id:allocated_cost:sum
                or
                entity_id:overhead_cost:sum
              ),
              "type", "total", "", "" # add type info
            )
        - record: node:total_cost:sum # Total cost of all nodes
          expr: |
            sum (
              avg by (node)(kube_node_labels{label_flyte_org_node_role="worker", label_node_kubernetes_io_instance_type!=""}) # only look at worker nodes
              * on (node) group_left() node_total_hourly_cost{instance_type!=""} * (15 / 3600) # convert hourly cost to 15-secondly cost
            )
        - record: node_type:total_cost:sum # Total cost of nodes grouped by node type
          expr: |
            sum by (node_type)(
              avg by (node)(kube_node_labels{label_flyte_org_node_role="worker", label_node_kubernetes_io_instance_type!=""}) # only look at worker nodes
              * on (node) group_left(node_type) label_replace(node_total_hourly_cost{instance_type!=""}, "node_type", "$1", "instance_type", "(.*)") * (15 / 3600) # convert hourly cost to 15-secondly cost and rename label
            )
        - record: node_type:uptime_hours:sum # Total uptime of nodes grouped by node type
          expr: |
            sum by (node_type)(
              avg by (node, node_type)( # dedupe
                label_replace(kube_node_labels{label_flyte_org_node_role="worker", label_node_kubernetes_io_instance_type!=""}, "node_type", "$1", "label_node_kubernetes_io_instance_type", "(.*)") # relabel
              )
            ) * (15 / 3600) # convert to number of hours per 15-second observation      # Aggregate the above into visible metrics
    - name: cost_rollup_15m
      interval: 15m
      rules:
        - record: execution_info15m
          expr: |
            max_over_time(execution_info[15m:15s])
        - record: app_info15m
          expr: |
            max_over_time(app_info[15m:15s])
        - record: workspace_info15m
          expr: |
            max_over_time(workspace_info[15m:15s])
        - record: entity_id:allocated_mem_bytes:sum15m
          expr: |
            sum_over_time(entity_id:allocated_mem_bytes:sum[15m:15s])
        - record: entity_id:used_mem_bytes:sum15m
          expr: |
            sum_over_time(entity_id:used_mem_bytes:sum[15m:15s])
        - record: entity_id:allocated_cpu:sum15m
          expr: |
            sum_over_time(entity_id:allocated_cpu:sum[15m:15s])
        - record: entity_id:used_cpu:sum15m
          expr: |
            sum_over_time(entity_id:used_cpu:sum[15m:15s])
        - record: entity_id:weighted_sm_occupancy:sum15m
          expr: |
            sum_over_time(entity_id:weighted_sm_occupancy:sum[15m:15s])
        - record: entity_id:gpu_count:sum15m
          expr: |
            sum_over_time(entity_id:gpu_count:sum[15m:15s])
        - record: entity_id:allocated_mem_cost:sum15m
          expr: |
            sum_over_time(entity_id:allocated_mem_cost:sum[15m:15s])
        - record: entity_id:allocated_cpu_cost:sum15m
          expr: |
            sum_over_time(entity_id:allocated_cpu_cost:sum[15m:15s])
        - record: entity_id:allocated_gpu_cost:sum15m
          expr: |
            sum_over_time(entity_id:allocated_gpu_cost:sum[15m:15s])
        - record: entity_id:allocated_cost:sum15m
          expr: |
            sum_over_time(entity_id:allocated_cost:sum[15m:15s])
        - record: entity_id:overhead_cost:sum15m
          expr: |
            sum_over_time(entity_id:overhead_cost:sum[15m:15s])
        - record: entity_id:total_cost:sum15m
          expr: |
            sum_over_time(entity_id:total_cost:sum[15m:15s])
        - record: node:total_cost:sum15m
          expr: |
            sum_over_time(node:total_cost:sum[15m:15s])
        - record: node_type:total_cost:sum15m
          expr: |
            sum_over_time(node_type:total_cost:sum[15m:15s])
        - record: node_type:uptime_hours:sum15m
          expr: |
            sum_over_time(node_type:uptime_hours:sum[15m:15s])
---
# Source: dataplane/charts/dcgm-exporter/templates/service-monitor.yaml
# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-dcgm-exporter
  namespace: union
  labels:
    helm.sh/chart: dcgm-exporter-4.1.0
    app.kubernetes.io/name: dcgm-exporter
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "4.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "dcgm-exporter"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: dcgm-exporter
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: "dcgm-exporter"
  namespaceSelector:
    matchNames:
    - "union"
  endpoints:
  - port: "metrics"
    path: "/metrics"
    interval: "15s"
    honorLabels: false
    relabelings:
      
      []
---
# Source: dataplane/charts/prometheus/charts/kube-state-metrics/templates/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-kube-state-metrics
  namespace: kube-system
  labels:    
    helm.sh/chart: kube-state-metrics-5.33.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.15.0"
    release: release-name
spec:
  jobLabel: app.kubernetes.io/name  
  selector:
    matchLabels:      
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/instance: release-name
  endpoints:
    - port: http
      honorLabels: true
---
# Source: dataplane/charts/prometheus/templates/exporters/core-dns/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: union-operator-coredns
  namespace: union
  labels:
    app: prometheus-coredns
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
spec:
  jobLabel: jobLabel
  
  selector:
    matchLabels:
      app: prometheus-coredns
      release: "release-name"
  namespaceSelector:
    matchNames:
      - "kube-system"
  endpoints:
  - port: http-metrics
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
---
# Source: dataplane/charts/prometheus/templates/exporters/kube-controller-manager/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: union-operator-kube-controller-manager
  namespace: union
  labels:
    app: prometheus-kube-controller-manager
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
spec:
  jobLabel: jobLabel
  
  selector:
    matchLabels:
      app: prometheus-kube-controller-manager
      release: "release-name"
  namespaceSelector:
    matchNames:
      - "kube-system"
  endpoints:
  - port: http-metrics
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    scheme: https
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
---
# Source: dataplane/charts/prometheus/templates/exporters/kube-etcd/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: union-operator-kube-etcd
  namespace: union
  labels:
    app: prometheus-kube-etcd
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
spec:
  jobLabel: jobLabel
  
  selector:
    matchLabels:
      app: prometheus-kube-etcd
      release: "release-name"
  namespaceSelector:
    matchNames:
      - "kube-system"
  endpoints:
  - port: http-metrics
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
---
# Source: dataplane/charts/prometheus/templates/exporters/kube-proxy/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: union-operator-kube-proxy
  namespace: union
  labels:
    app: prometheus-kube-proxy
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
spec:
  jobLabel: jobLabel
  
  selector:
    matchLabels:
      app: prometheus-kube-proxy
      release: "release-name"
  namespaceSelector:
    matchNames:
      - "kube-system"
  endpoints:
  - port: http-metrics
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
---
# Source: dataplane/charts/prometheus/templates/exporters/kube-scheduler/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: union-operator-kube-scheduler
  namespace: union
  labels:
    app: prometheus-kube-scheduler
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
spec:
  jobLabel: jobLabel
  
  selector:
    matchLabels:
      app: prometheus-kube-scheduler
      release: "release-name"
  namespaceSelector:
    matchNames:
      - "kube-system"
  endpoints:
  - port: http-metrics
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    scheme: https
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
---
# Source: dataplane/charts/prometheus/templates/exporters/kubelet/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: union-operator-kubelet
  namespace: union
  labels:
    app: prometheus-kubelet    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
spec:
  
  attachMetadata:
    node: false
  jobLabel: k8s-app
  namespaceSelector:
    matchNames:
    - kube-system
  selector:
    matchLabels:
      app.kubernetes.io/name: kubelet
      k8s-app: kubelet
  endpoints:
  - port: https-metrics
    scheme: https    
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    honorLabels: true
    honorTimestamps: true
    metricRelabelings:
    - action: drop
      regex: (csi_operations|storage_operation_duration)_seconds_bucket;(0.25|2.5|15|25|120|600)(\.0)?
      sourceLabels:
      - __name__
      - le
    relabelings:
    - action: replace
      sourceLabels:
      - __metrics_path__
      targetLabel: metrics_path
  - port: https-metrics
    scheme: https
    path: /metrics/cadvisor
    interval: 10s
    honorLabels: true
    honorTimestamps: true
    trackTimestampsStaleness: true    
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    metricRelabelings:
    - action: drop
      regex: container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)
      sourceLabels:
      - __name__
    - action: drop
      regex: container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)
      sourceLabels:
      - __name__
    - action: drop
      regex: container_memory_(mapped_file|swap)
      sourceLabels:
      - __name__
    - action: drop
      regex: container_(file_descriptors|tasks_state|threads_max)
      sourceLabels:
      - __name__
    - action: drop
      regex: container_memory_failures_total;hierarchy
      sourceLabels:
      - __name__
      - scope
    - action: drop
      regex: container_network_.*;(cali|cilium|cni|lxc|nodelocaldns|tunl).*
      sourceLabels:
      - __name__
      - interface
    - action: drop
      regex: container_spec.*
      sourceLabels:
      - __name__
    - action: drop
      regex: .+;
      sourceLabels:
      - id
      - pod
    relabelings:
    - action: replace
      sourceLabels:
      - __metrics_path__
      targetLabel: metrics_path
  - port: https-metrics
    scheme: https
    path: /metrics/probes
    honorLabels: true
    honorTimestamps: true    
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabelings:
    - action: replace
      sourceLabels:
      - __metrics_path__
      targetLabel: metrics_path
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: prometheus-operator
  namespace: union
  labels:
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app: prometheus-operator
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator
spec:
  
  endpoints:
  - port: https
    scheme: https
    tlsConfig:
      serverName: prometheus-operator
      ca:
        secret:
          name: union-operator-admission
          key: ca
          optional: false
    honorLabels: true
  selector:
    matchLabels:
      app: prometheus-operator
      release: "release-name"
  namespaceSelector:
    matchNames:
      - "union"
---
# Source: dataplane/charts/prometheus/templates/prometheus/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: union-operator-prometheus
  namespace: union
  labels:
    app: prometheus-prometheus
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
spec:
  
  selector:
    matchLabels:
      app: prometheus-prometheus
      release: "release-name"
      self-monitor: "true"
  namespaceSelector:
    matchNames:
      - "union"
  endpoints:
  - port: http-web
    path: "/prometheus/metrics"
  - port: reloader-web
    path: "/metrics"
---
# Source: dataplane/templates/monitoring/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: cost
  namespace: union
  labels:
    release: release-name
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: opencost
  namespaceSelector:
    matchNames:
      - "union"
  endpoints:
    - port: http
      interval: 1m
      path: /metrics
      honorLabels: true
      metricRelabelings:
        - sourceLabels: [ "__name__" ]
          separator: ";"
          regex: "kube_node_labels|kube_pod_labels|node_total_hourly_cost|node_ram_hourly_cost|node_cpu_hourly_cost|node_gpu_hourly_cost"
          action: keep
---
# Source: dataplane/templates/monitoring/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: union-service-monitor
  namespace: union
  labels:
    release: release-name
spec:
  selector:
    matchLabels:
      platform.union.ai/service-group: release-name
  namespaceSelector:
    matchNames:
      - "union"
  endpoints:
    - port: debug
      interval: 1m
      path: /metrics
      honorLabels: true
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/admission-webhooks/validatingWebhookConfiguration.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name:  union-operator-admission
  annotations:
    
  labels:
    app: prometheus-admission
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator-webhook
webhooks:
  - name: prometheusrulemutate.monitoring.coreos.com
    failurePolicy: Ignore
    rules:
      - apiGroups:
          - monitoring.coreos.com
        apiVersions:
          - "*"
        resources:
          - prometheusrules
        operations:
          - CREATE
          - UPDATE
    clientConfig:
      service:
        namespace: union
        name: prometheus-operator
        path: /admission-prometheusrules/validate
    timeoutSeconds: 10
    admissionReviewVersions: ["v1", "v1beta1"]
    sideEffects: None
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/admission-webhooks/job-patch/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name:  union-operator-admission
  namespace: union
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: prometheus-admission
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator-webhook
automountServiceAccountToken: true
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/admission-webhooks/job-patch/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name:  union-operator-admission
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: prometheus-admission
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator-webhook
rules:
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - validatingwebhookconfigurations
      - mutatingwebhookconfigurations
    verbs:
      - get
      - update
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/admission-webhooks/job-patch/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name:  union-operator-admission
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: prometheus-admission
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator-webhook
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: union-operator-admission
subjects:
  - kind: ServiceAccount
    name: union-operator-admission
    namespace: union
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/admission-webhooks/job-patch/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name:  union-operator-admission
  namespace: union
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: prometheus-admission
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator-webhook
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - create
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/admission-webhooks/job-patch/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name:  union-operator-admission
  namespace: union
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: prometheus-admission
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator-webhook
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: union-operator-admission
subjects:
  - kind: ServiceAccount
    name: union-operator-admission
    namespace: union
---
# Source: dataplane/charts/fluentbit/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-fluentbit-test-connection"
  namespace: union
  labels:
    helm.sh/chart: fluentbit-0.48.9
    app.kubernetes.io/version: "3.2.8"
    app.kubernetes.io/managed-by: Helm
  annotations:
    helm.sh/hook: test
    helm.sh/hook-delete-policy: hook-succeeded
spec:
  containers:
    - name: wget
      image: "busybox:latest"
      imagePullPolicy: Always
      command: ["sh"]
      args: ["-c", "sleep 5s && wget -O- release-name-fluentbit:2020"]
  restartPolicy: Never
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/admission-webhooks/job-patch/job-createSecret.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name:  union-operator-admission-create
  namespace: union
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: prometheus-admission-create
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator-webhook
spec:
  ttlSecondsAfterFinished: 60
  template:
    metadata:
      name:  union-operator-admission-create
      labels:
        app: prometheus-admission-create
        
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "72.9.1"
        app.kubernetes.io/part-of: prometheus
        chart: prometheus-72.9.1
        release: "release-name"
        heritage: "Helm"
        app.kubernetes.io/name: prometheus-prometheus-operator
        app.kubernetes.io/component: prometheus-operator-webhook
    spec:
      containers:
        - name: create
          image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3
          imagePullPolicy: IfNotPresent
          args:
            - create
            - --host=prometheus-operator,prometheus-operator.union.svc
            - --namespace=union
            - --secret-name=union-operator-admission
          securityContext:
          
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          resources:
            {}
      restartPolicy: OnFailure
      serviceAccountName: union-operator-admission
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
        seccompProfile:
          type: RuntimeDefault
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/admission-webhooks/job-patch/job-patchWebhook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name:  union-operator-admission-patch
  namespace: union
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: prometheus-admission-patch
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator-webhook
spec:
  ttlSecondsAfterFinished: 60
  template:
    metadata:
      name:  union-operator-admission-patch
      labels:
        app: prometheus-admission-patch
        
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "72.9.1"
        app.kubernetes.io/part-of: prometheus
        chart: prometheus-72.9.1
        release: "release-name"
        heritage: "Helm"
        app.kubernetes.io/name: prometheus-prometheus-operator
        app.kubernetes.io/component: prometheus-operator-webhook
    spec:
      containers:
        - name: patch
          image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3
          imagePullPolicy: IfNotPresent
          args:
            - patch
            - --webhook-name=union-operator-admission
            - --namespace=union
            - --secret-name=union-operator-admission
            - --patch-failure-policy=
          securityContext:
          
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          resources:
            {}
      restartPolicy: OnFailure
      serviceAccountName: union-operator-admission
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
        seccompProfile:
          type: RuntimeDefault
