---
# Source: dataplane/templates/common/namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: flytesnacks-development
---
# Source: dataplane/templates/common/namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: flytesnacks-staging
---
# Source: dataplane/templates/common/namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: flytesnacks-production
---
# Source: dataplane/templates/common/namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: union-health-monitoring-development
---
# Source: dataplane/templates/common/namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: union-health-monitoring-staging
---
# Source: dataplane/templates/common/namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: union-health-monitoring-production
---
# Source: dataplane/charts/fluentbit/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentbit-system
  namespace: union
  labels:
    helm.sh/chart: fluentbit-0.48.9
    app.kubernetes.io/name: fluentbit
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "3.2.8"
    app.kubernetes.io/managed-by: Helm
---
# Source: dataplane/charts/opencost/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: opencost
  namespace: union
  labels:
    helm.sh/chart: opencost-1.42.0
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.111.0"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: dataplane/charts/prometheus/charts/kube-state-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-5.33.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.15.0"
    release: release-name
  name: release-name-kube-state-metrics
  namespace: kube-system
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus-operator
  namespace: union
  labels:
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app: prometheus-operator
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator
automountServiceAccountToken: true
---
# Source: dataplane/charts/prometheus/templates/prometheus/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: union-operator-prometheus
  namespace: union
  labels:
    app: prometheus-prometheus
    app.kubernetes.io/name: prometheus-prometheus
    app.kubernetes.io/component: prometheus
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
automountServiceAccountToken: true
---
# Source: dataplane/templates/clusterresourcesync/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: clustersync-system
  namespace: union
---
# Source: dataplane/templates/nodeexecutor/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: executor
  namespace: union
  labels:
    app: executor
---
# Source: dataplane/templates/nodeobserver/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nodeobserver-system
  namespace: union
---
# Source: dataplane/templates/operator/serviceaccount-proxy.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: proxy-system
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: dataplane/templates/operator/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: operator-system
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: dataplane/templates/propeller/serviceaccount-webhook.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flytepropeller-webhook-system
  namespace: union
---
# Source: dataplane/templates/propeller/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flytepropeller-system
  namespace: union
---
# Source: dataplane/templates/common/cluster-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: operator-cluster-name
type: Opaque
data:
  cluster_name:
---
# Source: dataplane/templates/propeller/deployment-webhook.yaml
# Create an empty secret that the first propeller pod will populate
apiVersion: v1
kind: Secret
metadata:
  name: flyte-pod-webhook
  namespace: union
type: Opaque
---
# Source: dataplane/templates/clusterresourcesync/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: flyte-clusterresourcesync-config
  namespace: union
  labels:
    app.kubernetes.io/name: clusterresourcesync
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
data:
  cluster_resources.yaml: | 
    cluster_resources:
      clusterName: ''
      customData:
      - production:
        - projectQuotaCpu:
            value: "4096"
        - projectQuotaMemory:
            value: 2Ti
        - projectQuotaNvidiaGpu:
            value: "256"
        - defaultUserRoleKey:
            value: 'eks.amazonaws.com/role-arn'
        - defaultUserRoleValue:
            value: 'arn:aws:iam::ACCOUNT_ID:role/flyte_project_role'
      - staging:
        - projectQuotaCpu:
            value: "4096"
        - projectQuotaMemory:
            value: 2Ti
        - projectQuotaNvidiaGpu:
            value: "256"
        - defaultUserRoleKey:
            value: 'eks.amazonaws.com/role-arn'
        - defaultUserRoleValue:
            value: 'arn:aws:iam::ACCOUNT_ID:role/flyte_project_role'
      - development:
        - projectQuotaCpu:
            value: "4096"
        - projectQuotaMemory:
            value: 2Ti
        - projectQuotaNvidiaGpu:
            value: "256"
        - defaultUserRoleKey:
            value: 'eks.amazonaws.com/role-arn'
        - defaultUserRoleValue:
            value: 'arn:aws:iam::ACCOUNT_ID:role/flyte_project_role'
      refreshInterval: 5m
      standaloneDeployment: true
      templatePath: /etc/flyte/clusterresource/templates
    clusterResourcesPrivate:
      app:
        isServerless: false
    union:
      auth:
        authorizationMetadataKey: flyte-authorization
        clientId: 'dataplane-operator'
        clientSecretLocation: /etc/union/secret/client_secret
        tokenRefreshWindow: 5m
        type: ClientSecret
      connection:
        host: dns:///
  admin.yaml: | 
    admin:
      clientId: 'dataplane-operator'
      clientSecretLocation: /etc/union/secret/client_secret
      endpoint: dns:///
      insecure: false
    event:
      capacity: 1000
      rate: 500
      type: admin
  domain.yaml: | 
    domains:
    - id: development
      name: development
    - id: staging
      name: staging
    - id: production
      name: production
  clusters.yaml: |
    clusters:
      clusterConfigs: []
      labelClusterMap: {}
  logger.yaml: |
    logger:
      level: 4
      show-source: true
---
# Source: dataplane/templates/clusterresourcesync/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: clusterresource-template
  namespace: union
  labels:
    app.kubernetes.io/name: clusterresourcesync
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
data:
  a_namespace.yaml: | 
    apiVersion: v1
    kind: Namespace
    metadata:
      name: {{ namespace }}
      labels:
        union.ai/namespace-type: flyte
    spec:
      finalizers:
      - kubernetes
    
  b_default_service_account.yaml: | 
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: default
      namespace: {{ namespace }}
      annotations:
        {{ defaultUserRoleKey }}: {{ defaultUserRoleValue }}
    
  c_project_resource_quota.yaml: | 
    apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: project-quota
      namespace: {{ namespace }}
    spec:
      hard:
        limits.cpu: {{ projectQuotaCpu }}
        limits.memory: {{ projectQuotaMemory }}
        requests.nvidia.com/gpu: {{ projectQuotaNvidiaGpu }}
---
# Source: dataplane/templates/fluent-bit/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentbit-system
  namespace: union
  labels:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  custom_parsers.conf: |
    [PARSER]
        Name docker_no_time
        Format json
        Time_Keep Off
        Time_Key time
        Time_Format %Y-%m-%dT%H:%M:%S.%L
  fluent-bit.conf: |
    [SERVICE]
        Parsers_File /fluent-bit/etc/parsers.conf
        Parsers_File /fluent-bit/etc/conf/custom_parsers.conf
        HTTP_Server On
        HTTP_Listen 0.0.0.0
        Health_Check On
    [INPUT]
        Name                tail
        Tag                 namespace-<namespace_name>.pod-<pod_name>.cont-<container_name>
        Tag_Regex           (?<pod_name>[a-z0-9](?:[-a-z0-9]*[a-z0-9])?(?:\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*)_(?<namespace_name>[^_]+)_(?<container_name>.+)-
        Path                /var/log/containers/*.log
        DB                  /var/log/flb_kube.db
        multiline.parser    docker, cri
        Mem_Buf_Limit       5MB
        Skip_Long_Lines     On
        Refresh_Interval    10
    
    
    [OUTPUT]
        Name s3
        Match *
        upload_timeout 1m
        s3_key_format /persisted-logs/$TAG
        static_file_path true
        json_date_key false
        region us-east-1
        bucket
---
# Source: dataplane/templates/imagebuilder/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name : union-operator-buildkit
data:
  buildkitd.toml: |
    debug = false

    [log]
      format = "text"

    [worker.oci]
      enabled = true
      snapshotter = "auto"
      gc = true
      max-parallelism = 0

      # Should not be used if Policies are defined
      gckeepstorage = "10%"
      [[worker.oci.gcpolicy]]
        # Remove COPY/ADD and git checkout files
        keepBytes = "10%"
        keepDuration = "24h"
        filters = [ "type==source.local", "type==source.git.checkout" ]
      [[worker.oci.gcpolicy]]
        # Remove locally cached image layers after it's unused for 24 hours
        keepBytes = "10%"
        keepDuration = "24h"
        filters = [ "regular" ]
      [[worker.oci.gcpolicy]]
        # Remove shared cache mounts. E.G. Pip cache
        keepBytes = "10%"
        keepDuration = "72h"
        filters = [ "type==exec.cachemount" ]
      [[worker.oci.gcpolicy]]
        # Remove everything else to keep the cache size under total file system limit
        all = true
        keepBytes = "80%"
---
# Source: dataplane/templates/nodeexecutor/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: executor
  namespace: union
  labels:
    app: executor
data:
  task_logs.yaml: | 
    plugins:
      logs:
        cloudwatch-enabled: false
        kubernetes-enabled: true
  enabled_plugins.yaml: |
    tasks:
      task-plugins:
        default-for-task-types:
          actor: fast-task
          container: container
          container_array: k8s-array
          sidecar: sidecar
        enabled-plugins:
        - container
        - sidecar
        - k8s-array
        - echo
        - fast-task
        - connector-service
  config.yaml: |
    executor:
      cluster: ''
      evaluatorCount: 64
      maxActions: 2000
      organization: ''
      unionAuth:
        injectSecret: true
        secretName: EAGER_API_KEY
      workerName: worker1
      task_resources:
        defaults:
          cpu: 100m
          memory: 500Mi
        limits:
          cpu: 4096
          gpu: 256
          memory: 2Ti
    union:
      connection:
        host: dns:///
      auth:
        authorizationMetadataKey: flyte-authorization
        clientId: 'dataplane-operator'
        clientSecretLocation: /etc/union/secret/client_secret
        tokenRefreshWindow: 5m
        type: ClientSecret
    admin:
      clientId: 'dataplane-operator'
      clientSecretLocation: /etc/union/secret/client_secret
      endpoint: dns:///
      insecure: false
    authorizer:
      type: noop
    catalog-cache:
      cache-endpoint: dns:///
      endpoint: dns:///
      insecure: false
      type: fallback
      use-admin-auth: true
    logger:
      level: 4
      show-source: true
    sharedService:
      metrics:
        scope: 'executor:'
      security:
        allowCors: true
        allowLocalhostAccess: true
        allowedHeaders:
        - Content-Type
        allowedOrigins:
        - '*'
        secure: false
        useAuth: false
    propeller:
      node-config:
        disable-input-file-writes: true
    plugins:
      fasttask:
        additional-worker-args:
        - --last-ack-grace-period-seconds
        - "120"
        callback-uri: http://unionai-dataplane-executor.union.svc.cluster.local:15605
        grace-period-status-not-found: 2m
      ioutils:
        remoteFileOutputPaths:
          deckFilename: report.html
      k8s:
        disable-inject-owner-references: true
        default-cpus: 100m
        default-env-vars: []
        default-memory: 100Mi
        co-pilot:
          image: 'cr.flyte.org/flyteorg/flytecopilot:v1.14.1'
          name: flyte-copilot-
          start-timeout: 30s
    storage:
      container: ""
      type: stow
      stow:
        kind: s3
        config:
          auth_type: accesskey
          access_key_id: 
          secret_key: 
          disable_ssl: false
          endpoint: 
          region: us-east-1
      enable-multicontainer: false
      limits:
        maxDownloadMBs: 1024
      cache:
        max_size_mbs: 0
        target_gc_percent: 70
---
# Source: dataplane/templates/nodeobserver/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: union-nodeobserver
  namespace: union
data:
  config.yaml: | 
    criticalDaemonSets: []
---
# Source: dataplane/templates/operator/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: union-operator
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
data:
  k8s.yaml: | 
    plugins:
      k8s:
        default-cpus: 100m
        default-env-vars: []
        default-memory: 100Mi
  config.yaml: |
    union:
      connection:
        host: dns:///
      auth:
        authorizationMetadataKey: flyte-authorization
        clientId: 'dataplane-operator'
        clientSecretLocation: /etc/union/secret/client_secret
        tokenRefreshWindow: 5m
        type: ClientSecret
    sharedService:
      features:
        gatewayV2: true
      port: 8081
    authorizer:
      type: noop
    operator:
      enabled: true
      enableTunnelService: true
      tunnel:
        enableDirectToAppIngress: false
        deploymentToRestart: union-operator-proxy
      apps:
        enabled: 'false'
      syncClusterConfig:
        enabled: false
      clusterId:
        organization: ''
      clusterData:
        appId: 'dataplane-operator'
        bucketName: ''
        bucketRegion: 'us-east-1'
        cloudHostName: ''
        gcpProjectId: ''
        metadataBucketPrefix: 's3://'
        userRole: 'arn:aws:iam::ACCOUNT_ID:role/flyte_project_role'
        userRoleKey: 'eks.amazonaws.com/role-arn'
      collectUsages:
        enabled: true
      collectBillableResourceUsage:
        enabled: false
      billableUsageCollector:
        enabled: true
      dependenciesHeartbeat:
        prometheus:
          endpoint: 'http://union-operator-prometheus:80/-/healthy'
        propeller:
          endpoint: 'http://flytepropeller:10254'
        proxy:
          endpoint: 'http://union-operator-proxy:10254'
      imageBuilder:
        enabled: true
        executionNamespaceLabels:
          union.ai/namespace-type: flyte
        referenceConfigmapName: union-operator
        targetConfigMapName: "build-image-config"
    proxy:
      imageBuilderConfig:
        authenticationType: 'noop'
        defaultRepository: ''
      persistedLogs:
        objectStore:
          pathTemplate: namespace-{{.KubernetesNamespace}}.pod-{{.KubernetesPodName}}.cont-{{.KubernetesContainerName}}
          prefix: persisted-logs
        sourceType: ObjectStore
      smConfig:
        enabled: 'true'
        k8sConfig:
          namespace: 'union'
        type: 'K8s'
  logger.yaml: |
    logger:
      level: 4
      show-source: true
  config-overrides.yaml: | 
    cache:
      identity:
        enabled: false
  storage.yaml: | 
    storage:
      container: ""
      type: stow
      stow:
        kind: s3
        config:
          auth_type: accesskey
          access_key_id: 
          secret_key: 
          disable_ssl: false
          endpoint: 
          region: us-east-1
      enable-multicontainer: false
      limits:
        maxDownloadMBs: 1024
      cache:
        max_size_mbs: 0
        target_gc_percent: 70
  fast_registration_storage.yaml: | 
    fastRegistrationStorage:
      container: ""
      type: stow
      stow:
        kind: s3
        config:
          auth_type: accesskey
          access_key_id: 
          secret_key: 
          disable_ssl: false
          endpoint: 
          region: us-east-1
  image-builder.buildkit-uri: "tcp://union-operator-buildkit.union.svc.cluster.local:1234"
  image-builder.default-repository: ""
  image-builder.authentication-type: "noop"
---
# Source: dataplane/templates/propeller/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: flyte-propeller-config
  namespace: union
data:
  admin.yaml: | 
    admin:
      clientId: 'dataplane-operator'
      clientSecretLocation: /etc/union/secret/client_secret
      endpoint: dns:///
      insecure: false
    event:
      capacity: 1000
      rate: 500
      type: admin
  catalog.yaml: | 
    catalog-cache:
      cache-endpoint: dns:///
      endpoint: dns:///
      insecure: false
      type: fallback
      use-admin-auth: true
  copilot.yaml: | 
    plugins:
      k8s:
        co-pilot:
          image: 'cr.flyte.org/flyteorg/flytecopilot:v1.14.1'
          name: flyte-copilot-
          start-timeout: 30s
  core.yaml: | 
    propeller:
      downstream-eval-duration: 30s
      enable-admin-launcher: true
      leader-election:
        enabled: true
        lease-duration: 15s
        lock-config-map:
          name: propeller-leader
          namespace: 'union'
        renew-deadline: 10s
        retry-period: 2s
      limit-namespace: all
      literal-offloading-config:
        enabled: true
      max-workflow-retries: 30
      metadata-prefix: metadata/propeller
      metrics-prefix: flyte
      prof-port: 10254
      queue:
        batch-size: -1
        batching-interval: 2s
        queue:
          base-delay: 5s
          capacity: 1000
          max-delay: 120s
          rate: 100
          type: maxof
        sub-queue:
          capacity: 100
          rate: 10
          type: bucket
        type: batch
      rawoutput-prefix: 's3://'
      workers: 4
      workflow-reeval-duration: 30s
    webhook:
      certDir: /etc/webhook/certs
      embeddedSecretManagerConfig:
        imagePullSecrets:
          enabled: true
        k8sConfig:
          namespace: 'union'
        type: 'K8s'
      listenPort: '9443'
      secretManagerTypes:
      - Embedded
      - K8s
      serviceName: flyte-pod-webhook
      servicePort: '443'
  enabled_plugins.yaml: | 
    plugins:
      connector-service:
        defaultConnector:
          defaultServiceConfig: '{"loadBalancingConfig": [{"round_robin":{}}]}'
          endpoint: k8s:///flyteconnector.union:8000
    tasks:
      task-plugins:
        default-for-task-types:
          actor: fast-task
          container: container
          container_array: k8s-array
          sidecar: sidecar
        enabled-plugins:
        - container
        - sidecar
        - k8s-array
        - echo
        - fast-task
        - connector-service
  k8s.yaml: | 
    plugins:
      k8s:
        default-cpus: 100m
        default-env-vars: []
        default-memory: 100Mi
  logger.yaml: |
    logger:
      level: 4
      show-source: true
  resource_manager.yaml: | 
    propeller:
      resourcemanager:
        type: noop
  task_logs.yaml: | 
    plugins:
      logs:
        cloudwatch-enabled: false
        dynamic-log-links:
        - vscode:
            displayName: VS Code Debugger
            templateUris:
            - /dataplane/pod/v1/generated_name/task/{{.executionProject}}/{{.executionDomain}}/{{.executionName}}/{{.nodeID}}/{{.taskRetryAttempt}}/{{.taskProject}}/{{.taskDomain}}/{{.taskID}}/{{.taskVersion}}/
        kubernetes-enabled: false
        templates:
        - displayName: Task Logs
          scheme: TaskExecution
          templateUris:
          - /console/projects/{{.executionProject}}/domains/{{.executionDomain}}/executions/{{.executionName}}/nodeId/{{.nodeID}}/taskId/{{.taskID}}/attempt/{{.taskRetryAttempt}}/view/logs?duration=all&fromExecutionNav=true
  storage.yaml: | 
    storage:
      container: ""
      type: stow
      stow:
        kind: s3
        config:
          auth_type: accesskey
          access_key_id: 
          secret_key: 
          disable_ssl: false
          endpoint: 
          region: us-east-1
      enable-multicontainer: false
      limits:
        maxDownloadMBs: 1024
      cache:
        max_size_mbs: 0
        target_gc_percent: 70
---
# Source: dataplane/charts/fluentbit/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-fluentbit
  labels:
    helm.sh/chart: fluentbit-0.48.9
    app.kubernetes.io/name: fluentbit
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "3.2.8"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - namespaces
      - pods
    verbs:
      - get
      - list
      - watch
---
# Source: dataplane/charts/opencost/templates/clusterrole.yaml
# Cluster role giving opencost to get, list, watch required resources
# No write permissions are required
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: opencost
  labels:
    helm.sh/chart: opencost-1.42.0
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.111.0"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups: [""]
    resources:
      - configmaps
      - deployments
      - nodes
      - pods
      - services
      - resourcequotas
      - replicationcontrollers
      - limitranges
      - persistentvolumeclaims
      - persistentvolumes
      - namespaces
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
    resources:
      - daemonsets
      - deployments
      - replicasets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - statefulsets
      - deployments
      - daemonsets
      - replicasets
    verbs:
      - list
      - watch
  - apiGroups:
      - batch
    resources:
      - cronjobs
      - jobs
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - autoscaling
    resources:
      - horizontalpodautoscalers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - storage.k8s.io
    resources:
      - storageclasses
    verbs:
      - get
      - list
      - watch
---
# Source: dataplane/charts/prometheus/charts/kube-state-metrics/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-5.33.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.15.0"
    release: release-name
  name: release-name-kube-state-metrics
rules:

- apiGroups: ["certificates.k8s.io"]
  resources:
  - certificatesigningrequests
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - cronjobs
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - daemonsets
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - deployments
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - endpoints
  verbs: ["list", "watch"]

- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "networking.k8s.io"]
  resources:
  - ingresses
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - jobs
  verbs: ["list", "watch"]

- apiGroups: ["coordination.k8s.io"]
  resources:
  - leases
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - limitranges
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - mutatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - namespaces
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - networkpolicies
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - nodes
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumeclaims
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumes
  verbs: ["list", "watch"]

- apiGroups: ["policy"]
  resources:
    - poddisruptionbudgets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - pods
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - replicasets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - replicationcontrollers
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - resourcequotas
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - secrets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - services
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - statefulsets
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - storageclasses
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - validatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - volumeattachments
  verbs: ["list", "watch"]
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus-operator
  labels:
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app: prometheus-operator
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator
rules:
- apiGroups:
  - monitoring.coreos.com
  resources:
  - alertmanagers
  - alertmanagers/finalizers
  - alertmanagers/status
  - alertmanagerconfigs
  - prometheuses
  - prometheuses/finalizers
  - prometheuses/status
  - prometheusagents
  - prometheusagents/finalizers
  - prometheusagents/status
  - thanosrulers
  - thanosrulers/finalizers
  - thanosrulers/status
  - scrapeconfigs
  - servicemonitors
  - podmonitors
  - probes
  - prometheusrules
  verbs:
  - '*'
- apiGroups:
  - apps
  resources:
  - statefulsets
  verbs:
  - '*'
- apiGroups:
  - ""
  resources:
  - configmaps
  - secrets
  verbs:
  - '*'
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - delete
- apiGroups:
  - ""
  resources:
  - services
  - services/finalizers
  - endpoints
  verbs:
  - get
  - create
  - update
  - delete
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - patch
  - create
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - storage.k8s.io
  resources:
  - storageclasses
  verbs:
  - get
---
# Source: dataplane/charts/prometheus/templates/prometheus/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: union-operator-prometheus
  labels:
    app: prometheus-prometheus
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
rules:
# This permission are not in the kube-prometheus repo
# they're grabbed from https://github.com/prometheus/prometheus/blob/master/documentation/examples/rbac-setup.yml
- apiGroups: [""]
  resources:
  - nodes
  - nodes/metrics
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: ["discovery.k8s.io"]
  resources:
  - endpointslices
  verbs: ["get", "list", "watch"]
- apiGroups:
  - "networking.k8s.io"
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics", "/metrics/cadvisor"]
  verbs: ["get"]
---
# Source: dataplane/templates/clusterresourcesync/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: clustersync-resource
rules:
  - apiGroups:
      - ""
      - rbac.authorization.k8s.io
    resources:
      - configmaps
      - namespaces
      - pods
      - resourcequotas
      - roles
      - rolebindings
      - secrets
      - services
      - serviceaccounts
      - clusterrolebindings
    verbs:
      - '*'
---
# Source: dataplane/templates/nodeexecutor/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: union-executor
  labels:
    app: executor
rules:
# Allow RO access to PODS
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
# Allow Event recording access
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - update
  - delete
  - patch
# Allow Access All plugin objects
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
  - patch
# Allow Access to CRD
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - update
---
# Source: dataplane/templates/nodeobserver/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: union-nodeobserver
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "update"]
---
# Source: dataplane/templates/operator/serviceaccount-proxy.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: proxy-system
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - '*'
    resources:
      - events
      - flyteworkflows
      - pods/log
      - pods
      - rayjobs
      - resourcequotas
    verbs:
      - get
      - list
      - watch
---
# Source: dataplane/templates/operator/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: operator-system
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
rules:
  # Allow Access to all resources under flyte.lyft.com
  - apiGroups:
      - flyte.lyft.com
    resources:
      - flyteworkflows
      - flyteworkflows/finalizers
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - patch
      - post
      - deletecollection
  - apiGroups:
      - '*'
    resources:
      - resourcequotas
      - pods
      - configmaps
      - podtemplates
      - secrets
      - namespaces
      - nodes
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
---
# Source: dataplane/templates/propeller/serviceaccount-webhook.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: flytepropeller-webhook-role
  namespace: union
rules:
  - apiGroups:
      - "*"
    resources:
      - mutatingwebhookconfigurations
      - secrets
      - pods
      - replicasets/finalizers
    verbs:
      - get
      - create
      - update
      - patch
---
# Source: dataplane/templates/propeller/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: flytepropeller-role
rules:
  # Allow RO access to PODS
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
  # Allow Event recording access
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - update
      - delete
      - patch
  # Allow Access All plugin objects
  - apiGroups:
      - '*'
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - patch
  # Allow Access to CRD
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - update
  # Allow Access to all resources under flyte.lyft.com
  - apiGroups:
      - flyte.lyft.com
    resources:
      - flyteworkflows
      - flyteworkflows/finalizers
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - patch
      - post
      - deletecollection
---
# Source: dataplane/charts/fluentbit/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-fluentbit
  labels:
    helm.sh/chart: fluentbit-0.48.9
    app.kubernetes.io/name: fluentbit
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "3.2.8"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-fluentbit
subjects:
  - kind: ServiceAccount
    name: fluentbit-system
    namespace: union
---
# Source: dataplane/charts/opencost/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: opencost
  labels:
    helm.sh/chart: opencost-1.42.0
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.111.0"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: opencost
subjects:
  - kind: ServiceAccount
    name: opencost
    namespace: union
---
# Source: dataplane/charts/prometheus/charts/kube-state-metrics/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-5.33.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.15.0"
    release: release-name
  name: release-name-kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-kube-state-metrics
subjects:
- kind: ServiceAccount
  name: release-name-kube-state-metrics
  namespace: kube-system
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus-operator
  labels:
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app: prometheus-operator
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus-operator
subjects:
- kind: ServiceAccount
  name: prometheus-operator
  namespace: union
---
# Source: dataplane/charts/prometheus/templates/prometheus/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: union-operator-prometheus
  labels:
    app: prometheus-prometheus
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: union-operator-prometheus
subjects:
  - kind: ServiceAccount
    name: union-operator-prometheus
    namespace: union
---
# Source: dataplane/templates/clusterresourcesync/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: clustersync-resource
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: clustersync-resource
subjects:
  - kind: ServiceAccount
    name: clustersync-system
    namespace: union
---
# Source: dataplane/templates/clusterresourcesync/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: clustersync-auth-delegator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
  - kind: ServiceAccount
    name: clustersync-system
    namespace: union
---
# Source: dataplane/templates/nodeexecutor/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: union-executor
  labels:
    app: executor
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: union-executor
subjects:
- kind: ServiceAccount
  name: executor
  namespace: union
---
# Source: dataplane/templates/nodeobserver/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: union-nodeobserver
subjects:
- kind: ServiceAccount
  name: nodeobserver-system
  namespace: union
roleRef:
  kind: ClusterRole
  apiGroup: rbac.authorization.k8s.io
  name: union-nodeobserver
---
# Source: dataplane/templates/operator/serviceaccount-proxy.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: proxy-system
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: proxy-system
subjects:
  - kind: ServiceAccount
    name: proxy-system
    namespace: union
---
# Source: dataplane/templates/operator/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: operator-system
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: operator-system
subjects:
  - kind: ServiceAccount
    name: operator-system
    namespace: union
---
# Source: dataplane/templates/propeller/serviceaccount-webhook.yaml
# Create a binding from Role -> ServiceAccount
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: flytepropeller-webhook-binding
  namespace: union
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flytepropeller-webhook-role
subjects:
  - kind: ServiceAccount
    name: flytepropeller-webhook-system
    namespace: union
---
# Source: dataplane/templates/propeller/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: flytepropeller-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flytepropeller-role
subjects:
  - kind: ServiceAccount
    name: flytepropeller-system
    namespace: union
---
# Source: dataplane/templates/operator/serviceaccount-proxy-secret.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: proxy-system-secret
  namespace: union
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - '*'
    resources:
      - secrets
    verbs:
      - get
      - list
      - create
      - update
      - delete
---
# Source: dataplane/templates/operator/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: operator-system
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - '*'
    resources:
      - secrets
      - deployments
    verbs:
      - get
      - list
      - watch
      - create
      - update
---
# Source: dataplane/templates/operator/serviceaccount-proxy-secret.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: proxy-system-secret
  namespace: union
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: proxy-system-secret
subjects:
  - kind: ServiceAccount
    name: proxy-system
    namespace: union
---
# Source: dataplane/templates/operator/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: operator-system
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: operator-system
subjects:
  - kind: ServiceAccount
    name: operator-system
    namespace: union
---
# Source: dataplane/charts/fluentbit/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-fluentbit
  namespace: union
  labels:
    helm.sh/chart: fluentbit-0.48.9
    app.kubernetes.io/name: fluentbit
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "3.2.8"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 2020
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: fluentbit
    app.kubernetes.io/instance: release-name
---
# Source: dataplane/charts/opencost/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: opencost
  namespace: union
  labels:
    helm.sh/chart: opencost-1.42.0
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.111.0"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: release-name
  type: "ClusterIP"
  ports:
    - name: http
      port: 9003
      targetPort: 9003
---
# Source: dataplane/charts/prometheus/charts/kube-state-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-kube-state-metrics
  namespace: kube-system
  labels:    
    helm.sh/chart: kube-state-metrics-5.33.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.15.0"
    release: release-name
  annotations:
spec:
  type: "ClusterIP"
  ports:
  - name: "http"
    protocol: TCP
    port: 8080
    targetPort: 8080
  
  selector:    
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
---
# Source: dataplane/charts/prometheus/templates/exporters/core-dns/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator-coredns
  labels:
    app: prometheus-coredns
    jobLabel: coredns
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
  namespace: kube-system
spec:
  clusterIP: None
  ports:
    - name: http-metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
  selector:
    k8s-app: kube-dns
---
# Source: dataplane/charts/prometheus/templates/exporters/kube-controller-manager/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator-kube-controller-manager
  labels:
    app: prometheus-kube-controller-manager
    jobLabel: kube-controller-manager
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
  namespace: kube-system
spec:
  clusterIP: None
  ports:
    - name: http-metrics
      port: 10257
      protocol: TCP
      targetPort: 10257
  selector:
    component: kube-controller-manager
  type: ClusterIP
---
# Source: dataplane/charts/prometheus/templates/exporters/kube-etcd/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator-kube-etcd
  labels:
    app: prometheus-kube-etcd
    jobLabel: kube-etcd
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
  namespace: kube-system
spec:
  clusterIP: None
  ports:
    - name: http-metrics
      port: 2381
      protocol: TCP
      targetPort: 2381
  selector:
    component: etcd
  type: ClusterIP
---
# Source: dataplane/charts/prometheus/templates/exporters/kube-proxy/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator-kube-proxy
  labels:
    app: prometheus-kube-proxy
    jobLabel: kube-proxy
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
  namespace: kube-system
spec:
  clusterIP: None
  ports:
    - name: http-metrics
      port: 10249
      protocol: TCP
      targetPort: 10249
  selector:
    k8s-app: kube-proxy
  type: ClusterIP
---
# Source: dataplane/charts/prometheus/templates/exporters/kube-scheduler/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator-kube-scheduler
  labels:
    app: prometheus-kube-scheduler
    jobLabel: kube-scheduler
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
  namespace: kube-system
spec:
  clusterIP: None
  ports:
    - name: http-metrics
      port: 10259
      protocol: TCP
      targetPort: 10259
  selector:
    component: kube-scheduler
  type: ClusterIP
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: prometheus-operator
  namespace: union
  labels:
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app: prometheus-operator
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator
spec:
  ports:
  - name: https
    port: 443
    targetPort: https
  selector:
    app: prometheus-operator
    release: "release-name"
  type: "ClusterIP"
---
# Source: dataplane/charts/prometheus/templates/prometheus/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator-prometheus
  namespace: union
  labels:
    app: prometheus-prometheus
    self-monitor: "true"
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
spec:
  ports:
  - name: http-web
    port: 80
    targetPort: 9090
  - name: reloader-web
    appProtocol: http
    port: 8080
    targetPort: reloader-web
  publishNotReadyAddresses: false
  selector:
    app.kubernetes.io/name: prometheus
    operator.prometheus.io/name: union-operator-prometheus
  sessionAffinity: None
  type: "ClusterIP"
---
# Source: dataplane/templates/imagebuilder/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator-buildkit
  labels:
    app.kubernetes.io/name: imagebuilder-buildkit
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 1234
      targetPort: tcp
      protocol: TCP
      name: tcp
  selector:
    app.kubernetes.io/name: imagebuilder-buildkit
    app.kubernetes.io/instance: release-name
---
# Source: dataplane/templates/nodeexecutor/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-dataplane-executor
  labels:
    app: executor
spec:
  type: ClusterIP
  ports:
    - port: 15605
      targetPort: 15605
      protocol: TCP
      name: fasttask
  selector:
    app: executor
---
# Source: dataplane/templates/operator/service-proxy.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator-proxy
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
    - port: 10254
      targetPort: debug
      protocol: TCP
      name: debug
  selector:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
---
# Source: dataplane/templates/operator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: debug
      protocol: TCP
      name: debug
  selector:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
---
# Source: dataplane/templates/propeller/service-webhook.yaml
apiVersion: v1
kind: Service
metadata:
  name: flyte-pod-webhook
  namespace: union
  labels:
    app.kubernetes.io/name: flyte-pod-webhook
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
    projectcontour.io/upstream-protocol.h2c: grpc
spec:
  selector:
    app.kubernetes.io/name: flyte-pod-webhook
    app.kubernetes.io/instance: release-name
  ports:
    - name: https
      protocol: TCP
      port: 443
      targetPort: 9443
    - name: debug
      protocol: TCP
      port: 10254
      targetPort: 10254
---
# Source: dataplane/templates/propeller/service.yaml
apiVersion: v1
kind: Service
metadata:
  namespace: union
  name: flytepropeller
  labels:
    app.kubernetes.io/name: flytepropeller
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: debug
      protocol: TCP
      port: 10254
    - name: fasttask
      port: 15605
      protocol: TCP
      targetPort: 15605
  selector:
    app.kubernetes.io/name: flytepropeller
    app.kubernetes.io/instance: release-name
---
# Source: dataplane/charts/fluentbit/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-fluentbit
  namespace: union
  labels:
    helm.sh/chart: fluentbit-0.48.9
    app.kubernetes.io/name: fluentbit
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "3.2.8"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: fluentbit
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: fluentbit
        app.kubernetes.io/instance: release-name
      annotations:
        checksum/config: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
    spec:
      serviceAccountName: fluentbit-system
      hostNetwork: false
      dnsPolicy: ClusterFirst
      containers:
        - name: fluentbit
          image: "cr.fluentbit.io/fluent/fluent-bit:3.2.8"
          imagePullPolicy: IfNotPresent
          command:
            - /fluent-bit/bin/fluent-bit
          args:
            - --workdir=/fluent-bit/etc
            - --config=/fluent-bit/etc/conf/fluent-bit.conf
          ports:
            - name: http
              containerPort: 2020
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /api/v1/health
              port: http
          volumeMounts:
            - name: config
              mountPath: /fluent-bit/etc/conf
            - mountPath: /var/log
              name: varlog
            - mountPath: /var/lib/docker/containers
              name: varlibdockercontainers
              readOnly: true
            - mountPath: /etc/machine-id
              name: etcmachineid
              readOnly: true
      volumes:
        - name: config
          configMap:
            name: fluentbit-system
        - hostPath:
            path: /var/log
          name: varlog
        - hostPath:
            path: /var/lib/docker/containers
          name: varlibdockercontainers
        - hostPath:
            path: /etc/machine-id
            type: File
          name: etcmachineid
      tolerations:
        - operator: Exists
---
# Source: dataplane/templates/nodeobserver/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: union-nodeobserver
  labels:
    app.kubernetes.io/name: nodeobserver
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: nodeobserver
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        configChecksum: "53588c33a77619d28361e2b7ac9d8d903680ecb12be9214e4adb74210f52d03"
        
      labels:
        
        app.kubernetes.io/name: nodeobserver
        app.kubernetes.io/instance: release-name
        platform.union.ai/service-group: release-name
        app.kubernetes.io/managed-by: Helm
    spec:
      priorityClassName: 
      serviceAccountName: nodeobserver-system
      securityContext:
        {}
      volumes:
        - name: config-volume
          configMap:
            name: union-nodeobserver
      containers:
        - name: node-observer
          securityContext:
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
            runAsNonRoot: false
            runAsUser: 0
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2026.2.12"
          imagePullPolicy: IfNotPresent
          terminationMessagePolicy: FallbackToLogsOnError
          resources:
            limits:
              cpu: "1"
              memory: 500Mi
            requests:
              cpu: 500m
              memory: 100Mi
          volumeMounts:
            - mountPath: /etc/union
              name: config-volume
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.cpu
          - name: CLUSTER_NAME
            valueFrom:
              secretKeyRef:
                name: operator-cluster-name
                key: cluster_name
          - name: DEPLOYMENT_NAME
            value: operator
          - name: PROXY_SERVICE_URL
            value: http://union-operator-proxy:8080
          - name: PROMETHEUS_SERVICE_URL
            value: http://union-operator-prometheus:80
          - name: KNATIVE_PROXY_SERVICE_URL
            value: http://kourier-internal
          - name: KUBE_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: LOG_LEVEL
            value: "4"
          args:
            - nodeobserver
            - --config
            - /etc/union/config.yaml
---
# Source: dataplane/charts/opencost/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opencost
  namespace: union
  labels:
    helm.sh/chart: opencost-1.42.0
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.111.0"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: opencost
      app.kubernetes.io/instance: release-name
  strategy: 
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: opencost
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: opencost
      containers:
        - name: opencost
          image: ghcr.io/opencost/opencost:1.111.0@sha256:6aa68e52a24b14ba41f23db08d1b9db1429a1c0300f4c0381ecc2c61fc311a97
          imagePullPolicy: IfNotPresent
          args:
          ports:
            - containerPort: 9003
              name: http
          resources:
            limits:
              cpu: 1000m
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 1Gi
          startupProbe:
            httpGet:
              path: /healthz
              port: 9003
            initialDelaySeconds: 10
            periodSeconds: 5
            failureThreshold: 30
          livenessProbe:
            httpGet:
              path: /healthz
              port: 9003
            initialDelaySeconds: 10
            periodSeconds: 20
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /healthz
              port: 9003
            initialDelaySeconds: 10
            periodSeconds: 10
            failureThreshold: 3
          env:
            - name: LOG_LEVEL
              value: info
            - name: CUSTOM_COST_ENABLED
              value: "false"
            - name: KUBECOST_NAMESPACE
              value: union
            - name: API_PORT
              value: "9003"
            - name: PROMETHEUS_SERVER_ENDPOINT
              value: "http://union-operator-prometheus.union.svc:80/prometheus"
            - name: CLUSTER_ID
              value: "default-cluster"
            - name: DATA_RETENTION_DAILY_RESOLUTION_DAYS
              value: "15"
            - name: CLOUD_COST_ENABLED
              value: "false"
            - name: CLOUD_COST_MONTH_TO_DATE_INTERVAL
              value: "6"
            - name: CLOUD_COST_REFRESH_RATE_HOURS
              value: "6"
            - name: CLOUD_COST_QUERY_WINDOW_DAYS
              value: "7"
            - name: CLOUD_COST_RUN_WINDOW_DAYS
              value: "3"
            # Add any additional provided variables
---
# Source: dataplane/charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-kube-state-metrics
  namespace: kube-system
  labels:    
    helm.sh/chart: kube-state-metrics-5.33.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.15.0"
    release: release-name
spec:
  selector:
    matchLabels:      
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/instance: release-name
  replicas: 1
  strategy:
    type: RollingUpdate
  revisionHistoryLimit: 10
  template:
    metadata:
      labels:        
        helm.sh/chart: kube-state-metrics-5.33.2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: kube-state-metrics
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "2.15.0"
        release: release-name
    spec:
      automountServiceAccountToken: true
      hostNetwork: false
      serviceAccountName: release-name-kube-state-metrics
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      dnsPolicy: ClusterFirst
      containers:
      - name: kube-state-metrics
        args:
        - --port=8080
        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
        imagePullPolicy: IfNotPresent
        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.15.0
        ports:
        - containerPort: 8080
          name: "http"
        livenessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders:
            path: /livez
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders:
            path: /readyz
            port: 8081
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          {}
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-operator
  namespace: union
  labels:
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app: prometheus-operator
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: prometheus-operator
      release: "release-name"
  template:
    metadata:
      labels:
        
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "72.9.1"
        app.kubernetes.io/part-of: prometheus
        chart: prometheus-72.9.1
        release: "release-name"
        heritage: "Helm"
        app: prometheus-operator
        app.kubernetes.io/name: prometheus-prometheus-operator
        app.kubernetes.io/component: prometheus-operator
    spec:
      containers:
        - name: prometheus
          image: "quay.io/prometheus-operator/prometheus-operator:v0.82.2"
          imagePullPolicy: "IfNotPresent"
          args:
            - --kubelet-service=kube-system/union-operator-kubelet
            - --kubelet-endpoints=true
            - --kubelet-endpointslice=false
            - --localhost=127.0.0.1
            - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.82.2
            - --config-reloader-cpu-request=0
            - --config-reloader-cpu-limit=0
            - --config-reloader-memory-request=0
            - --config-reloader-memory-limit=0
            - --thanos-default-base-image=quay.io/thanos/thanos:v0.38.0
            - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
            - --web.enable-tls=true
            - --web.cert-file=/cert/cert
            - --web.key-file=/cert/key
            - --web.listen-address=:10250
            - --web.tls-min-version=VersionTLS13
          ports:
            - containerPort: 10250
              name: https
          env:
          - name: GOGC
            value: "30"
          resources:
            {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          volumeMounts:
            - name: tls-secret
              mountPath: /cert
              readOnly: true
          readinessProbe:
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
          livenessProbe:
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
      volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: union-operator-admission
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: prometheus-operator
      automountServiceAccountToken: true
      terminationGracePeriodSeconds: 30
---
# Source: dataplane/templates/clusterresourcesync/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: syncresources
  namespace: union
  labels:
    app.kubernetes.io/name: clusterresourcesync
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: clusterresourcesync
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        configChecksum: "ef57effb8a4388e02ca6dc0593b7546e36e7175dca3885c92c4f645b8bfaaf3"
        
      labels:
        
        app.kubernetes.io/name: clusterresourcesync
        app.kubernetes.io/instance: release-name
        platform.union.ai/service-group: release-name
        app.kubernetes.io/managed-by: Helm
    spec:
      containers:
        - command:
            - clusterresource
            - --config
            - /etc/flyte/config/*.yaml
            - clusterresource
            - run
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.cpu
          - name: CLUSTER_NAME
            valueFrom:
              secretKeyRef:
                name: operator-cluster-name
                key: cluster_name
          - name: DEPLOYMENT_NAME
            value: operator
          - name: PROXY_SERVICE_URL
            value: http://union-operator-proxy:8080
          - name: PROMETHEUS_SERVICE_URL
            value: http://union-operator-prometheus:80
          - name: KNATIVE_PROXY_SERVICE_URL
            value: http://kourier-internal
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2026.2.12"
          imagePullPolicy: "IfNotPresent"
          name: sync-cluster-resources
          resources:
            limits:
              cpu: "1"
              memory: 500Mi
            requests:
              cpu: 500m
              memory: 100Mi
          volumeMounts:
            - name: auth
              mountPath: /etc/union/secret
            - name: resource-templates
              mountPath: /etc/flyte/clusterresource/templates
            - name: config-volume
              mountPath: /etc/flyte/config
          ports:
            - containerPort: 10254
      serviceAccountName: clustersync-system
      volumes:
        - configMap:
            name: clusterresource-template
          name: resource-templates
        - configMap:
            name: flyte-clusterresourcesync-config
          name: config-volume
        - name: auth
          secret:
            secretName: union-secret-auth
---
# Source: dataplane/templates/imagebuilder/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: union-operator-buildkit
  labels:
    app.kubernetes.io/name: imagebuilder-buildkit
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  strategy:
    type: Recreate
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: imagebuilder-buildkit
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        container.apparmor.security.beta.kubernetes.io/buildkit: unconfined
      labels:
        app.kubernetes.io/name: imagebuilder-buildkit
        app.kubernetes.io/instance: release-name
    spec:
      containers:
        - name: "buildkit"
          image: "moby/buildkit:buildx-stable-1-rootless"
          imagePullPolicy: IfNotPresent
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  divisor: 1
                  resource: limits.memory
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  divisor: 1
                  resource: limits.cpu
            - name: CLUSTER_NAME
              valueFrom:
                secretKeyRef:
                  name: operator-cluster-name
                  key: cluster_name
            - name: DEPLOYMENT_NAME
              value: operator
            - name: PROXY_SERVICE_URL
              value: http://union-operator-proxy:8080
            - name: PROMETHEUS_SERVICE_URL
              value: http://union-operator-prometheus:80
            - name: KNATIVE_PROXY_SERVICE_URL
              value: http://kourier-internal
          volumeMounts:
            - mountPath: /home/user/.local/share/buildkit
              name: buildkitd
            - mountPath: /etc/buildkit
              name: buildkit-config
          args:
            - --config
            - /etc/buildkit/buildkitd.toml
            - --addr
            - unix:///run/user/1000/buildkit/buildkitd.sock
            - --addr
            - tcp://0.0.0.0:1234
            - --oci-worker-no-process-sandbox
          ports:
            - name: tcp
              containerPort: 1234
              protocol: TCP
          readinessProbe:
            exec:
              command:
              - buildctl
              - debug
              - workers
            initialDelaySeconds: 5
            periodSeconds: 30
          livenessProbe:
            exec:
              command:
              - buildctl
              - debug
              - workers
            initialDelaySeconds: 5
            periodSeconds: 30
          securityContext:
            seccompProfile: # Needs Kubernetes >= 1.19
              type: Unconfined
            runAsUser: 1000
            runAsGroup: 1000
          resources:
            requests:
              cpu: 1
              ephemeral-storage: 20Gi
              memory: 1Gi
      volumes:
      - name: buildkitd
        emptyDir: {}
      - configMap:
          name: union-operator-buildkit
        name: buildkit-config
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/name: imagebuilder-buildkit
                app.kubernetes.io/instance: release-name
            topologyKey: "kubernetes.io/hostname"
---
# Source: dataplane/templates/nodeexecutor/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: executor
  namespace: union
  labels:
    app: executor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: executor
  template:
    metadata:
      annotations:
        configChecksum: "8683fc84a121ff0f0c39cefe5053de1bb0d12da999ffc27c2cde56aa561c425"
      labels:
        
        app: executor
    spec:
      securityContext:
        fsGroup: 1337
      serviceAccountName: executor
      volumes:
        - name: config-volume
          configMap:
            name: executor
        - name: secret-volume
          secret:
            secretName: union-secret-auth
        - name: auth
          secret:
            secretName: union-secret-auth
      containers:
        - name: executor
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2026.2.12"
          imagePullPolicy: IfNotPresent
          command:
            - executor
            - serve
            - --config
            - /etc/config/*.yaml
          ports:
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: metrics
              containerPort: 10254
              protocol: TCP
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  divisor: 1
                  resource: limits.memory
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  divisor: 1
                  resource: limits.cpu
            - name: CLUSTER_NAME
              valueFrom:
                secretKeyRef:
                  name: operator-cluster-name
                  key: cluster_name
            - name: DEPLOYMENT_NAME
              value: operator
            - name: PROXY_SERVICE_URL
              value: http://union-operator-proxy:8080
            - name: PROMETHEUS_SERVICE_URL
              value: http://union-operator-prometheus:80
            - name: KNATIVE_PROXY_SERVICE_URL
              value: http://kourier-internal
          resources:
            limits:
              cpu:    "4"
              memory: "8Gi"
            requests:
              cpu:    "1"
              memory: "1Gi"
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: secret-volume
              mountPath: /etc/union/secret
            - name: auth
              mountPath: /etc/secrets/
---
# Source: dataplane/templates/operator/deployment-proxy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: union-operator-proxy
  namespace: union
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: operator-proxy
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        configChecksum: "ed595cc567241e7a3afe291439b335c127b008a1c5d471ca8622ec0796fa1c0"
        
      labels:
        
        app.kubernetes.io/name: operator-proxy
        app.kubernetes.io/instance: release-name
        platform.union.ai/service-group: release-name
        app.kubernetes.io/managed-by: Helm
    spec:
      volumes:
        - name: config-volume
          projected:
            sources:
            - configMap:
                name: union-operator
            - configMap:
                name: flyte-clusterresourcesync-config
        - name: secret-volume
          secret:
            secretName: union-secret-auth
      serviceAccountName: proxy-system
      securityContext:
        {}
      containers:
        - name: operator-proxy
          securityContext:
            {}
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2026.2.12"
          imagePullPolicy: IfNotPresent
          terminationMessagePolicy: FallbackToLogsOnError
          resources:
            limits:
              cpu: "3"
              memory: 3Gi
            requests:
              cpu: 500m
              memory: 500Mi
          volumeMounts:
            - mountPath: /etc/union/config
              name: config-volume
            - mountPath: /etc/union/secret
              name: secret-volume
          args:
            - operator
            - proxy
            - --config
            - /etc/union/config/*.yaml
          ports:
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: connect
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 8081
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.cpu
          - name: CLUSTER_NAME
            valueFrom:
              secretKeyRef:
                name: operator-cluster-name
                key: cluster_name
          - name: DEPLOYMENT_NAME
            value: operator
          - name: PROXY_SERVICE_URL
            value: http://union-operator-proxy:8080
          - name: PROMETHEUS_SERVICE_URL
            value: http://union-operator-prometheus:80
          - name: KNATIVE_PROXY_SERVICE_URL
            value: http://kourier-internal
        - name: "tunnel"
          securityContext:
            {}
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2026.2.12"
          imagePullPolicy: IfNotPresent
          args:
            - cloudflared
            - tunnel
            - --no-autoupdate
            - run
            - --protocol=http2
            - --token
            - $(TUNNEL_TOKEN)
          env:
            - name: TUNNEL_TOKEN
              valueFrom:
                secretKeyRef:
                  name: union-secret-auth
                  key: tunnel_token
                  optional: true
          resources:
            limits:
              cpu: "3"
              memory: 3Gi
            requests:
              cpu: 500m
              memory: 500Mi
---
# Source: dataplane/templates/operator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: union-operator
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: union-operator
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        configChecksum: "ed595cc567241e7a3afe291439b335c127b008a1c5d471ca8622ec0796fa1c0"
        
      labels:
        
        app.kubernetes.io/name: union-operator
        app.kubernetes.io/instance: release-name
        platform.union.ai/service-group: release-name
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: operator-system
      securityContext:
        {}
      volumes:
        - name: config-volume
          configMap:
            name: union-operator
        - name: secret-volume
          secret:
            secretName: union-secret-auth
      containers:
        - name: operator
          securityContext:
            {}
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2026.2.12"
          imagePullPolicy: IfNotPresent
          terminationMessagePolicy: FallbackToLogsOnError
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: "1"
              memory: 1Gi
          volumeMounts:
            - mountPath: /etc/union/config
              name: config-volume
            - mountPath: /etc/union/secret
              name: secret-volume
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.cpu
          - name: CLUSTER_NAME
            valueFrom:
              secretKeyRef:
                name: operator-cluster-name
                key: cluster_name
          - name: DEPLOYMENT_NAME
            value: operator
          - name: PROXY_SERVICE_URL
            value: http://union-operator-proxy:8080
          - name: PROMETHEUS_SERVICE_URL
            value: http://union-operator-prometheus:80
          - name: KNATIVE_PROXY_SERVICE_URL
            value: http://kourier-internal
          args:
            - operator
            - serve
            - --config
            - /etc/union/config/*.yaml
            - --operator.clusterId.name
            - "$(CLUSTER_NAME)"
            - --operator.tunnel.k8sSecretName
            - union-secret-auth
          ports:
            - name: grpc
              containerPort: 8080
              protocol: TCP
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
---
# Source: dataplane/templates/propeller/deployment-webhook.yaml
# Create the actual deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flytepropeller-webhook
  namespace: union
  labels:
    app.kubernetes.io/name: flyte-pod-webhook
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flyte-pod-webhook
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        
        app.kubernetes.io/name: flyte-pod-webhook
        app.kubernetes.io/instance: release-name
        platform.union.ai/service-group: release-name
        app.kubernetes.io/managed-by: Helm
      annotations:
        configChecksum: "915dd1c16576bf7f84b7293876a05f5b66993798003e1ce6a9fc2da8a861b26"
        
    spec:
      securityContext:
        fsGroup: 65534
        fsGroupChangePolicy: Always
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions:
          type: spc_t
      serviceAccountName: flytepropeller-webhook-system
      initContainers:
        - name: generate-secrets
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2026.2.12"
          imagePullPolicy: "IfNotPresent"
          command:
            - flytepropeller
          args:
            - webhook
            - init-certs
            - --config
            - /etc/flyte/config/*.yaml
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.cpu
          - name: CLUSTER_NAME
            valueFrom:
              secretKeyRef:
                name: operator-cluster-name
                key: cluster_name
          - name: DEPLOYMENT_NAME
            value: operator
          - name: PROXY_SERVICE_URL
            value: http://union-operator-proxy:8080
          - name: PROMETHEUS_SERVICE_URL
            value: http://union-operator-prometheus:80
          - name: KNATIVE_PROXY_SERVICE_URL
            value: http://kourier-internal
          volumeMounts:
            - name: config-volume
              mountPath: /etc/flyte/config
          resources:
            limits:
              cpu: 1
              ephemeral-storage: 500Mi
              memory: 500Mi
            requests:
              cpu: 200m
              ephemeral-storage: 500Mi
              memory: 500Mi
      containers:
        - name: webhook
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2026.2.12"
          imagePullPolicy: "IfNotPresent"
          command:
            - flytepropeller
          args:
            - webhook
            - --config
            - /etc/flyte/config/*.yaml
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.cpu
          - name: CLUSTER_NAME
            valueFrom:
              secretKeyRef:
                name: operator-cluster-name
                key: cluster_name
          - name: DEPLOYMENT_NAME
            value: operator
          - name: PROXY_SERVICE_URL
            value: http://union-operator-proxy:8080
          - name: PROMETHEUS_SERVICE_URL
            value: http://union-operator-prometheus:80
          - name: KNATIVE_PROXY_SERVICE_URL
            value: http://kourier-internal
          ports:
            - containerPort: 9443
            - containerPort: 10254
          resources:
            limits:
              cpu: 1
              ephemeral-storage: 500Mi
              memory: 500Mi
            requests:
              cpu: 200m
              ephemeral-storage: 500Mi
              memory: 500Mi
          volumeMounts:
            - name: config-volume
              mountPath: /etc/flyte/config
              readOnly: true
            - name: webhook-certs
              mountPath: /etc/webhook/certs
              readOnly: true
      volumes:
        - name: config-volume
          configMap:
            name: flyte-propeller-config
        - name: webhook-certs
          secret:
            secretName: flyte-pod-webhook
---
# Source: dataplane/templates/propeller/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: union
  name: flytepropeller
  labels:
    app.kubernetes.io/name: flytepropeller
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flytepropeller
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        configChecksum: "915dd1c16576bf7f84b7293876a05f5b66993798003e1ce6a9fc2da8a861b26"
        
      labels:
        
        
        app.kubernetes.io/name: flytepropeller
        app.kubernetes.io/instance: release-name
        platform.union.ai/service-group: release-name
        app.kubernetes.io/managed-by: Helm
    spec:
      priorityClassName: system-cluster-critical
      containers:
        - command:
            - flytepropeller
            - --config
            - /etc/flyte/config/*.yaml
            - --propeller.cluster-id
            - ''
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.cpu
          - name: CLUSTER_NAME
            valueFrom:
              secretKeyRef:
                name: operator-cluster-name
                key: cluster_name
          - name: DEPLOYMENT_NAME
            value: operator
          - name: PROXY_SERVICE_URL
            value: http://union-operator-proxy:8080
          - name: PROMETHEUS_SERVICE_URL
            value: http://union-operator-prometheus:80
          - name: KNATIVE_PROXY_SERVICE_URL
            value: http://kourier-internal
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2026.2.12"
          imagePullPolicy: "IfNotPresent"
          name: flytepropeller
          ports:
            - containerPort: 10254
          resources:
            limits:
              cpu: "3"
              memory: 3Gi
            requests:
              cpu: "1"
              memory: 1Gi
          volumeMounts:
            - name: config-volume
              mountPath: /etc/flyte/config
            - name: auth
              mountPath: /etc/union/secret
      serviceAccountName: flytepropeller-system
      volumes:
        - configMap:
            name: flyte-propeller-config
          name: config-volume
        - name: auth
          secret:
            secretName: union-secret-auth
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/admission-webhooks/mutatingWebhookConfiguration.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name:  union-operator-admission
  annotations:
    
  labels:
    app: prometheus-admission
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator-webhook
webhooks:
  - name: prometheusrulemutate.monitoring.coreos.com
    failurePolicy: Ignore
    rules:
      - apiGroups:
          - monitoring.coreos.com
        apiVersions:
          - "*"
        resources:
          - prometheusrules
        operations:
          - CREATE
          - UPDATE
    clientConfig:
      service:
        namespace: union
        name: prometheus-operator
        path: /admission-prometheusrules/mutate
    timeoutSeconds: 10
    admissionReviewVersions: ["v1", "v1beta1"]
    sideEffects: None
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/admission-webhooks/validatingWebhookConfiguration.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name:  union-operator-admission
  annotations:
    
  labels:
    app: prometheus-admission
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator-webhook
webhooks:
  - name: prometheusrulemutate.monitoring.coreos.com
    failurePolicy: Ignore
    rules:
      - apiGroups:
          - monitoring.coreos.com
        apiVersions:
          - "*"
        resources:
          - prometheusrules
        operations:
          - CREATE
          - UPDATE
    clientConfig:
      service:
        namespace: union
        name: prometheus-operator
        path: /admission-prometheusrules/validate
    timeoutSeconds: 10
    admissionReviewVersions: ["v1", "v1beta1"]
    sideEffects: None
---
# Source: dataplane/charts/prometheus/templates/prometheus/prometheus.yaml
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: union-operator-prometheus
  namespace: union
  labels:
    app: prometheus-prometheus
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
spec:
  automountServiceAccountToken: true
  image: "quay.io/prometheus/prometheus:v3.4.1"
  version: v3.4.1
  externalUrl: http://union-operator-prometheus.union:80
  paused: false
  replicas: 1
  shards: 1
  logLevel:  info
  logFormat:  logfmt
  listenLocal: false
  enableAdminAPI: false
  resources:
    limits:
      cpu: "3"
      memory: 3500Mi
    requests:
      cpu: "1"
      memory: 1Gi
  retention: "3d"
  tsdb:
    outOfOrderTimeWindow: 0s
  walCompression: true
  routePrefix: "/prometheus/"
  serviceAccountName: union-operator-prometheus
  serviceMonitorSelector: {}
  serviceMonitorNamespaceSelector: {}
  podMonitorSelector: {}
  podMonitorNamespaceSelector: {}
  probeSelector:
    matchLabels:
      release: "release-name"

  probeNamespaceSelector: {}
  securityContext:
    fsGroup: 2000
    runAsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
    seccompProfile:
      type: RuntimeDefault
  ruleNamespaceSelector: {}
  ruleSelector: {}
  scrapeConfigSelector: {}
  scrapeConfigNamespaceSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          topologyKey: kubernetes.io/hostname
          labelSelector:
            matchExpressions:            
            - {key: app.kubernetes.io/name, operator: In, values: [prometheus]}
            - {key: app.kubernetes.io/instance, operator: In, values: [union-operator-prometheus]}
  portName: http-web
  maximumStartupDurationSeconds: 900
  hostNetwork: false
---
# Source: dataplane/templates/monitoring/prometheusrule.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: union-opencost-rules
  namespace: union
  labels:
    release: release-name
spec:
  groups:
    - name: cost_calculations_15s
      interval: 15s
      rules:
        - record: pod_gpu_allocation
          expr: |
            sum by (namespace, pod) (DCGM_FI_DEV_GPU_UTIL >= bool 0) * on (namespace, pod) group_left() (max by (namespace, pod) (kube_pod_status_phase{phase=~"Running|Pending"} == 1))
        - record: execution_info # A join metric to look up execution-level info. Used to disambiguate workflow/task executions, apps, and workspaces.
          expr: |
            max by (label_domain, label_project, label_entity_name, label_execution_id, label_entity_id)(
              label_replace(
                label_replace(
                  label_replace(
                    label_replace(
                      label_replace(
                        flyte:propeller:all:round:execution_info{domain!="", project!="", workflow_name!="", execution_id!=""}, # filter for workflow/task executions
                        "label_entity_id", "$1", "execution_id", "(.*)" # join key
                      ), "label_entity_name", "$1", "workflow_name", "(.*)" # set label_entity_name to the workflow/task name from the workflow_execution_id
                    ),
                    "label_execution_id", "$1", "execution_id", "(.*)"
                  ),
                  "label_project", "$1", "project", "(.*)" # project
                ),
                "label_domain", "$1", "domain", "(.*)" # domain
              )
            )
        - record: app_info # A join metric to look up app-level info. Used to disambiguate workflow/task executions, apps, and workspaces.
          expr: |
            max by (label_domain, label_project, label_app_name, label_app_version, label_entity_id)(
              label_replace(
                label_replace(
                  label_replace(
                    kube_pod_labels{
                      label_domain!="",
                      label_project!="",
                      label_serving_unionai_dev_app_name!="",
                      label_serving_knative_dev_revision!=""
                    }, # this filters for apps
                    "label_app_name", "$1", "label_serving_unionai_dev_app_name", "(.*)" # rename to cleanup
                  ),
                  "label_app_version", "$1", "label_serving_knative_dev_revision", "(.*)" # the app_version is equivalent to an execution_id for workflows (lowest level of granularity)
                ),
                "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # join key
              )
            )
        - record: workspace_info # A join metric to look up workspace info. Used to disambiguate workflow/task executions, apps, and workspaces.
          expr: |
            max by (label_domain, label_project, label_workspace_name, label_entity_id)(
              label_replace(
                label_replace(
                  kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # filter for workspaces
                  "label_entity_id", "$1", "label_node_id", "(.*)" # join key
                ), "label_workspace_name", "$1", "label_node_id", "(.*)" # set label_workspace_name to the workspace name from the kube_pod_labels
              )
            )
        - record: entity_id:mem_usage_bytes_total_per_node:sum # Allocated memory (max(requested, consumed)) aggregated per node and entity, where entity is either a task/workflow execution or an app.
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, node) ( # aggregate up to entity
              # First, calculate the allocated memory for each pod
              max by (namespace, pod) ( # this is the case where consumed (the memory working set) exceeds requested memory
                (
                  sum by (namespace, pod) (
                    container_memory_working_set_bytes{namespace!="",pod!="",image!=""}
                  )
                  > sum by (namespace, pod) (
                    kube_pod_container_resource_requests{namespace!="", pod!="", node!="", resource="memory"}
                  )
                )
                or sum by (namespace, pod) ( # this is the case where memory requests are <= consumed memory
                  kube_pod_container_resource_requests{namespace!="", pod!="", node!="", resource="memory"} # needed to add node!="" to dedupe
                )
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but we do not want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
              # Now join in node identifiers which are used for subsequent overhead calculations
              * on (namespace, pod) group_left(node) (
                max by (namespace, pod, node) (kube_pod_info{node!=""}) # needed to add node!="" to dedupe
              )
            )
        - record: entity_id:cpu_usage_per_node:sum # Allocated cpu (max(requested, consumed)) aggregated per node and entity, where entity is either a task/workflow execution or an app.
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, node) (
              # First, calculate the allocated cpu for each pod
              max by (namespace, pod) ( # this is the case where consumed (the cpu usage seconds total) exceeds requested cpu
                (
                  sum by (namespace, pod) (
                    irate(container_cpu_usage_seconds_total{namespace!="",pod!="",image!=""}[5m])
                  )
                  > sum by (namespace, pod) (
                    kube_pod_container_resource_requests{namespace!="", pod!="", node!="", resource="cpu"}
                  )
                )
                or sum by (namespace, pod) ( # this is the case where cpu requests are <= consumed cpu
                    kube_pod_container_resource_requests{namespace!="", pod!="", node!="", resource="cpu"}
                )
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but I didn't want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
              # Now join in node identifiers which are used for subsequent overhead calculations
              * on (namespace, pod) group_left(node) (
                max by (namespace, pod, node) (kube_pod_info{node!=""}) # needed to add node!="" to dedupe
              )
            )
        - record: entity_id:gpu_usage_per_node:sum # Allocated gpu aggregated per node and entity, where entity is either a task/workflow execution or an app.
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, node) (
              # First, grab the allocated gpu for each pod (which is always either 1 or zero, since k8s can't split gpus the way it can with cpu/memory)
              max by (namespace, pod) (
                pod_gpu_allocation
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but I didn't want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
              # Now join in node identifiers which are used for subsequent overhead calculations
              * on (namespace, pod) group_left(node) (
                max by (namespace, pod, node) (kube_pod_info{node!=""}) # needed to add node!="" to dedupe
              )
            )
        - record: entity_id:used_mem_bytes:sum # the sum of used memory across all containers in an entity (numerator for aggregate utilization calculations)
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id) ( # aggregate up to entity
              # First, calculate the used memory for each pod
              sum by (namespace, pod) (
                container_memory_working_set_bytes{namespace!="",pod!="",image!=""}
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but we do not want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
            )
        - record: entity_id:allocated_mem_bytes:sum # the sum of allocated memory across all containers in an entity (denominator for aggregate utilization calculations)
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id) ( # aggregate up to entity (remove node)
              entity_id:mem_usage_bytes_total_per_node:sum
            )
        - record: entity_id:used_cpu:sum # the sum of used cpu across all containers in an entity (numerator for aggregate utilization calculations)
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id) (
              sum by (namespace, pod) (
                irate(container_cpu_usage_seconds_total{namespace!="",pod!="",image!=""}[5m])
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but I didn't want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
            )
        - record: entity_id:allocated_cpu:sum # the sum of allocated cpu across all containers in an entity (denominator for aggregate utilization calculations)
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id) ( # aggregate up to entity (remove node)
              entity_id:cpu_usage_per_node:sum
            )
        - record: entity_id:sm_occupancy:avg # the simple average of SM occupancy (a good generic measure of GPU utilization) per entity
          expr: |
            avg by (label_entity_type, label_domain, label_project, label_entity_id) (
              # First, grab the SM occupancy for each pod
              max by (namespace, pod) (
                DCGM_FI_PROF_SM_OCCUPANCY # SM occupancy is a good proxy for actual GPU usage
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but I didn't want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
            )
        - record: entity_id:gpu_count:sum # the count of running gpu pods per entity (need this to weight the gpu utilization when aggregating upwards - i.e. project-level)
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, node) (
              # First, grab the allocated gpu for each pod (which is always either 1 or zero, since k8s can't split gpus the way it can with cpu/memory)
              max by (namespace, pod) (
                pod_gpu_allocation
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but I didn't want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
            )
        - record: entity_id:weighted_sm_occupancy:sum # product of SM occupancy and allocated GPU count (something like "used memory", numerator of weighted calcs)
          expr: |
            entity_id:sm_occupancy:avg
            * on (label_domain, label_project, label_entity_type, label_entity_id) entity_id:gpu_count:sum
        - record: entity_id:allocated_mem_cost:sum # Allocated cost of memory for each workflow/task execution and app.
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, type) (
              entity_id:mem_usage_bytes_total_per_node:sum / (1024 * 1024 * 1024) # convert bytes to GB
              * on (node) group_left(type) label_replace(avg by (node) (node_ram_hourly_cost * (15 / 3600)), "type", "mem", "", "") # convert hourly cost to 15-secondly cost and add type
            )
        - record: entity_id:allocated_cpu_cost:sum # Allocated cost of cpu for each workflow/task execution and app.
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, type)(
              entity_id:cpu_usage_per_node:sum
              * on (node) group_left(type) label_replace(avg by (node) (node_cpu_hourly_cost * (15 / 3600)), "type", "cpu", "", "") # convert hourly cost to 15-secondly cost and add type
            )
        - record: entity_id:allocated_gpu_cost:sum # Allocated cost of gpu for each workflow/task execution and app.
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, type)(
              entity_id:gpu_usage_per_node:sum
              * on (node) group_left(type) label_replace(avg by (node) (node_gpu_hourly_cost * (15 / 3600)), "type", "gpu", "", "") # convert hourly cost to 15-secondly cost and add type
            )
        - record: entity_id:allocated_cost:sum # Allocated cost of memory, cpu, and gpu for each workflow/task execution and app.
          expr: |
            label_replace(
              sum by (label_entity_type, label_domain, label_project, label_entity_id) ( # for the sum to work, the labels need to be different on each "or" element (type label)
                entity_id:allocated_mem_cost:sum
                or
                entity_id:allocated_cpu_cost:sum
                or
                entity_id:allocated_gpu_cost:sum
              ),
              "type", "allocated", "", "" # add type info
            )
        - record: entity_id:overhead_cost:sum # The amount of overhead costs (node costs that we can't allocate with container resources) to allocate to each entity (workflow/task execution or app)
          expr: |
            label_replace(
              sum by (label_entity_type, label_entity_id, label_domain, label_project)( # Aggregate the per-node metrics up to workflow/task execution or app (label_entity_id)
                # Start with each execution's and app's allocated cost per node
                sum by (label_entity_type, label_domain, label_project, label_entity_id, node) ( # for the sum to work, the labels need to be different on each "or" element (type label)
                  entity_id:mem_usage_bytes_total_per_node:sum / (1024 * 1024 * 1024) # convert bytes to GB
                  * on (node) group_left(type) label_replace(avg by (node) (node_ram_hourly_cost * (15 / 3600)), "type", "mem", "", "") # convert hourly cost to 15-secondly cost and add type
                  or
                  entity_id:cpu_usage_per_node:sum
                  * on (node) group_left(type) label_replace(avg by (node) (node_cpu_hourly_cost * (15 / 3600)), "type", "cpu", "", "") # convert hourly cost to 15-secondly cost and add type
                  or
                  entity_id:gpu_usage_per_node:sum
                  * on (node) group_left(type) label_replace(avg by (node) (node_gpu_hourly_cost * (15 / 3600)), "type", "gpu", "", "") # convert hourly cost to 15-secondly cost and add type
                )
                # Then divide out the total allocated cost per node to get the proportion of allocated cost associated with each entity
                / on (node) group_left()(
                  sum by (node) ( # for the sum to work, the labels need to be different on each "or" element (type label)
                    entity_id:mem_usage_bytes_total_per_node:sum / (1024 * 1024 * 1024) # convert bytes to GB
                    * on (node) group_left(type) label_replace(avg by (node) (node_ram_hourly_cost * (15 / 3600)), "type", "mem", "", "") # convert hourly cost to 15-secondly cost and add type
                    or
                    entity_id:cpu_usage_per_node:sum
                    * on (node) group_left(type) label_replace(avg by (node) (node_cpu_hourly_cost * (15 / 3600)), "type", "cpu", "", "") # convert hourly cost to 15-secondly cost and add type
                    or
                    entity_id:gpu_usage_per_node:sum
                    * on (node) group_left(type) label_replace(avg by (node) (node_gpu_hourly_cost * (15 / 3600)), "type", "gpu", "", "") # convert hourly cost to 15-secondly cost and add type
                  )
                  > 0 # need to avoid dividing by zero, or gaps in the data can cause NaNs to proliferate, borking all charts
                )
                # Then multiply by the overhead cost per node
                * on (node) group_left() (
                  # To calculate overhead, start with the true cost of running each node
                  avg by (node)(kube_node_labels{label_flyte_org_node_role="worker"}) # only look at worker nodes
                  * on (node) max by (node) (
                    node_total_hourly_cost{instance_type!=""} # sometimes, the instance_type can be null, causing an unlabeled label to show up in the Compute Costs dashboard charts
                  ) * (15 / 3600) # convert hourly cost to 15-secondly cost
                  # Then subtract out the total allocated cost on each node
                  - on (node) group_left()(
                    sum by (node) ( # for the sum to work, the labels need to be different on each "or" element (type label)
                      entity_id:mem_usage_bytes_total_per_node:sum / (1024 * 1024 * 1024) # convert bytes to GB
                      * on (node) group_left(type) label_replace(avg by (node) (node_ram_hourly_cost * (15 / 3600)), "type", "mem", "", "") # convert hourly cost to 15-secondly cost and add type
                      or
                      entity_id:cpu_usage_per_node:sum
                      * on (node) group_left(type) label_replace(avg by (node) (node_cpu_hourly_cost * (15 / 3600)), "type", "cpu", "", "") # convert hourly cost to 15-secondly cost and add type
                      or
                      entity_id:gpu_usage_per_node:sum
                      * on (node) group_left(type) label_replace(avg by (node) (node_gpu_hourly_cost * (15 / 3600)), "type", "gpu", "", "") # convert hourly cost to 15-secondly cost and add type
                    )
                  )
                )
              ),
              "type", "overhead", "", "" # add type info
            )
        - record: entity_id:total_cost:sum # Total cost of each entity (workflow/task execution or app), including allocated (from container resources) and overhead (proportion of unallocated node costs)
          expr: |
            label_replace(
              sum by (label_domain, label_project, label_entity_id, label_entity_type) (
                entity_id:allocated_cost:sum
                or
                entity_id:overhead_cost:sum
              ),
              "type", "total", "", "" # add type info
            )
        - record: node:total_cost:sum # Total cost of all nodes
          expr: |
            sum (
              avg by (node)(kube_node_labels{label_flyte_org_node_role="worker", label_node_kubernetes_io_instance_type!=""}) # only look at worker nodes
              * on (node) group_left() node_total_hourly_cost{instance_type!=""} * (15 / 3600) # convert hourly cost to 15-secondly cost
            )
        - record: node_type:total_cost:sum # Total cost of nodes grouped by node type
          expr: |
            sum by (node_type)(
              avg by (node)(kube_node_labels{label_flyte_org_node_role="worker", label_node_kubernetes_io_instance_type!=""}) # only look at worker nodes
              * on (node) group_left(node_type) label_replace(node_total_hourly_cost{instance_type!=""}, "node_type", "$1", "instance_type", "(.*)") * (15 / 3600) # convert hourly cost to 15-secondly cost and rename label
            )
        - record: node_type:uptime_hours:sum # Total uptime of nodes grouped by node type
          expr: |
            sum by (node_type)(
              avg by (node, node_type)( # dedupe
                label_replace(kube_node_labels{label_flyte_org_node_role="worker", label_node_kubernetes_io_instance_type!=""}, "node_type", "$1", "label_node_kubernetes_io_instance_type", "(.*)") # relabel
              )
            ) * (15 / 3600) # convert to number of hours per 15-second observation      # Aggregate the above into visible metrics
    - name: cost_rollup_15m
      interval: 15m
      rules:
        - record: execution_info15m
          expr: |
            max_over_time(execution_info[15m:15s])
        - record: app_info15m
          expr: |
            max_over_time(app_info[15m:15s])
        - record: workspace_info15m
          expr: |
            max_over_time(workspace_info[15m:15s])
        - record: entity_id:allocated_mem_bytes:sum15m
          expr: |
            sum_over_time(entity_id:allocated_mem_bytes:sum[15m:15s])
        - record: entity_id:used_mem_bytes:sum15m
          expr: |
            sum_over_time(entity_id:used_mem_bytes:sum[15m:15s])
        - record: entity_id:allocated_cpu:sum15m
          expr: |
            sum_over_time(entity_id:allocated_cpu:sum[15m:15s])
        - record: entity_id:used_cpu:sum15m
          expr: |
            sum_over_time(entity_id:used_cpu:sum[15m:15s])
        - record: entity_id:weighted_sm_occupancy:sum15m
          expr: |
            sum_over_time(entity_id:weighted_sm_occupancy:sum[15m:15s])
        - record: entity_id:gpu_count:sum15m
          expr: |
            sum_over_time(entity_id:gpu_count:sum[15m:15s])
        - record: entity_id:allocated_mem_cost:sum15m
          expr: |
            sum_over_time(entity_id:allocated_mem_cost:sum[15m:15s])
        - record: entity_id:allocated_cpu_cost:sum15m
          expr: |
            sum_over_time(entity_id:allocated_cpu_cost:sum[15m:15s])
        - record: entity_id:allocated_gpu_cost:sum15m
          expr: |
            sum_over_time(entity_id:allocated_gpu_cost:sum[15m:15s])
        - record: entity_id:allocated_cost:sum15m
          expr: |
            sum_over_time(entity_id:allocated_cost:sum[15m:15s])
        - record: entity_id:overhead_cost:sum15m
          expr: |
            sum_over_time(entity_id:overhead_cost:sum[15m:15s])
        - record: entity_id:total_cost:sum15m
          expr: |
            sum_over_time(entity_id:total_cost:sum[15m:15s])
        - record: node:total_cost:sum15m
          expr: |
            sum_over_time(node:total_cost:sum[15m:15s])
        - record: node_type:total_cost:sum15m
          expr: |
            sum_over_time(node_type:total_cost:sum[15m:15s])
        - record: node_type:uptime_hours:sum15m
          expr: |
            sum_over_time(node_type:uptime_hours:sum[15m:15s])
---
# Source: dataplane/charts/prometheus/charts/kube-state-metrics/templates/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-kube-state-metrics
  namespace: kube-system
  labels:    
    helm.sh/chart: kube-state-metrics-5.33.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.15.0"
    release: release-name
spec:
  jobLabel: app.kubernetes.io/name  
  selector:
    matchLabels:      
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/instance: release-name
  endpoints:
    - port: http
      honorLabels: true
---
# Source: dataplane/charts/prometheus/templates/exporters/core-dns/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: union-operator-coredns
  namespace: union
  labels:
    app: prometheus-coredns
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
spec:
  jobLabel: jobLabel
  
  selector:
    matchLabels:
      app: prometheus-coredns
      release: "release-name"
  namespaceSelector:
    matchNames:
      - "kube-system"
  endpoints:
  - port: http-metrics
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
---
# Source: dataplane/charts/prometheus/templates/exporters/kube-controller-manager/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: union-operator-kube-controller-manager
  namespace: union
  labels:
    app: prometheus-kube-controller-manager
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
spec:
  jobLabel: jobLabel
  
  selector:
    matchLabels:
      app: prometheus-kube-controller-manager
      release: "release-name"
  namespaceSelector:
    matchNames:
      - "kube-system"
  endpoints:
  - port: http-metrics
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    scheme: https
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
---
# Source: dataplane/charts/prometheus/templates/exporters/kube-etcd/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: union-operator-kube-etcd
  namespace: union
  labels:
    app: prometheus-kube-etcd
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
spec:
  jobLabel: jobLabel
  
  selector:
    matchLabels:
      app: prometheus-kube-etcd
      release: "release-name"
  namespaceSelector:
    matchNames:
      - "kube-system"
  endpoints:
  - port: http-metrics
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
---
# Source: dataplane/charts/prometheus/templates/exporters/kube-proxy/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: union-operator-kube-proxy
  namespace: union
  labels:
    app: prometheus-kube-proxy
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
spec:
  jobLabel: jobLabel
  
  selector:
    matchLabels:
      app: prometheus-kube-proxy
      release: "release-name"
  namespaceSelector:
    matchNames:
      - "kube-system"
  endpoints:
  - port: http-metrics
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
---
# Source: dataplane/charts/prometheus/templates/exporters/kube-scheduler/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: union-operator-kube-scheduler
  namespace: union
  labels:
    app: prometheus-kube-scheduler
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
spec:
  jobLabel: jobLabel
  
  selector:
    matchLabels:
      app: prometheus-kube-scheduler
      release: "release-name"
  namespaceSelector:
    matchNames:
      - "kube-system"
  endpoints:
  - port: http-metrics
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    scheme: https
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
---
# Source: dataplane/charts/prometheus/templates/exporters/kubelet/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: union-operator-kubelet
  namespace: union
  labels:
    app: prometheus-kubelet    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
spec:
  
  attachMetadata:
    node: false
  jobLabel: k8s-app
  namespaceSelector:
    matchNames:
    - kube-system
  selector:
    matchLabels:
      app.kubernetes.io/name: kubelet
      k8s-app: kubelet
  endpoints:
  - port: https-metrics
    scheme: https    
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    honorLabels: true
    honorTimestamps: true
    metricRelabelings:
    - action: drop
      regex: (csi_operations|storage_operation_duration)_seconds_bucket;(0.25|2.5|15|25|120|600)(\.0)?
      sourceLabels:
      - __name__
      - le
    relabelings:
    - action: replace
      sourceLabels:
      - __metrics_path__
      targetLabel: metrics_path
  - port: https-metrics
    scheme: https
    path: /metrics/cadvisor
    interval: 10s
    honorLabels: true
    honorTimestamps: true
    trackTimestampsStaleness: true    
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    metricRelabelings:
    - action: drop
      regex: container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)
      sourceLabels:
      - __name__
    - action: drop
      regex: container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)
      sourceLabels:
      - __name__
    - action: drop
      regex: container_memory_(mapped_file|swap)
      sourceLabels:
      - __name__
    - action: drop
      regex: container_(file_descriptors|tasks_state|threads_max)
      sourceLabels:
      - __name__
    - action: drop
      regex: container_memory_failures_total;hierarchy
      sourceLabels:
      - __name__
      - scope
    - action: drop
      regex: container_network_.*;(cali|cilium|cni|lxc|nodelocaldns|tunl).*
      sourceLabels:
      - __name__
      - interface
    - action: drop
      regex: container_spec.*
      sourceLabels:
      - __name__
    - action: drop
      regex: .+;
      sourceLabels:
      - id
      - pod
    relabelings:
    - action: replace
      sourceLabels:
      - __metrics_path__
      targetLabel: metrics_path
  - port: https-metrics
    scheme: https
    path: /metrics/probes
    honorLabels: true
    honorTimestamps: true    
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabelings:
    - action: replace
      sourceLabels:
      - __metrics_path__
      targetLabel: metrics_path
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: prometheus-operator
  namespace: union
  labels:
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app: prometheus-operator
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator
spec:
  
  endpoints:
  - port: https
    scheme: https
    tlsConfig:
      serverName: prometheus-operator
      ca:
        secret:
          name: union-operator-admission
          key: ca
          optional: false
    honorLabels: true
  selector:
    matchLabels:
      app: prometheus-operator
      release: "release-name"
  namespaceSelector:
    matchNames:
      - "union"
---
# Source: dataplane/charts/prometheus/templates/prometheus/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: union-operator-prometheus
  namespace: union
  labels:
    app: prometheus-prometheus
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
spec:
  
  selector:
    matchLabels:
      app: prometheus-prometheus
      release: "release-name"
      self-monitor: "true"
  namespaceSelector:
    matchNames:
      - "union"
  endpoints:
  - port: http-web
    path: "/prometheus/metrics"
  - port: reloader-web
    path: "/metrics"
---
# Source: dataplane/templates/monitoring/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: cost
  namespace: union
  labels:
    release: release-name
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: opencost
  namespaceSelector:
    matchNames:
      - "union"
  endpoints:
    - port: http
      interval: 1m
      path: /metrics
      honorLabels: true
      metricRelabelings:
        - sourceLabels: [ "__name__" ]
          separator: ";"
          regex: "kube_node_labels|kube_pod_labels|node_total_hourly_cost|node_ram_hourly_cost|node_cpu_hourly_cost|node_gpu_hourly_cost"
          action: keep
---
# Source: dataplane/templates/monitoring/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: union-service-monitor
  namespace: union
  labels:
    release: release-name
spec:
  selector:
    matchLabels:
      platform.union.ai/service-group: release-name
  namespaceSelector:
    matchNames:
      - "union"
  endpoints:
    - port: debug
      interval: 1m
      path: /metrics
      honorLabels: true
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/admission-webhooks/job-patch/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name:  union-operator-admission
  namespace: union
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: prometheus-admission
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator-webhook
automountServiceAccountToken: true
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/admission-webhooks/job-patch/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name:  union-operator-admission
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: prometheus-admission
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator-webhook
rules:
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - validatingwebhookconfigurations
      - mutatingwebhookconfigurations
    verbs:
      - get
      - update
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/admission-webhooks/job-patch/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name:  union-operator-admission
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: prometheus-admission
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator-webhook
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: union-operator-admission
subjects:
  - kind: ServiceAccount
    name: union-operator-admission
    namespace: union
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/admission-webhooks/job-patch/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name:  union-operator-admission
  namespace: union
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: prometheus-admission
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator-webhook
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - create
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/admission-webhooks/job-patch/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name:  union-operator-admission
  namespace: union
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: prometheus-admission
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator-webhook
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: union-operator-admission
subjects:
  - kind: ServiceAccount
    name: union-operator-admission
    namespace: union
---
# Source: dataplane/charts/fluentbit/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-fluentbit-test-connection"
  namespace: union
  labels:
    helm.sh/chart: fluentbit-0.48.9
    app.kubernetes.io/version: "3.2.8"
    app.kubernetes.io/managed-by: Helm
  annotations:
    helm.sh/hook: test
    helm.sh/hook-delete-policy: hook-succeeded
spec:
  containers:
    - name: wget
      image: "busybox:latest"
      imagePullPolicy: Always
      command: ["sh"]
      args: ["-c", "sleep 5s && wget -O- release-name-fluentbit:2020"]
  restartPolicy: Never
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/admission-webhooks/job-patch/job-createSecret.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name:  union-operator-admission-create
  namespace: union
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: prometheus-admission-create
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator-webhook
spec:
  ttlSecondsAfterFinished: 60
  template:
    metadata:
      name:  union-operator-admission-create
      labels:
        app: prometheus-admission-create
        
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "72.9.1"
        app.kubernetes.io/part-of: prometheus
        chart: prometheus-72.9.1
        release: "release-name"
        heritage: "Helm"
        app.kubernetes.io/name: prometheus-prometheus-operator
        app.kubernetes.io/component: prometheus-operator-webhook
    spec:
      containers:
        - name: create
          image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3
          imagePullPolicy: IfNotPresent
          args:
            - create
            - --host=prometheus-operator,prometheus-operator.union.svc
            - --namespace=union
            - --secret-name=union-operator-admission
          securityContext:
          
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          resources:
            {}
      restartPolicy: OnFailure
      serviceAccountName: union-operator-admission
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
        seccompProfile:
          type: RuntimeDefault
---
# Source: dataplane/charts/prometheus/templates/prometheus-operator/admission-webhooks/job-patch/job-patchWebhook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name:  union-operator-admission-patch
  namespace: union
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: prometheus-admission-patch
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "72.9.1"
    app.kubernetes.io/part-of: prometheus
    chart: prometheus-72.9.1
    release: "release-name"
    heritage: "Helm"
    app.kubernetes.io/name: prometheus-prometheus-operator
    app.kubernetes.io/component: prometheus-operator-webhook
spec:
  ttlSecondsAfterFinished: 60
  template:
    metadata:
      name:  union-operator-admission-patch
      labels:
        app: prometheus-admission-patch
        
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "72.9.1"
        app.kubernetes.io/part-of: prometheus
        chart: prometheus-72.9.1
        release: "release-name"
        heritage: "Helm"
        app.kubernetes.io/name: prometheus-prometheus-operator
        app.kubernetes.io/component: prometheus-operator-webhook
    spec:
      containers:
        - name: patch
          image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3
          imagePullPolicy: IfNotPresent
          args:
            - patch
            - --webhook-name=union-operator-admission
            - --namespace=union
            - --secret-name=union-operator-admission
            - --patch-failure-policy=
          securityContext:
          
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          resources:
            {}
      restartPolicy: OnFailure
      serviceAccountName: union-operator-admission
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
        seccompProfile:
          type: RuntimeDefault
