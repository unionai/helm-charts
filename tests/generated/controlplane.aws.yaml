---
# Source: controlplane/templates/scylla/namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: scylla-operator
---
# Source: controlplane/charts/scylla-operator/templates/operator.pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: scylla-operator
  namespace: scylla-operator
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: scylla-operator
      app.kubernetes.io/instance: scylla-operator
---
# Source: controlplane/charts/scylla-operator/templates/webhookserver.pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: webhook-server
  namespace: scylla-operator
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: webhook-server
      app.kubernetes.io/instance: webhook-server
---
# Source: controlplane/templates/console/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: unionconsole
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: unionconsole
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  minAvailable: "33%"
  selector:
    matchLabels:
      app.kubernetes.io/name: unionconsole
      app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/flyte-core-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: flyteadmin
  namespace: union
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flyteadmin
---
# Source: controlplane/templates/flyte-core-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: datacatalog
  namespace: union
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: datacatalog
---
# Source: controlplane/templates/flyte-core-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: cacheservice
  namespace: union
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: cacheservice
---
# Source: controlplane/templates/pdb.yaml
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: authorizer
spec:
  minAvailable: "33%"
  selector:
    matchLabels:
      app.kubernetes.io/name: authorizer
      app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: cluster
spec:
  minAvailable: "33%"
  selector:
    matchLabels:
      app.kubernetes.io/name: cluster
      app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: dataproxy
spec:
  minAvailable: "33%"
  selector:
    matchLabels:
      app.kubernetes.io/name: dataproxy
      app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: executions
spec:
  minAvailable: "33%"
  selector:
    matchLabels:
      app.kubernetes.io/name: executions
      app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: queue
spec:
  minAvailable: "33%"
  selector:
    matchLabels:
      app.kubernetes.io/name: queue
      app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: run-scheduler
spec:
  minAvailable: "33%"
  selector:
    matchLabels:
      app.kubernetes.io/name: run-scheduler
      app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: usage
spec:
  minAvailable: "33%"
  selector:
    matchLabels:
      app.kubernetes.io/name: usage
      app.kubernetes.io/instance: release-name
---
# Source: controlplane/charts/flyte/templates/admin/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flyteadmin
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.1
    #app.kubernetes.io/managed-by: Helm
  annotations: 
    eks.amazonaws.com/role-arn: arn:aws:iam::<account-id>:role/adminflyterole
---
# Source: controlplane/charts/scylla-operator/templates/operator.serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: scylla-operator
  namespace: scylla-operator
  labels:
    app.kubernetes.io/name: scylla-operator
    app.kubernetes.io/instance: scylla-operator
---
# Source: controlplane/charts/scylla-operator/templates/webhookserver.serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: scylla-operator
  name: webhook-server
  labels:
    app.kubernetes.io/name: webhook-server
    app.kubernetes.io/instance: webhook-server
---
# Source: controlplane/templates/cacheservice/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cacheservice
  namespace: union
  labels: 
    app.kubernetes.io/name: cacheservice
    app.kubernetes.io/instance: release-name
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/managed-by: Helm
---
# Source: controlplane/templates/console/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: unionconsole
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: unionconsole
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
---
# Source: controlplane/templates/serviceaccount.yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: authorizer
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: authorizer
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
---
# Source: controlplane/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cluster
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: cluster
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
---
# Source: controlplane/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: dataproxy
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: dataproxy
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
---
# Source: controlplane/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: executions
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: executions
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
---
# Source: controlplane/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: queue
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: queue
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
---
# Source: controlplane/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: run-scheduler
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: run-scheduler
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
---
# Source: controlplane/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: usage
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: usage
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
---
# Source: controlplane/charts/flyte/templates/admin/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: flyte-admin-secrets
  namespace: union
type: Opaque
stringData:
---
# Source: controlplane/charts/flyte/templates/common/secret-auth.yaml
apiVersion: v1
kind: Secret
metadata:
  name: flyte-secret-auth
  namespace: union
type: Opaque
stringData:
  client_secret: foobar
---
# Source: controlplane/charts/flyte/templates/admin/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: flyte-admin-clusters-config
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.1
    #app.kubernetes.io/managed-by: Helm
data:
  clusters.yaml: |
    clusters:
      clusterConfigs: []
      labelClusterMap: {}
---
# Source: controlplane/charts/flyte/templates/admin/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: flyte-admin-base-config
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.1
    #app.kubernetes.io/managed-by: Helm
data:
  db.yaml: | 
    database:
      connMaxLifeTime: 120s
      dbname: flyteadmin
      host: ''
      maxIdleConnections: 10
      maxOpenConnections: 80
      passwordPath: /etc/db/pass.txt
      port: 5432
      username: ''
  domain.yaml: | 
    domains:
    - id: development
      name: development
    - id: staging
      name: staging
    - id: production
      name: production
  logger.yaml: | 
    level: null
  otel.yaml: | 
    otel:
      file:
        filename: /tmp/trace.txt
      jaeger:
        endpoint: http://localhost:14268/api/traces
      otlpgrpc:
        endpoint: http://localhost:4317
      otlphttp:
        endpoint: http://localhost:4318/v1/traces
      sampler:
        parentSampler: always
      type: noop
  server.yaml: | 
    admin:
      endpoint: dns:///
      insecure: false
    auth:
      appAuth:
        thirdPartyConfig:
          flyteClient:
            clientId: flytectl
            redirectUri: http://localhost:53593/callback
            scopes:
            - offline
            - all
      authorizedUris:
      - https://localhost:30081
      - http://flyteadmin:80
      - http://flyteadmin.flyte.svc.cluster.local:80
      userAuth:
        openId:
          baseUrl: https://accounts.google.com
          clientId: 657465813211-6eog7ek7li5k7i7fvgv2921075063hpe.apps.googleusercontent.com
          scopes:
          - profile
          - openid
    authorizer:
      internalCommunicationConfig:
        enabled: false
      type: Noop
    cloudEvents:
      enable: false
    connection:
      environment: staging
      region: ''
      rootTenantURLPattern: dns:///
    flyteadmin:
      eventVersion: 2
      metadataStoragePrefix:
      - metadata
      - admin
      metricsKeys:
      - phase
      metricsScope: 'flyte:'
      profilerPort: 10254
      roleNameKey: iam.amazonaws.com/role
      useOffloadedInputs: true
      useOffloadedWorkflowClosure: true
    otel:
      type: noop
    private:
      app:
        cacheProviderConfig:
          kind: bypass
        populateUserFields: false
    server:
      grpc:
        port: 8089
      httpPort: 8088
      security:
        allowCors: true
        allowedHeaders:
        - Content-Type
        - flyte-authorization
        allowedOrigins:
        - '*'
        secure: false
        useAuth: false
    sharedService:
      connectPort: 8089
      httpPort: 8088
      port: 8089
    union:
      internalConnectionConfig:
        enabled: true
        urlPattern: '_SERVICE_.union.svc.cluster.local:80'
  remoteData.yaml: | 
    remoteData:
      region: us-east-1
      scheme: local
      signedUrls:
        durationMinutes: 3
  storage.yaml: | 
    storage:
      type: s3
      container: ""
      connection:
        auth-type: iam
        region: 
      enable-multicontainer: false
      limits:
        maxDownloadMBs: 10
      cache:
        max_size_mbs: 1024
        target_gc_percent: 70
  task_resource_defaults.yaml: | 
    task_resources:
      defaults:
        cpu: 100m
        memory: 500Mi
      limits:
        cpu: 2
        gpu: 1
        memory: 1Gi
---
# Source: controlplane/charts/flyte/templates/console/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: flyte-console-config
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteconsole
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.1
    app.kubernetes.io/managed-by: Helm
data: 
  BASE_URL: /console
  CONFIG_DIR: /etc/flyte/config
---
# Source: controlplane/templates/cacheservice/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cacheservice-config
  namespace: union
  labels: 
    app.kubernetes.io/name: cacheservice
    app.kubernetes.io/instance: release-name
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/managed-by: Helm
data:
  db.yaml: | 
    database:
      connMaxLifeTime: 120s
      dbname: cacheservice
      host: ''
      maxIdleConnections: 10
      maxOpenConnections: 20
      passwordPath: /etc/db/pass.txt
      port: 5432
      username: ''
  logger.yaml: | 
    level: null
  server.yaml: | 
    authorizer:
      internalCommunicationConfig:
        enabled: false
      type: Noop
    cache-server:
      grpcPort: 8089
      grpcServerReflection: true
      httpPort: 8080
    cacheservice:
      heartbeat-grace-period-multiplier: 3
      max-reservation-heartbeat: 30s
      metrics-scope: flyte
      profiler-port: 10254
      storage-prefix: cached_outputs
    otel:
      type: noop
    private:
      app:
        cacheProviderConfig:
          kind: bypass
    union:
      internalConnectionConfig:
        enabled: true
        urlPattern: '_SERVICE_.union.svc.cluster.local:80'
  storage.yaml: | 
    storage:
      type: s3
      container: ""
      connection:
        auth-type: iam
        region: 
      enable-multicontainer: false
      limits:
        maxDownloadMBs: 10
      cache:
        max_size_mbs: 1024
        target_gc_percent: 70
---
# Source: controlplane/templates/configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: authorizer
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: authorizer
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
data:
  config.yaml: |
    authorizer:
      internalCommunicationConfig:
        enabled: false
      type: Noop
    cache:
      identity:
        enabled: false
    connection:
      environment: staging
      region: us-east-2
      rootTenantURLPattern: dns:///fake-host.domain
    logger:
      level: 6
    otel:
      type: noop
    sharedService:
      metrics:
        scope: 'authorizer:'
    union:
      auth:
        enable: false
      internalConnectionConfig:
        enabled: true
        urlPattern: _SERVICE_.union.svc.cluster.local:80
---
# Source: controlplane/templates/configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: cluster
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
data:
  config.yaml: |
    authorizer:
      internalCommunicationConfig:
        enabled: false
      type: Noop
    cache:
      identity:
        enabled: false
    cloudProvider:
      provider: Mock
    cluster:
      cloudflare:
        active: false
    connection:
      environment: staging
      region: us-east-2
      rootTenantURLPattern: dns:///fake-host.domain
    db:
      connectionPool:
        maxConnectionLifetime: 1m
        maxIdleConnections: 20
        maxOpenConnections: 20
      dbname: ''
      host: ''
      passwordPath: /etc/db/pass.txt
      port: 5432
      username: ''
    logger:
      level: 6
    otel:
      type: noop
    sharedService:
      metrics:
        scope: 'cluster:'
    union:
      auth:
        enable: false
      internalConnectionConfig:
        enabled: true
        urlPattern: _SERVICE_.union.svc.cluster.local:80
---
# Source: controlplane/templates/configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: dataproxy
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: dataproxy
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
data:
  config.yaml: |
    authorizer:
      internalCommunicationConfig:
        enabled: false
      type: Noop
    cache:
      identity:
        enabled: false
    connection:
      environment: staging
      region: us-east-2
      rootTenantURLPattern: dns:///fake-host.domain
    dataproxy:
      clusterSelector:
        type: local
      secureTunnelTenantURLPattern: http://ingress-nginx-internal.ingress-nginx.svc.cluster.local:80
    logger:
      level: 6
    otel:
      type: noop
    sharedService:
      metrics:
        scope: 'dataproxy:'
    union:
      auth:
        enable: false
      internalConnectionConfig:
        enabled: true
        urlPattern: _SERVICE_.union.svc.cluster.local:80
---
# Source: controlplane/templates/configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: executions
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: executions
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
data:
  config.yaml: |
    authorizer:
      internalCommunicationConfig:
        enabled: false
      type: Noop
    cache:
      identity:
        enabled: false
    cloudEventsProcessor:
      cloudProvider: Local
    connection:
      environment: staging
      region: us-east-2
      rootTenantURLPattern: dns:///fake-host.domain
    db:
      connectionPool:
        maxConnectionLifetime: 1m
        maxIdleConnections: 20
        maxOpenConnections: 20
      dbname: ''
      host: ''
      passwordPath: /etc/db/pass.txt
      port: 5432
      username: ''
    executions:
      apps:
        enrichIdentities: false
        publicURLPattern: https://%s.apps.
      task:
        enabled: true
        enrichIdentities: false
    logger:
      level: 6
    otel:
      type: noop
    sharedService:
      metrics:
        scope: 'executions:'
    union:
      auth:
        enable: false
      internalConnectionConfig:
        enabled: true
        urlPattern: _SERVICE_.union.svc.cluster.local:80
    workspace:
      enable: false
---
# Source: controlplane/templates/configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: queue
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: queue
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
data:
  config.yaml: |
    authorizer:
      internalCommunicationConfig:
        enabled: false
      type: Noop
    cache:
      identity:
        enabled: false
    connection:
      environment: staging
      region: us-east-2
      rootTenantURLPattern: dns:///fake-host.domain
    logger:
      level: 6
    otel:
      type: noop
    queue:
      db:
        hosts:
        - 'scylla-client.union.svc.cluster.local'
        threadCount: 64
        type: cql
      eventer:
        recordActionThreadCount: 16
        type: runservice
        updateActionStatusThreadCount: 16
    sharedService:
      metrics:
        scope: 'queue:'
    union:
      auth:
        enable: false
      internalConnectionConfig:
        enabled: true
        urlPattern: _SERVICE_.union.svc.cluster.local:80
---
# Source: controlplane/templates/configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: run-scheduler
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: run-scheduler
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
data:
  config.yaml: |
    authorizer:
      internalCommunicationConfig:
        enabled: false
      type: Noop
    cache:
      identity:
        enabled: false
    connection:
      environment: staging
      region: us-east-2
      rootTenantURLPattern: dns:///fake-host.domain
    db:
      connectionPool:
        maxConnectionLifetime: 1m
        maxIdleConnections: 20
        maxOpenConnections: 20
      dbname: ''
      host: ''
      passwordPath: /etc/db/pass.txt
      port: 5432
      username: ''
    logger:
      level: 6
    otel:
      type: noop
    sharedService:
      metrics:
        scope: 'run-scheduler:'
    union:
      auth:
        enable: false
      internalConnectionConfig:
        enabled: true
        urlPattern: _SERVICE_.union.svc.cluster.local:80
---
# Source: controlplane/templates/configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: usage
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: usage
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
data:
  config.yaml: |
    authorizer:
      internalCommunicationConfig:
        enabled: false
      type: Noop
    billing:
      enable: false
    cache:
      identity:
        enabled: false
    cloudProvider:
      provider: Mock
    connection:
      environment: staging
      region: us-east-2
      rootTenantURLPattern: dns:///fake-host.domain
    logger:
      level: 6
    otel:
      type: noop
    sharedService:
      metrics:
        scope: 'usage:'
    union:
      auth:
        enable: false
      internalConnectionConfig:
        enabled: true
        urlPattern: _SERVICE_.union.svc.cluster.local:80
    usage:
      taskMetrics:
        agentQuery:
          mappings:
            dgx_job:
              queries:
                EXECUTION_METRIC_ALLOCATED_CPU_AVG: CPU_ALLOCATION:MEAN
                EXECUTION_METRIC_ALLOCATED_MEMORY_BYTES_AVG: MEM_ALLOCATION:MEAN
                EXECUTION_METRIC_CPU_UTILIZATION: CPU_UTILIZATION:MEAN
                EXECUTION_METRIC_GPU_UTILIZATION: GPU_UTILIZATION:MEAN
                EXECUTION_METRIC_MEMORY_UTILIZATION: MEM_UTILIZATION:MEAN
        metricDelayToleranceDuration: 0s
        promQuery:
          queries:
            EXECUTION_METRIC_ALLOCATED_CPU_AVG: |
              max by (namespace, pod) (
                (
                  sum by (namespace, pod) (irate(container_cpu_usage_seconds_total{namespace="{{.Namespace}}",pod=~"{{.PodName}}",image!=""}[5m])) >
                  sum by (namespace, pod) (kube_pod_container_resource_requests{namespace="{{.Namespace}}",pod=~"{{.PodName}}",resource="cpu"})
                )
                or
                sum by (namespace, pod) (kube_pod_container_resource_requests{namespace="{{.Namespace}}",pod=~"{{.PodName}}",resource="cpu"})
              ) *
              on (namespace, pod) group_left max by (namespace, pod) (kube_pod_status_phase{namespace="{{.Namespace}}",pod=~"{{.PodName}}",phase=~"Pending|Running"} == 1)
            EXECUTION_METRIC_ALLOCATED_MEMORY_BYTES_AVG: |
              max by (namespace, pod) (
                (
                  sum by (namespace, pod) (container_memory_working_set_bytes{namespace="{{.Namespace}}",pod=~"{{.PodName}}",image!=""}) >
                  sum by (namespace, pod) (kube_pod_container_resource_requests{namespace="{{.Namespace}}",pod=~"{{.PodName}}",resource="memory"})
                )
                or
                sum by (namespace, pod) (kube_pod_container_resource_requests{namespace="{{.Namespace}}",pod=~"{{.PodName}}",resource="memory"})
              ) *
              on (namespace, pod) group_left max by (namespace, pod) (kube_pod_status_phase{namespace="{{.Namespace}}",pod=~"{{.PodName}}",phase=~"Pending|Running"} == 1)
            EXECUTION_METRIC_APP_REPLICA_COUNT: |
              sum (kube_pod_status_phase{phase=~"Running|Pending", namespace="{{.Namespace}}", pod=~"{{.AppName}}.*"} == 1) or vector(0)
            EXECUTION_METRIC_APP_REQUESTS: |
              sum(rate((
                envoy_cluster_upstream_rq_xx{
                  job="serving-envoy",
                  project=~"{{.Project}}",
                  domain=~"{{.Domain}}",
                  name=~"{{.AppName}}",
                  name!=""}
              )[5m:])) by (project, domain, name, envoy_response_code_class)
            EXECUTION_METRIC_APP_RESPONSE_TIME_P50: |
              histogram_quantile(0.5, sum(rate((
                envoy_cluster_upstream_rq_time_bucket{
                  job="serving-envoy",
                  project=~"${{.Project}}",
                  domain=~"{{.Domain}}",
                  name=~"{{.AppName}}",
                  name!=""}
              )[5m:])) by (project, domain, name, le))
            EXECUTION_METRIC_APP_RESPONSE_TIME_P90: |
              histogram_quantile(0.90, sum(rate((
                envoy_cluster_upstream_rq_time_bucket{
                  job="serving-envoy",
                  project=~"${{.Project}}",
                  domain=~"{{.Domain}}",
                  name=~"{{.AppName}}",
                  name!=""}
              )[5m:])) by (project, domain, name, le))
            EXECUTION_METRIC_APP_RESPONSE_TIME_P95: |
              histogram_quantile(0.95, sum(rate((
                envoy_cluster_upstream_rq_time_bucket{
                  job="serving-envoy",
                  project=~"${{.Project}}",
                  domain=~"{{.Domain}}",
                  name=~"{{.AppName}}",
                  name!=""}
              )[5m:])) by (project, domain, name, le))
            EXECUTION_METRIC_CPU_UTILIZATION: |
              (sum by (namespace, pod) (irate(container_cpu_usage_seconds_total{namespace="{{.Namespace}}",pod=~"{{.PodName}}",image!=""}[5m])) /
                sum by (namespace, pod) (kube_pod_container_resource_requests{namespace="{{.Namespace}}",pod=~"{{.PodName}}",resource="cpu"})) *
              on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{.Namespace}}",pod=~"{{.PodName}}",phase=~"Pending|Running"} == 1)
            EXECUTION_METRIC_GPU_FRAME_BUFFER_UTILIZATION: |
              (sum by (namespace, pod, gpu) (DCGM_FI_DEV_FB_USED{namespace="{{.Namespace}}",pod=~"{{.PodName}}"}) /
                sum by (namespace, pod, gpu) (DCGM_FI_DEV_FB_USED{namespace="{{.Namespace}}",pod=~"{{.PodName}}"} + DCGM_FI_DEV_FB_FREE{namespace="{{.Namespace}}",pod=~"{{.PodName}}"})) *
              on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{.Namespace}}",pod=~"{{.PodName}}",phase=~"Pending|Running"} == 1)
            EXECUTION_METRIC_GPU_MEMORY_UTILIZATION: |
              sum by (gpu) (DCGM_FI_DEV_MEM_COPY_UTIL{namespace="{{.Namespace}}",pod=~"{{.PodName}}"} *
              on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{.Namespace}}",pod=~"{{.PodName}}",phase=~"Pending|Running"} == 1)) / 100.0
            EXECUTION_METRIC_GPU_SM_ACTIVE: |
              sum by (gpu) (DCGM_FI_PROF_SM_ACTIVE{namespace="{{.Namespace}}",pod=~"{{.PodName}}"} *
              on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{.Namespace}}",pod=~"{{.PodName}}",phase=~"Pending|Running"} == 1))
            EXECUTION_METRIC_GPU_SM_OCCUPANCY: |
              sum by (gpu) (DCGM_FI_PROF_SM_OCCUPANCY{namespace="{{.Namespace}}",pod=~"{{.PodName}}"} *
              on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{.Namespace}}",pod=~"{{.PodName}}",phase=~"Pending|Running"} == 1))
            EXECUTION_METRIC_GPU_UTILIZATION: |
              sum by (gpu) (DCGM_FI_DEV_GPU_UTIL{namespace="{{.Namespace}}",pod=~"{{.PodName}}"} *
              on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{.Namespace}}",pod=~"{{.PodName}}",phase=~"Pending|Running"} == 1)) / 100.0
            EXECUTION_METRIC_LIMIT_CPU: |
              sum by (namespace, pod) (kube_pod_container_resource_limits{namespace="{{.Namespace}}",pod=~"{{.PodName}}",resource="cpu"} *
              on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{.Namespace}}",pod=~"{{.PodName}}",phase=~"Pending|Running"} == 1))
            EXECUTION_METRIC_LIMIT_MEMORY_BYTES: |
              sum by (namespace, pod) (kube_pod_container_resource_limits{namespace="{{.Namespace}}",pod=~"{{.PodName}}",resource="memory"} *
              on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{.Namespace}}",pod=~"{{.PodName}}",phase=~"Pending|Running"} == 1))
            EXECUTION_METRIC_MEMORY_UTILIZATION: |
              (sum by (namespace, pod) (container_memory_working_set_bytes{namespace="{{.Namespace}}",pod=~"{{.PodName}}",image!=""}) /
                sum by (namespace, pod) (kube_pod_container_resource_requests{namespace="{{.Namespace}}",pod=~"{{.PodName}}",resource="memory"})) *
              on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{.Namespace}}",pod=~"{{.PodName}}",phase=~"Pending|Running"} == 1)
            EXECUTION_METRIC_REQUEST_CPU: |
              sum by (namespace, pod) (kube_pod_container_resource_requests{namespace="{{.Namespace}}",pod=~"{{.PodName}}",resource="cpu"} *
              on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{.Namespace}}",pod=~"{{.PodName}}",phase=~"Pending|Running"} == 1))
            EXECUTION_METRIC_REQUEST_MEMORY_BYTES: |
              sum by (namespace, pod) (kube_pod_container_resource_requests{namespace="{{.Namespace}}",pod=~"{{.PodName}}",resource="memory"} *
              on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{.Namespace}}",pod=~"{{.PodName}}",phase=~"Pending|Running"} == 1))
            EXECUTION_METRIC_USED_CPU_AVG: |
              sum by (namespace, pod) (irate(container_cpu_usage_seconds_total{namespace="{{.Namespace}}",pod=~"{{.PodName}}",image!=""}[5m]) *
              on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{.Namespace}}",pod=~"{{.PodName}}",phase=~"Pending|Running"} == 1))
            EXECUTION_METRIC_USED_MEMORY_BYTES_AVG: |
              sum by (namespace, pod) (container_memory_working_set_bytes{namespace="{{.Namespace}}",pod=~"{{.PodName}}",image!=""} *
              on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{namespace="{{.Namespace}}",pod=~"{{.PodName}}",phase=~"Pending|Running"} == 1))
      workers: 10
---
# Source: controlplane/templates/scylla/storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: scylladb
provisioner: ebs.csi.eks.amazonaws.com
volumeBindingMode: WaitForFirstConsumer
parameters:
  fsType: ext4
  type: gp2
reclaimPolicy: Delete
allowVolumeExpansion: true
---
# Source: controlplane/charts/flyte/templates/admin/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: union-flyteadmin
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.1
    #app.kubernetes.io/managed-by: Helm
rules:
- apiGroups: 
    - ""
    - flyte.lyft.com
    - rbac.authorization.k8s.io
  resources: 
    - configmaps
    - flyteworkflows
    - namespaces
    - pods
    - resourcequotas
    - roles
    - rolebindings
    - secrets
    - services
    - serviceaccounts
    - spark-role
    - limitranges
  verbs: 
    - '*'
---
# Source: controlplane/charts/scylla-operator/templates/edit_clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: scyllacluster-edit
  labels:
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
rules:
- apiGroups:
  - scylla.scylladb.com
  resources:
  - scyllaclusters
  - scylladbmonitorings
  - scylladbdatacenters
  - scylladbclusters
  - scylladbmanagerclusterregistrations
  - scylladbmanagertasks
  verbs:
  - create
  - patch
  - update
  - delete
  - deletecollection
---
# Source: controlplane/charts/scylla-operator/templates/operator.clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: scylladb:controller:operator
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.operator.scylladb.com/aggregate-to-scylla-operator: "true"
---
# Source: controlplane/charts/scylla-operator/templates/operator.clusterrole_def.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: scylladb:controller:aggregate-to-operator
  labels:
    rbac.operator.scylladb.com/aggregate-to-scylla-operator: "true"
rules:
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - nodes
  - endpoints
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - persistentvolumeclaims
  verbs:
  - delete
  - get
  - list
  - patch
  - update
  - watch
  - patch
- apiGroups:
  - ""
  resources:
  - persistentvolumes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - delete
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - pods/eviction
  verbs:
  - create
- apiGroups:
  - ""
  resources:
  - configmaps
  - endpoints
  - namespaces
  - secrets
  - serviceaccounts
  - services
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - apps
  resources:
  - statefulsets
  - daemonsets
  - deployments
  verbs:
  - create
  - get
  - list
  - watch
  - update
  - patch
  - delete
- apiGroups:
  - apps
  resources:
  - statefulsets/scale
  verbs:
  - update
- apiGroups:
  - scylla.scylladb.com
  resources:
  - scyllaclusters
  - scylladbmonitorings
  - scylladbdatacenters
  - remotekubernetesclusters
  - scylladbclusters
  - scylladbmanagerclusterregistrations
  - scylladbmanagertasks
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - scylla.scylladb.com
  resources:
  - scyllaclusters/status
  - scylladbmonitorings/status
  - scylladbdatacenters/status
  - remotekubernetesclusters/status
  - scylladbclusters/status
  - scylladbmanagerclusterregistrations/status
  - scylladbmanagertasks/status
  verbs:
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - scylla.scylladb.com
  resources:
  - nodeconfigs
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - clusterroles
  - clusterrolebindings
  - roles
  - rolebindings
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - scylla.scylladb.com
  resources:
  - nodeconfigs/status
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - batch
  resources:
  - jobs
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - scylla.scylladb.com
  resources:
  - scyllaoperatorconfigs
  - scyllaoperatorconfigs/status
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - monitoring.coreos.com
  resources:
  - prometheuses
  - prometheusrules
  - servicemonitors
  verbs:
  - get
  - list
  - watch
  - create
  - patch
  - update
  - delete
- apiGroups:
  - ""
  resources:
  - configmaps/finalizers
  - secrets/finalizers
  - pods/finalizers
  verbs:
  - update
- apiGroups:
  - apps
  resources:
  - daemonsets/finalizers
  verbs:
  - update
- apiGroups:
  - scylla.scylladb.com
  resources:
  - scyllaclusters/finalizers
  - scylladbdatacenters/finalizers
  - scylladbmonitorings/finalizers
  - scylladbmanagerclusterregistrations/finalizers
  - scylladbmanagertasks/finalizers
  verbs:
  - update
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets/finalizers
  verbs:
  - update
- apiGroups:
  - scylla.scylladb.com
  resources:
  - nodeconfigs/finalizers
  verbs:
  - update
- apiGroups:
  - ""
  resources:
  - configmaps/finalizers
  - secrets/finalizers
  - pods/finalizers
  verbs:
  - update
- apiGroups:
  - apps
  resources:
  - daemonsets/finalizers
  verbs:
  - update
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets/finalizers
  verbs:
  - update
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
---
# Source: controlplane/charts/scylla-operator/templates/operator.clusterrole_def_openshift.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: scylladb:controller:aggregate-to-operator-openshift
  labels:
    rbac.operator.scylladb.com/aggregate-to-scylla-operator: "true"
rules:
- apiGroups:
  - security.openshift.io
  resourceNames:
  - privileged
  resources:
  - securitycontextconstraints
  verbs:
  - use
---
# Source: controlplane/charts/scylla-operator/templates/operator_remote.clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: scylladb:controller:operator-remote
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.operator.scylladb.com/aggregate-to-scylla-operator-remote: "true"
---
# Source: controlplane/charts/scylla-operator/templates/operator_remote.clusterrole_def.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: scylladb:controller:aggregate-to-operator-remote
  labels:
    rbac.operator.scylladb.com/aggregate-to-scylla-operator-remote: "true"
rules:
- apiGroups:
  - scylla.scylladb.com
  resources:
  - scylladbdatacenters
  - remoteowners
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - scylla.scylladb.com
  resources:
  - scylladbdatacenters/status
  - remoteowners/status
  verbs:
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - ""
  resources:
  - endpoints
  - namespaces
  - services
  - secrets
  - configmaps
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
---
# Source: controlplane/charts/scylla-operator/templates/scyllacluster_member_clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: scyllacluster-member
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.operator.scylladb.com/aggregate-to-scylla-member: "true"
---
# Source: controlplane/charts/scylla-operator/templates/scyllacluster_member_clusterrole_def.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: scylladb:aggregate-to-scyllacluster-member
  labels:
    rbac.operator.scylladb.com/aggregate-to-scylla-member: "true"
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - "apps"
  resources:
  - statefulsets
  verbs:
  - get
  - list
  - patch
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps/finalizers
  - secrets/finalizers
  verbs:
  - update
---
# Source: controlplane/charts/scylla-operator/templates/scyllacluster_member_clusterrole_def_openshift.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: scylladb:aggregate-to-scyllacluster-member-openshift
  labels:
    rbac.operator.scylladb.com/aggregate-to-scylla-member: "true"
rules:
- apiGroups:
  - security.openshift.io
  resourceNames:
  - privileged
  resources:
  - securitycontextconstraints
  verbs:
  - use
---
# Source: controlplane/charts/scylla-operator/templates/scylladbmonitoring_grafana_clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: scylladb:monitoring:grafana
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      rbac.operator.scylladb.com/aggregate-to-scylladb-monitoring-grafana: "true"
---
# Source: controlplane/charts/scylla-operator/templates/scylladbmonitoring_grafana_clusterrole_def_openshift.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: scylladb:aggregate-to-scylladb-monitoring-grafana-openshift
  labels:
    rbac.operator.scylladb.com/aggregate-to-scylladb-monitoring-grafana: "true"
rules:
- apiGroups:
  - security.openshift.io
  resourceNames:
  - privileged
  resources:
  - securitycontextconstraints
  verbs:
  - use
---
# Source: controlplane/charts/scylla-operator/templates/scylladbmonitoring_prometheus_clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: scylladb:monitoring:prometheus
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      rbac.operator.scylladb.com/aggregate-to-scylladb-monitoring-prometheus: "true"
---
# Source: controlplane/charts/scylla-operator/templates/scylladbmonitoring_prometheus_clusterrole_def.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: scylladb:aggregate-to-scylladb-monitoring-prometheus
  labels:
    rbac.operator.scylladb.com/aggregate-to-scylladb-monitoring-prometheus: "true"
rules:
- apiGroups:
  - ""
  resources:
  - services
  - endpoints
  - pods
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
---
# Source: controlplane/charts/scylla-operator/templates/scylladbmonitoring_prometheus_clusterrole_def_openshift.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: scylladb:aggregate-to-scylladb-monitoring-prometheus-openshift
  labels:
    rbac.operator.scylladb.com/aggregate-to-scylladb-monitoring-prometheus: "true"
rules:
- apiGroups:
  - security.openshift.io
  resourceNames:
  - privileged
  resources:
  - securitycontextconstraints
  verbs:
  - use
---
# Source: controlplane/charts/scylla-operator/templates/view_clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: scyllacluster-view
  labels:
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    rbac.authorization.k8s.io/aggregate-to-view: "true"
rules:
- apiGroups:
  - scylla.scylladb.com
  resources:
  - scyllaclusters
  - scylladbmonitorings
  - scylladbdatacenters
  - scylladbclusters
  - scylladbmanagerclusterregistrations
  - scylladbmanagertasks
  verbs:
  - get
  - list
  - watch
---
# Source: controlplane/charts/flyte/templates/admin/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: union-flyteadmin-binding
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.1
    #app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: union-flyteadmin
subjects:
- kind: ServiceAccount
  name: flyteadmin
  namespace: union
---
# Source: controlplane/charts/scylla-operator/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: scylladb:controller:operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: scylladb:controller:operator
subjects:
- kind: ServiceAccount
  name: scylla-operator
  namespace: scylla-operator
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: flyteadmin
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: controlplane-2026.1.4
    #app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - ""
      - flyte.lyft.com
      - rbac.authorization.k8s.io
    resources:
      - configmaps
      - flyteworkflows
      - namespaces
      - pods
      - resourcequotas
      - roles
      - rolebindings
      - secrets
      - services
      - serviceaccounts
      - spark-role
    verbs:
      - '*'
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: flyteadmin-binding
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: controlplane-2026.1.4
    #app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: flyteadmin
subjects:
  - kind: ServiceAccount
    name: flyteadmin
    namespace: union
---
# Source: controlplane/charts/flyte/templates/admin/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: flyteadmin
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.1
    #app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8088
    - name: grpc
      port: 81
      protocol: TCP
      # intentionally set to TCP instead of grpc
      targetPort: 8089
    - name: redoc
      protocol: TCP
      port: 87
      targetPort: 8087
    - name: http-metrics
      protocol: TCP
      port: 10254
  selector: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/charts/flyte/templates/console/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: flyteconsole
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteconsole
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.1
    app.kubernetes.io/managed-by: Helm
  annotations: 
    external-dns.alpha.kubernetes.io/hostname: flyte.example.com
    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "600"
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
  selector: 
    app.kubernetes.io/name: flyteconsole
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/charts/scylla-operator/templates/webhookserver.service.yaml
apiVersion: v1
kind: Service
metadata:
  namespace: scylla-operator
  name: scylla-operator-webhook
  labels:
    app.kubernetes.io/name: webhook-server
    app.kubernetes.io/instance: webhook-server
spec:
  ports:
  - port: 443
    targetPort: 5000
    name: webhook
  selector:
    app.kubernetes.io/name: webhook-server
    app.kubernetes.io/instance: webhook-server
---
# Source: controlplane/templates/cacheservice/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: cacheservice
  namespace: union
  labels: 
    app.kubernetes.io/name: cacheservice
    app.kubernetes.io/instance: release-name
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 88
    protocol: TCP
    targetPort: 8088
  - name: grpc
    port: 89
    protocol: TCP
    targetPort: 8089
  - name: http-metrics
    protocol: TCP
    port: 10254
  selector: 
    app.kubernetes.io/name: cacheservice
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/console/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: unionconsole
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: unionconsole
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: http
    protocol: TCP
    name: http
  - port: 8081
    targetPort: http-metrics
    protocol: TCP
    name: http-metrics
  selector:
    app.kubernetes.io/name: unionconsole
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/service.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: authorizer
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: authorizer
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: grpc
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: connect
      port:  83
      protocol: TCP
      targetPort: 8081
    - name: http
      port: 81
      protocol: TCP
      targetPort: 8089
    - name: debug
      port: 82
      protocol: TCP
      targetPort: 10254
  selector:
    app.kubernetes.io/name: authorizer
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: cluster
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: cluster
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: grpc
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: connect
      port:  83
      protocol: TCP
      targetPort: 8081
    - name: http
      port: 81
      protocol: TCP
      targetPort: 8089
    - name: debug
      port: 82
      protocol: TCP
      targetPort: 10254
  selector:
    app.kubernetes.io/name: cluster
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: dataproxy
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: dataproxy
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: grpc
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: connect
      port:  83
      protocol: TCP
      targetPort: 8081
    - name: http
      port: 81
      protocol: TCP
      targetPort: 8089
    - name: debug
      port: 82
      protocol: TCP
      targetPort: 10254
  selector:
    app.kubernetes.io/name: dataproxy
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: executions
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: executions
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: grpc
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: connect
      port:  83
      protocol: TCP
      targetPort: 8081
    - name: http
      port: 81
      protocol: TCP
      targetPort: 8089
    - name: debug
      port: 82
      protocol: TCP
      targetPort: 10254
  selector:
    app.kubernetes.io/name: executions
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: queue
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: queue
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: grpc
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: connect
      port:  83
      protocol: TCP
      targetPort: 8081
    - name: http
      port: 81
      protocol: TCP
      targetPort: 8089
    - name: debug
      port: 82
      protocol: TCP
      targetPort: 10254
  selector:
    app.kubernetes.io/name: queue
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: run-scheduler
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: run-scheduler
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: grpc
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: connect
      port:  83
      protocol: TCP
      targetPort: 8081
    - name: http
      port: 81
      protocol: TCP
      targetPort: 8089
    - name: debug
      port: 82
      protocol: TCP
      targetPort: 10254
  selector:
    app.kubernetes.io/name: run-scheduler
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: usage
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: usage
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: grpc
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: connect
      port:  83
      protocol: TCP
      targetPort: 8081
    - name: http
      port: 81
      protocol: TCP
      targetPort: 8089
    - name: debug
      port: 82
      protocol: TCP
      targetPort: 10254
  selector:
    app.kubernetes.io/name: usage
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/charts/flyte/templates/admin/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flyteadmin
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.1
    #app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels: 
      app.kubernetes.io/name: flyteadmin
      app.kubernetes.io/instance: release-name
  strategy: 
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        configChecksum: "ee8b608bab1c43643fbc05f59e3669a30a61d71aec606521d5e65cafd2c33a9"
        kubectl.kubernetes.io/default-container: flyteadmin
      labels: 
        app.kubernetes.io/name: flyteadmin
        app.kubernetes.io/instance: release-name
        helm.sh/chart: flyte-v1.16.1
        #app.kubernetes.io/managed-by: Helm
    spec:
      securityContext: 
        fsGroup: 65534
        fsGroupChangePolicy: Always
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions:
          type: spc_t
      initContainers:
        - command:
          - flyteadmin
          - --config
          - /etc/flyte/config/*.yaml
          - migrate
          - run
          image: "643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services:"
          imagePullPolicy: "IfNotPresent"
          name: run-migrations
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
          volumeMounts:
          - mountPath: /etc/db
            name: <KUBERNETES_SECRET_NAME>
          - mountPath: /etc/flyte/config
            name: base-config-volume
        - command:
          - flyteadmin
          - --config
          - /etc/flyte/config/*.yaml
          - migrate
          - seed-projects
          - union-health-monitoring
          - flytesnacks
          image: "643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services:"
          imagePullPolicy: "IfNotPresent"
          name: seed-projects
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
          volumeMounts:
          - mountPath: /etc/db
            name: <KUBERNETES_SECRET_NAME>
          - mountPath: /etc/flyte/config
            name: base-config-volume
        - name: generate-secrets
          image: "643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services:"
          imagePullPolicy: "IfNotPresent"
          command: ["/bin/sh", "-c"]
          args:
            [
                "flyteadmin --config=/etc/flyte/config/*.yaml secrets init --localPath /etc/scratch/secrets && flyteadmin --config=/etc/flyte/config/*.yaml secrets create --name flyte-admin-secrets --fromPath /etc/scratch/secrets",
            ]
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
          volumeMounts:
            - mountPath: /etc/flyte/config
              name: base-config-volume
            - mountPath: /etc/scratch
              name: scratch
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
      containers:
      - command:
        - flyteadmin
        - --config
        - /etc/flyte/config/*.yaml
        - serve
        image: "643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services:"
        imagePullPolicy: "IfNotPresent"
        name: flyteadmin
        ports:
        - containerPort: 8088
        - containerPort: 8089
        - containerPort: 10254
        readinessProbe:
          httpGet:
            path: /healthcheck
            port: 8088
          initialDelaySeconds: 15
          timeoutSeconds: 1
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 3
        livenessProbe:
          httpGet:
            path: /healthcheck
            port: 8088
          initialDelaySeconds: 20
          timeoutSeconds: 1
          periodSeconds: 5
          successThreshold: 1
          failureThreshold: 3
        resources:
          limits:
            cpu: 2
            ephemeral-storage: 500Mi
            memory: 3Gi
          requests:
            cpu: 50m
            ephemeral-storage: 200Mi
            memory: 500Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
        volumeMounts:
        - mountPath: /etc/db
          name: <KUBERNETES_SECRET_NAME>
        - mountPath: /srv/flyte
          name: shared-data
        - mountPath: /etc/flyte/config
          name: clusters-config-volume
        - mountPath: /etc/secrets/
          name: admin-secrets
      serviceAccountName: flyteadmin
      volumes:
      - name: <KUBERNETES_SECRET_NAME>
        secret:
          secretName: <KUBERNETES_SECRET_NAME>
      - emptyDir: {}
        name: shared-data
      - emptyDir: {}
        name: scratch
      - projected:
          sources:
            - configMap:
                name: flyte-admin-base-config
        name: base-config-volume
      - projected:
          sources:
            - configMap:
                name: flyte-admin-base-config
            - configMap:
                name: flyte-admin-clusters-config
        name: clusters-config-volume
      - name: admin-secrets
        secret:
          secretName: flyte-admin-secrets
---
# Source: controlplane/charts/flyte/templates/console/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flyteconsole
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteconsole
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.1
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels: 
      app.kubernetes.io/name: flyteconsole
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        configChecksum: "2620ed20cf30d64460b231bbcf13fc096a23b6d373b46e69ab5f2e051f3d3d1"
        linkerd.io/inject: disabled
        prometheus.io/scrape: "false"
      labels: 
        app.kubernetes.io/name: flyteconsole
        app.kubernetes.io/instance: release-name
        helm.sh/chart: flyte-v1.16.1
        app.kubernetes.io/managed-by: Helm
    spec:
      securityContext: 
        fsGroupChangePolicy: OnRootMismatch
        runAsNonRoot: true
        runAsUser: 1000
        seLinuxOptions:
          type: spc_t
      containers:
      - image: "643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/flyteconsole:"
        imagePullPolicy: "IfNotPresent"
        name: flyteconsole
        envFrom:
        - configMapRef:
            name: flyte-console-config
        ports:
        - containerPort: 8080
        env:
        - name: ENABLE_GA
          value: "true"
        - name: GA_TRACKING_ID
          value: ""
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
        resources: 
          limits:
            cpu: 250m
            ephemeral-storage: 200Mi
            memory: 250Mi
          requests:
            cpu: 10m
            ephemeral-storage: 20Mi
            memory: 50Mi
        volumeMounts:
        - mountPath: /srv/flyte
          name: shared-data
      volumes:
      - emptyDir: {}
        name: shared-data
---
# Source: controlplane/charts/scylla-operator/templates/operator.deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: scylla-operator
  namespace: scylla-operator
  labels:
    app.kubernetes.io/name: scylla-operator
    app.kubernetes.io/instance: scylla-operator
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: scylla-operator
      app.kubernetes.io/instance: scylla-operator
  template:
    metadata:
      labels:
        app.kubernetes.io/name: scylla-operator
        app.kubernetes.io/instance: scylla-operator
    spec:
      serviceAccountName: scylla-operator
      containers:
      - name: scylla-operator
        image: scylladb/scylla-operator:1.18.1
        imagePullPolicy: IfNotPresent
        env:
        - name: SCYLLA_OPERATOR_IMAGE
          value: scylladb/scylla-operator:1.18.1
        args:
        - operator
        - --loglevel=2
        resources:
          requests:
            cpu: 100m
            memory: 20Mi
      terminationGracePeriodSeconds: 10
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/instance: scylla-operator
                  app.kubernetes.io/name: scylla-operator
              topologyKey: kubernetes.io/hostname
            weight: 1
---
# Source: controlplane/charts/scylla-operator/templates/webhookserver.deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: scylla-operator
  name: webhook-server
  labels:
    app.kubernetes.io/name: webhook-server
    app.kubernetes.io/instance: webhook-server
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: webhook-server
      app.kubernetes.io/instance: webhook-server
  template:
    metadata:
      labels:
        app.kubernetes.io/name: webhook-server
        app.kubernetes.io/instance: webhook-server
    spec:
      serviceAccountName: "webhook-server"
      containers:
      - name: webhook-server
        image: scylladb/scylla-operator:1.18.1
        imagePullPolicy: IfNotPresent
        args:
        - run-webhook-server
        - --loglevel=2
        - --tls-cert-file=/tmp/serving-certs/tls.crt
        - --tls-private-key-file=/tmp/serving-certs/tls.key
        livenessProbe:
          httpGet:
            path: /readyz
            port: 5000
            scheme: HTTPS
        readinessProbe:
          httpGet:
            path: /readyz
            port: 5000
            scheme: HTTPS
          initialDelaySeconds: 5
          periodSeconds: 10
        lifecycle:
          preStop:
            exec:
              command:
              - /usr/bin/sleep
              - 15s
        ports:
        - containerPort: 5000
          name: webhook-server
          protocol: TCP
        resources:
          requests:
            cpu: 10m
            memory: 20Mi
        volumeMounts:
        - mountPath: /tmp/serving-certs
          name: cert
          readOnly: true
      terminationGracePeriodSeconds: 75
      volumes:
      - name: cert
        secret:
          defaultMode: 420
          secretName: scylla-operator-serving-cert
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/instance: webhook-server
                  app.kubernetes.io/name: webhook-server
              topologyKey: kubernetes.io/hostname
            weight: 1
---
# Source: controlplane/templates/cacheservice/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cacheservice
  namespace: union
  labels: 
    app.kubernetes.io/name: cacheservice
    app.kubernetes.io/instance: release-name
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels: 
      app.kubernetes.io/name: cacheservice
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        configChecksum: "591e0b69b8f1c2f4b80cf97c3d2a301d0cbb1cf383d369e43062ecaba666632"
      labels: 
        app.kubernetes.io/name: cacheservice
        app.kubernetes.io/instance: release-name
        helm.sh/chart: controlplane-2026.1.4
        app.kubernetes.io/managed-by: Helm
    spec:
      securityContext: 
        fsGroup: 1001
        fsGroupChangePolicy: OnRootMismatch
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions:
          type: spc_t
      initContainers:
      - command:
        - cacheservice
        - --config
        - /etc/cacheservice/config/*.yaml
        - migrate
        - run
        image: "643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services:"
        imagePullPolicy: "IfNotPresent"
        name: run-migrations
        volumeMounts:
        - mountPath: /etc/db
          name: <KUBERNETES_SECRET_NAME>
        - mountPath: /etc/cacheservice/config
          name: config-volume
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
      containers:
      - command:
        - cacheservice
        - --config
        - /etc/cacheservice/config/*.yaml
        - serve
        image: "643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services:"
        imagePullPolicy: "IfNotPresent"
        name: cacheservice
        ports:
        - containerPort: 8088
        - containerPort: 8089
        - containerPort: 10254
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
        resources:
          limits:
            cpu: 1
            ephemeral-storage: 200Mi
          requests:
            cpu: 500m
            ephemeral-storage: 200Mi
            memory: 200Mi
        volumeMounts:
        - mountPath: /etc/db
          name: <KUBERNETES_SECRET_NAME>
        - mountPath: /etc/cacheservice/config
          name: config-volume
      serviceAccountName: cacheservice
      volumes:
      - name: <KUBERNETES_SECRET_NAME>
        secret:
          secretName: <KUBERNETES_SECRET_NAME>
      - emptyDir: {}
        name: shared-data
      - configMap:
          name: cacheservice-config
        name: config-volume
      affinity: 
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/name: cacheservice
            topologyKey: kubernetes.io/hostname
---
# Source: controlplane/templates/console/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: unionconsole
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: unionconsole
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: unionconsole
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: unionconsole
      labels:
        app.kubernetes.io/name: unionconsole
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: unionconsole
      securityContext:
        fsGroupChangePolicy: OnRootMismatch
        runAsNonRoot: true
        runAsUser: 1000
        seLinuxOptions:
          type: spc_t
      containers:
      - name: unionconsole
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
        image: "643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/unionconsole:2026.1.4"
        imagePullPolicy: IfNotPresent
        ports:
        - name: http
          containerPort: 8080
          protocol: TCP
        - name: http-metrics
          containerPort: 8081
          protocol: TCP
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 250m
            memory: 250Mi
---
# Source: controlplane/templates/deployment.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: authorizer
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: authorizer
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: authorizer
      app.kubernetes.io/instance: release-name
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: authorizer
        linkerd.io/inject: disabled
        prometheus.io/path: /metrics
        prometheus.io/port: "10254"
      labels:
        app.kubernetes.io/name: authorizer
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: authorizer
      volumes:
      - name: secrets
        secret:
          secretName: 
      - name: db-pass
        secret:
          secretName: 
      - name: config
        configMap:
          name: authorizer
      containers:
        - name: authorizer
          image: 643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services:2026.1.4
          imagePullPolicy: IfNotPresent
          args:
            - authorizer
            - serve
            - --config
            - /etc/config/*.yaml
          ports:
            - name: grpc
              containerPort: 8080
              protocol: TCP
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
          volumeMounts:
            - name: db-pass
              mountPath: /etc/db
            - name: secrets
              mountPath: /etc/secrets/union
            - name: config
              mountPath: /etc/config/
          env:
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.memory
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.cpu
          resources:
              limits:
                cpu: 500m
                memory: 512Mi
              requests:
                cpu: 250m
                memory: 250Mi
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: authorizer
                    app.kubernetes.io/instance: release-name
                topologyKey: "kubernetes.io/hostname"
---
# Source: controlplane/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: cluster
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: cluster
      app.kubernetes.io/instance: release-name
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: cluster
        linkerd.io/inject: disabled
        prometheus.io/path: /metrics
        prometheus.io/port: "10254"
      labels:
        app.kubernetes.io/name: cluster
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: cluster
      volumes:
      - name: secrets
        secret:
          secretName: 
      - name: db-pass
        secret:
          secretName: 
      - name: config
        configMap:
          name: cluster
      initContainers:
        - name: cluster-migrate
          image: 643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services:2026.1.4
          imagePullPolicy: IfNotPresent
          args:
          - cloudcluster
          - migrate
          - --config
          - /etc/config/*.yaml
          volumeMounts:
          - name: db-pass
            mountPath: /etc/db
          - name: secrets
            mountPath: /etc/secrets/union
          - name: config
            mountPath: /etc/config/
      containers:
        - name: cluster
          image: 643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services:2026.1.4
          imagePullPolicy: IfNotPresent
          args:
            - cloudcluster
            - serve
            - --config
            - /etc/config/*.yaml
          ports:
            - name: grpc
              containerPort: 8080
              protocol: TCP
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
          volumeMounts:
            - name: db-pass
              mountPath: /etc/db
            - name: secrets
              mountPath: /etc/secrets/union
            - name: config
              mountPath: /etc/config/
          env:
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.memory
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.cpu
          resources:
              limits:
                cpu: 500m
                memory: 512Mi
              requests:
                cpu: 250m
                memory: 250Mi
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: cluster
                    app.kubernetes.io/instance: release-name
                topologyKey: "kubernetes.io/hostname"
---
# Source: controlplane/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dataproxy
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: dataproxy
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: dataproxy
      app.kubernetes.io/instance: release-name
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: dataproxy
        linkerd.io/inject: disabled
        prometheus.io/path: /metrics
        prometheus.io/port: "10254"
      labels:
        app.kubernetes.io/name: dataproxy
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: dataproxy
      volumes:
      - name: secrets
        secret:
          secretName: 
      - name: db-pass
        secret:
          secretName: 
      - name: config
        configMap:
          name: dataproxy
      containers:
        - name: dataproxy
          image: 643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services:2026.1.4
          imagePullPolicy: IfNotPresent
          args:
            - dataproxy
            - serve
            - --config
            - /etc/config/*.yaml
          ports:
            - name: grpc
              containerPort: 8080
              protocol: TCP
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
          volumeMounts:
            - name: db-pass
              mountPath: /etc/db
            - name: secrets
              mountPath: /etc/secrets/union
            - name: config
              mountPath: /etc/config/
          env:
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.memory
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.cpu
          resources:
              limits:
                cpu: 500m
                memory: 512Mi
              requests:
                cpu: 250m
                memory: 250Mi
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: dataproxy
                    app.kubernetes.io/instance: release-name
                topologyKey: "kubernetes.io/hostname"
---
# Source: controlplane/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: executions
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: executions
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: executions
      app.kubernetes.io/instance: release-name
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: executions
        linkerd.io/inject: disabled
        prometheus.io/path: /metrics
        prometheus.io/port: "10254"
      labels:
        app.kubernetes.io/name: executions
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: executions
      volumes:
      - name: secrets
        secret:
          secretName: 
      - name: db-pass
        secret:
          secretName: 
      - name: config
        configMap:
          name: executions
      initContainers:
        - name: executions-migrate
          image: 643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services:2026.1.4
          imagePullPolicy: IfNotPresent
          args:
          - cloudpropeller
          - migrate
          - --config
          - /etc/config/*.yaml
          volumeMounts:
          - name: db-pass
            mountPath: /etc/db
          - name: secrets
            mountPath: /etc/secrets/union
          - name: config
            mountPath: /etc/config/
      containers:
        - name: executions
          image: 643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services:2026.1.4
          imagePullPolicy: IfNotPresent
          args:
            - cloudpropeller
            - serve
            - --config
            - /etc/config/*.yaml
          ports:
            - name: grpc
              containerPort: 8080
              protocol: TCP
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
          volumeMounts:
            - name: db-pass
              mountPath: /etc/db
            - name: secrets
              mountPath: /etc/secrets/union
            - name: config
              mountPath: /etc/config/
          env:
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.memory
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.cpu
          resources:
              limits:
                cpu: 500m
                memory: 512Mi
              requests:
                cpu: 250m
                memory: 250Mi
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: executions
                    app.kubernetes.io/instance: release-name
                topologyKey: "kubernetes.io/hostname"
---
# Source: controlplane/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: queue
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: queue
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: queue
      app.kubernetes.io/instance: release-name
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: queue
        linkerd.io/inject: disabled
        prometheus.io/path: /metrics
        prometheus.io/port: "10254"
      labels:
        app.kubernetes.io/name: queue
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: queue
      volumes:
      - name: secrets
        secret:
          secretName: 
      - name: db-pass
        secret:
          secretName: 
      - name: config
        configMap:
          name: queue
      initContainers:
        - name: queue-migrate
          image: 643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services:2026.1.4
          imagePullPolicy: IfNotPresent
          args:
          - queue
          - migrate
          - --config
          - /etc/config/*.yaml
          volumeMounts:
          - name: db-pass
            mountPath: /etc/db
          - name: secrets
            mountPath: /etc/secrets/union
          - name: config
            mountPath: /etc/config/
      containers:
        - name: queue
          image: 643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services:2026.1.4
          imagePullPolicy: IfNotPresent
          args:
            - queue
            - serve
            - --config
            - /etc/config/*.yaml
          ports:
            - name: grpc
              containerPort: 8080
              protocol: TCP
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
          volumeMounts:
            - name: db-pass
              mountPath: /etc/db
            - name: secrets
              mountPath: /etc/secrets/union
            - name: config
              mountPath: /etc/config/
          env:
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.memory
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.cpu
          resources:
              limits:
                cpu: 500m
                memory: 512Mi
              requests:
                cpu: 250m
                memory: 250Mi
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: queue
                    app.kubernetes.io/instance: release-name
                topologyKey: "kubernetes.io/hostname"
---
# Source: controlplane/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: run-scheduler
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: run-scheduler
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: run-scheduler
      app.kubernetes.io/instance: release-name
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: run-scheduler
        linkerd.io/inject: disabled
        prometheus.io/path: /metrics
        prometheus.io/port: "10254"
      labels:
        app.kubernetes.io/name: run-scheduler
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: run-scheduler
      volumes:
      - name: secrets
        secret:
          secretName: 
      - name: db-pass
        secret:
          secretName: 
      - name: config
        configMap:
          name: run-scheduler
      initContainers:
        - name: run-scheduler-migrate
          image: 643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services:2026.1.4
          imagePullPolicy: IfNotPresent
          args:
          - cloudpropeller
          - migrate
          - --config
          - /etc/config/*.yaml
          volumeMounts:
          - name: db-pass
            mountPath: /etc/db
          - name: secrets
            mountPath: /etc/secrets/union
          - name: config
            mountPath: /etc/config/
      containers:
        - name: run-scheduler
          image: 643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services:2026.1.4
          imagePullPolicy: IfNotPresent
          args:
            - cloudpropeller
            - scheduler
            - start
            - --config
            - /etc/config/*.yaml
          ports:
            - name: grpc
              containerPort: 8080
              protocol: TCP
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
          volumeMounts:
            - name: db-pass
              mountPath: /etc/db
            - name: secrets
              mountPath: /etc/secrets/union
            - name: config
              mountPath: /etc/config/
          env:
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.memory
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.cpu
          resources:
              limits:
                cpu: 500m
                memory: 512Mi
              requests:
                cpu: 250m
                memory: 250Mi
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: run-scheduler
                    app.kubernetes.io/instance: release-name
                topologyKey: "kubernetes.io/hostname"
---
# Source: controlplane/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: usage
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: usage
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: usage
      app.kubernetes.io/instance: release-name
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: usage
        linkerd.io/inject: disabled
        prometheus.io/path: /metrics
        prometheus.io/port: "10254"
      labels:
        app.kubernetes.io/name: usage
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: usage
      volumes:
      - name: secrets
        secret:
          secretName: 
      - name: db-pass
        secret:
          secretName: 
      - name: config
        configMap:
          name: usage
      containers:
        - name: usage
          image: 643379628101.dkr.ecr.us-east-1.amazonaws.com/union-cp/services:2026.1.4
          imagePullPolicy: IfNotPresent
          args:
            - usage
            - serve
            - --config
            - /etc/config/*.yaml
          ports:
            - name: grpc
              containerPort: 8080
              protocol: TCP
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
          volumeMounts:
            - name: db-pass
              mountPath: /etc/db
            - name: secrets
              mountPath: /etc/secrets/union
            - name: config
              mountPath: /etc/config/
          env:
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.memory
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.cpu
          resources:
              limits:
                cpu: 3
                memory: 512Mi
              requests:
                cpu: 500m
                memory: 250Mi
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: usage
                    app.kubernetes.io/instance: release-name
                topologyKey: "kubernetes.io/hostname"
---
# Source: controlplane/charts/flyte/templates/admin/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: flyteadmin
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.1
    #app.kubernetes.io/managed-by: Helm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: flyteadmin
  minReplicas: 4
  maxReplicas: 10
  metrics:
    
    - resource:
        name: cpu
        target:
          averageUtilization: 70
          type: Utilization
      type: Resource
---
# Source: controlplane/templates/console/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: unionconsole
  labels:
    helm.sh/chart: controlplane-2026.1.4
    app.kubernetes.io/name: unionconsole
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2026.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: unionconsole
  minReplicas: 1
  maxReplicas: 1
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
---
# Source: controlplane/templates/hpa.yaml
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: authorizer
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: authorizer
  minReplicas: 1
  maxReplicas: 1
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
---
# Source: controlplane/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: cluster
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: cluster
  minReplicas: 1
  maxReplicas: 1
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
---
# Source: controlplane/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: dataproxy
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: dataproxy
  minReplicas: 1
  maxReplicas: 1
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
---
# Source: controlplane/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: executions
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: executions
  minReplicas: 1
  maxReplicas: 1
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
---
# Source: controlplane/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: queue
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: queue
  minReplicas: 1
  maxReplicas: 1
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
---
# Source: controlplane/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: run-scheduler
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: run-scheduler
  minReplicas: 1
  maxReplicas: 1
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
---
# Source: controlplane/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: usage
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: usage
  minReplicas: 1
  maxReplicas: 1
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane-dataproxy
  namespace: union
  annotations: 
    nginx.ingress.kubernetes.io/app-root: /v2
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.org/websocket-services: dataproxy-service
spec:
  ingressClassName: "controlplane"
  tls:
    - hosts:
      - fake-host.domain
      secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          - path: /data/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /data
            pathType: Prefix
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane-usage-grpc
  namespace: union
  annotations: 
    nginx.ingress.kubernetes.io/app-root: /v2
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/backend-protocol: GRPC
    nginx.ingress.kubernetes.io/use-regex: "true"
spec:
  ingressClassName: "controlplane"
  tls:
    - hosts:
      - fake-host.domain
      secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          - path: /cloudidl.usage.UsageService(/(?!GetCustomMeasuresNames|GetMeasureGroup|GetMeasureGroups|GetBillableMeasures|GetBillingInfo|ReportBillableUsage|ReportServerlessBillableUsage|CreateCustomer|AttachBillingPlanToCustomer|GetCustomerCredits|EnqueueMetronomeRequest|EnqueueStripeRequest|GetOrgCheckoutSession).*|$)
            pathType: ImplementationSpecific
            backend:
              service:
                name: usage
                port:
                  number: 80
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane-usage
  namespace: union
  annotations: 
    nginx.ingress.kubernetes.io/app-root: /v2
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.org/websocket-services: dataproxy-service
    nginx.ingress.kubernetes.io/use-regex: "true"
spec:
  ingressClassName: "controlplane"
  tls:
    - hosts:
      - fake-host.domain
      secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          - path: /usage/api/v1(/(?!custom_measures_names|measure_group|measure_groups|billable_measures|billing_info|report_billable_usage|customer_credits|checkout_session).*|$)
            pathType: ImplementationSpecific
            backend:
              service:
                name: usage
                port:
                  number: 81
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane-protected
  namespace: union
  annotations: 
    nginx.ingress.kubernetes.io/app-root: /v2
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.org/websocket-services: dataproxy-service
spec:
  ingressClassName: "controlplane"
  tls:
    - hosts:
      - fake-host.domain
      secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          - path: /api
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /api/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /v1/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /cloudadmin
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /cloudadmin/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /actor
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /actor/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /agent
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /agent/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /dataplane
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /dataplane/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /spark-history-server
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /spark-history-server/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /api/v1/dataproxy
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /api/v1/dataproxy/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /app
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 81
          - path: /app/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 81
          - path: /apps
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 81
          - path: /apps/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 81
          - path: /cluster
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 81
          - path: /cluster/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 81
          - path: /clusterpool
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 81
          - path: /clusterpool/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 81
          - path: /clusterconfig
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 81
          - path: /clusterconfig/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 81
          - path: /org
            pathType: ImplementationSpecific
            backend:
              service:
                name: organizations
                port:
                  number: 81
          - path: /org/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: organizations
                port:
                  number: 81
          - path: /managed_cluster
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 81
          - path: /managed_cluster/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 81
          - path: /authorizer
            pathType: ImplementationSpecific
            backend:
              service:
                name: authorizer
                port:
                  number: 81
          - path: /authorizer/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: authorizer
                port:
                  number: 81
          - path: /oauth_app
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /oauth_app/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /users
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /users/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /roles
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /roles/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /policies
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /policies/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /identities
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /identities/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /echo
            pathType: ImplementationSpecific
            backend:
              service:
                name: execution
                port:
                  number: 81
          - path: /echo/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: execution
                port:
                  number: 81
          - path: /execution
            pathType: ImplementationSpecific
            backend:
              service:
                name: execution
                port:
                  number: 81
          - path: /execution/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: execution
                port:
                  number: 81
          - path: /workspace_registry
            pathType: ImplementationSpecific
            backend:
              service:
                name: execution
                port:
                  number: 81
          - path: /workspace_registry/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: execution
                port:
                  number: 81
          - path: /workspace_instance
            pathType: ImplementationSpecific
            backend:
              service:
                name: execution
                port:
                  number: 81
          - path: /workspace_instance/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: execution
                port:
                  number: 81
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane-protected-grpc
  namespace: union
  annotations: 
    nginx.ingress.kubernetes.io/app-root: /v2
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/backend-protocol: GRPC
spec:
  ingressClassName: "controlplane"
  tls:
    - hosts:
      - fake-host.domain
      secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          - path: /cloudidl.execution.ExecutionService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.execution.ExecutionService
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.cluster.ClusterService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.cluster.ClusterService
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.apikey.APIKeyService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 83
          - path: /cloudidl.apikey.APIKeyService
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 83
          - path: /cloudidl.identity.AppsService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.AppsService
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.org.OrgService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: organizations
                port:
                  number: 80
          - path: /cloudidl.org.OrgService
            pathType: ImplementationSpecific
            backend:
              service:
                name: organizations
                port:
                  number: 80
          - path: /cloudidl.cloudaccounts.CloudAccountsService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.cloudaccounts.CloudAccountsService
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.cluster.ManagedClusterService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.cluster.ManagedClusterService
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.identity.UserService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.UserService
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.RoleService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.RoleService
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.PolicyService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.PolicyService
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.SelfServe/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.SelfServe
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.IdentityService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.IdentityService
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.clusterpool.ClusterPoolService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.clusterpool.ClusterPoolService
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.clusterconfig.ClusterConfigService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.clusterconfig.ClusterConfigService
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.authorizer.AuthorizerService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: authorizer
                port:
                  number: 80
          - path: /cloudidl.authorizer.AuthorizerService
            pathType: ImplementationSpecific
            backend:
              service:
                name: authorizer
                port:
                  number: 80
          - path: /datacatalog.DataCatalog/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: datacatalog
                port:
                  number: 89
          - path: /datacatalog.DataCatalog
            pathType: ImplementationSpecific
            backend:
              service:
                name: datacatalog
                port:
                  number: 89
          - path: /flyteidl.cacheservice.CacheService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cacheservice
                port:
                  number: 89
          - path: /flyteidl.cacheservice.CacheService
            pathType: ImplementationSpecific
            backend:
              service:
                name: cacheservice
                port:
                  number: 89
          - path: /flyteidl.cacheservice.v2.CacheService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cacheservice
                port:
                  number: 89
          - path: /flyteidl.cacheservice.v2.CacheService
            pathType: ImplementationSpecific
            backend:
              service:
                name: cacheservice
                port:
                  number: 89
          - path: /cloudidl.actor.ActorEnvironmentService
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.actor.ActorEnvironmentService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.agent.AgentService
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.agent.AgentService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.secret.SecretService
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.secret.SecretService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /flyteidl2.secret.SecretService
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /flyteidl2.secret.SecretService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.support.SupportService
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.clouddataproxy.CloudDataProxyService
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.clouddataproxy.CloudDataProxyService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /flyteidl.service.DataProxyService
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /flyteidl.service.DataProxyService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.logs.LogsService
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.logs.LogsService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.workspace.WorkspaceRegistryService
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workspace.WorkspaceRegistryService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workspace.WorkspaceInstanceService
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workspace.WorkspaceInstanceService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workflow.RunService
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workflow.RunService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workflow.InternalRunService
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workflow.InternalRunService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workflow.TranslatorService
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workflow.TranslatorService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workflow.TaskService
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workflow.TaskService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workflow.TriggerService
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workflow.TriggerService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workflow.QueueService
            pathType: ImplementationSpecific
            backend:
              service:
                name: queue
                port:
                  number: 80
          - path: /cloudidl.workflow.QueueService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: queue
                port:
                  number: 80
          - path: /cloudidl.workflow.StateService
            pathType: ImplementationSpecific
            backend:
              service:
                name: queue
                port:
                  number: 80
          - path: /cloudidl.workflow.StateService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: queue
                port:
                  number: 80
          
          - path: /flyteidl2.workflow.RunService
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /flyteidl2.workflow.RunService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /flyteidl2.workflow.TranslatorService
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /flyteidl2.workflow.TranslatorService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /flyteidl2.task.TaskService
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /flyteidl2.task.TaskService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /flyteidl2.workflow.QueueService
            pathType: ImplementationSpecific
            backend:
              service:
                name: queue
                port:
                  number: 80
          - path: /flyteidl2.workflow.QueueService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: queue
                port:
                  number: 80
          - path: /flyteidl2.trigger.TriggerService
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /flyteidl2.trigger.TriggerService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /flyteidl2.workflow.StateService
            pathType: ImplementationSpecific
            backend:
              service:
                name: queue
                port:
                  number: 80
          - path: /flyteidl2.workflow.StateService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: queue
                port:
                  number: 80
          - path: /cloudidl.imagebuilder.ImageService
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.imagebuilder.ImageService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.app.AppService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.app.AppLogsService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.app.ReplicaService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane-protected-grpc-streaming
  namespace: union
  annotations: 
    nginx.ingress.kubernetes.io/app-root: /v2
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/backend-protocol: GRPC
spec:
  ingressClassName: "controlplane"
  tls:
    - hosts:
      - fake-host.domain
      secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          - path: /flyteidl.service.AdminService
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /flyteidl.service.AdminService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          
          - path: /flyteidl.service.WatchService
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          
          - path: /flyteidl.service.WatchService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /cloudidl.cloudadmin.CloudAdminService
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /cloudidl.cloudadmin.CloudAdminService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /flyteidl.service.IdentityService
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /flyteidl.service.IdentityService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /cloudidl.echo.EchoService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.echo.EchoService
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /flyteidl.service.SignalService
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /flyteidl.service.SignalService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /cloudidl.actor.ActorEnvironmentService/Stream*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.execution.ExecutionService/GetExecutionOperation
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workflow.RunLogsService/TailLogs
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workflow.RunService/Watch*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workflow.InternalRunService/Record*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workflow.InternalRunService/Update*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workflow.TaskService/Watch*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workflow.LeaseService/Heartbeat
            pathType: ImplementationSpecific
            backend:
              service:
                name: queue
                port:
                  number: 80
          - path: /cloudidl.workflow.QueueService/Heartbeat
            pathType: ImplementationSpecific
            backend:
              service:
                name: queue
                port:
                  number: 80
          - path: /cloudidl.workflow.StateService/Watch*
            pathType: ImplementationSpecific
            backend:
              service:
                name: queue
                port:
                  number: 80
          - path: /cloudidl.workflow.QueueService/StreamLeases
            pathType: ImplementationSpecific
            backend:
              service:
                name: queue
                port:
                  number: 80
          - path: /cloudidl.workflow.LeaseService/StreamLeases
            pathType: ImplementationSpecific
            backend:
              service:
                name: queue
                port:
                  number: 80
          
          - path: /flyteidl2.workflow.RunLogsService/TailLogs
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /flyteidl2.workflow.RunService/Watch*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /flyteidl2.task.TaskService/Watch*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /flyteidl2.workflow.QueueService/Heartbeat
            pathType: ImplementationSpecific
            backend:
              service:
                name: queue
                port:
                  number: 80
          - path: /flyteidl2.workflow.StateService/Watch*
            pathType: ImplementationSpecific
            backend:
              service:
                name: queue
                port:
                  number: 80
          - path: /flyteidl2.workflow.QueueService/StreamLeases
            pathType: ImplementationSpecific
            backend:
              service:
                name: queue
                port:
                  number: 80
          - path: /cloudidl.logs.LogsService/TailTaskExecutionLogs
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.workspace.WorkspaceInstanceService/WatchWorkspaceInstances
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.app.AppService/Watch
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.app.AppService/Lease
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.app.AppLogsService/TailLogs
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.app.ReplicaService/WatchReplicas
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane
  namespace: union
  annotations: 
    nginx.ingress.kubernetes.io/app-root: /v2
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
spec:
  ingressClassName: "controlplane"
  tls:
    - hosts:
      - fake-host.domain
      secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          # Port 87 in FlyteAdmin maps to the redoc container.
          - path: /openapi
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 87
          - path: /healthcheck
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /me
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          # Port 87 in FlyteAdmin maps to the redoc container.
          - path: /openapi/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 87
          - path: /.well-known
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /.well-known/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /login
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /login/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /logout
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /logout/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /callback
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /callback/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /config
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /config/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /oauth2
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /oauth2/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /auth
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /auth/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /enqueue_metronome_request/v1
            pathType: ImplementationSpecific
            backend:
              service:
                name: usage
                port:
                  number: 81
          - path: /enqueue_metronome_request/v1/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: usage
                port:
                  number: 81
          - path: /enqueue_stripe_request/v1
            pathType: ImplementationSpecific
            backend:
              service:
                name: usage
                port:
                  number: 81
          - path: /enqueue_stripe_request/v1/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: usage
                port:
                  number: 81
---
# Source: controlplane/templates/flyte-core-app.yaml
# Certain ingress controllers like nginx cannot serve HTTP 1 and GRPC with a single ingress because GRPC can only
# enabled on the ingress object, not on backend services (GRPC annotation is set on the ingress, not on the services).
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane-grpc
  namespace: union
  annotations:
    nginx.ingress.kubernetes.io/app-root: /v2
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/backend-protocol: GRPC
spec:
  ingressClassName: "controlplane"
  tls:
    - hosts:
      - fake-host.domain
      secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          # NOTE: Port 81 in flyteadmin is the GRPC server port for FlyteAdmin.
          - path: /grpc.health.v1.Health
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /grpc.health.v1.Health/*
            pathType:  ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /flyteidl.service.AuthMetadataService
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /flyteidl.service.AuthMetadataService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane-grpc-streaming
  namespace: union
  annotations:
    nginx.ingress.kubernetes.io/app-root: /v2
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/backend-protocol: GRPC
spec:
  ingressClassName: "controlplane"
  tls:
    - hosts:
      - fake-host.domain
      secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          - path: /flyteidl.service.WatchService/WatchExecutionStatusUpdates
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane-console-protected
  namespace: union
  annotations:
    nginx.ingress.kubernetes.io/app-root: /v2
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/auth-cache-key: $http_flyte_authorization$http_cookie
    nginx.org/websocket-services: dataproxy-service
spec:
  ingressClassName: "controlplane"
  tls:
    - hosts:
      - fake-host.domain
      secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          - path: /
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          # NOTE: If you change this, you must update the BASE_URL value in flyteconsole.yaml
          - path: /console
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /console/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /dashboard
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /dashboard/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /resources
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /resources/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /cost
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /cost/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /loading
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /loading/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /v2
            pathType: ImplementationSpecific
            backend:
              service:
                name: unionconsole
                port:
                  number: 80
          - path: /v2/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: unionconsole
                port:
                  number: 80
---
# Source: controlplane/charts/scylla-operator/templates/validatingwebhook.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  annotations:
    cert-manager.io/inject-ca-from: scylla-operator/scylla-operator-serving-cert
  name: scylla-operator
webhooks:
- name: webhook.scylla.scylladb.com
  clientConfig:
    service:
      name: scylla-operator-webhook
      namespace: scylla-operator
      path: /validate
  admissionReviewVersions:
  - v1
  sideEffects: None
  failurePolicy: Fail
  rules:
  - apiGroups:
    - scylla.scylladb.com
    apiVersions:
    - v1
    operations:
    - CREATE
    - UPDATE
    resources:
    - scyllaclusters
  - apiGroups:
    - scylla.scylladb.com
    apiVersions:
    - v1alpha1
    operations:
    - CREATE
    - UPDATE
    resources:
    - nodeconfigs
    - scyllaoperatorconfigs
    - scylladbdatacenters
    - scylladbclusters
    - scylladbmanagerclusterregistrations
    - scylladbmanagertasks
---
# Source: controlplane/templates/secret.yaml
---
---
# Source: controlplane/templates/secret.yaml
---
---
# Source: controlplane/templates/secret.yaml
---
---
# Source: controlplane/templates/secret.yaml
---
---
# Source: controlplane/charts/scylla-operator/templates/certificate.yaml
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: scylla-operator-serving-cert
  namespace: scylla-operator
spec:
  dnsNames:
  - scylla-operator-webhook.scylla-operator.svc
  issuerRef:
    kind: Issuer
    name: scylla-operator-selfsigned-issuer
  secretName: scylla-operator-serving-cert
---
# Source: controlplane/charts/scylla-operator/templates/issuer.yaml
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: scylla-operator-selfsigned-issuer
  namespace: scylla-operator
spec:
  selfSigned: {}
---
# Source: controlplane/charts/scylla/templates/scyllacluster.yaml
apiVersion: scylla.scylladb.com/v1
kind: ScyllaCluster
metadata:
  name: scylla
  namespace: union
spec:
  version: 2025.1.5
  agentVersion: 3.5.1@sha256:d1b57d08b9949c8faad2048fdf4dc7c502dae81da856c3c6b3a77dd347d5c7fc
  repository: scylladb/scylla
  agentRepository: scylladb/scylla-manager-agent
  developerMode: true
  sysctls:
    - fs.aio-max-nr=30000000
  datacenter:
    name: dc1
    racks:
    - agentResources:
        requests:
          cpu: 50m
          memory: 10M
      members: 3
      name: rack1
      placement:
        nodeAffinity: {}
        tolerations: []
      resources:
        limits:
          cpu: 2
          memory: 4Gi
        requests:
          cpu: 1
          memory: 2Gi
      storage:
        capacity: 100Gi
        storageClassName: scylladb
