---
# Source: dataplane/templates/nodeexecutor/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: executor
  namespace: union
  labels:
    app: executor
---
# Source: dataplane/templates/operator/serviceaccount-proxy.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: proxy-system
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: dataplane/templates/operator/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: operator-system
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: dataplane/templates/propeller/serviceaccount-webhook.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flytepropeller-webhook-system
  namespace: union
---
# Source: dataplane/templates/propeller/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flytepropeller-system
  namespace: union
---
# Source: dataplane/templates/common/auth-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: union-secret-auth
  namespace: union
type: Opaque
data:
  # TODO(rob): update or configure operator to use client_secret like all the other components.
  app_secret: bXktc2VjcmV0
  client_secret: bXktc2VjcmV0
---
# Source: dataplane/templates/common/cluster-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: operator-cluster-name
type: Opaque
data:
  cluster_name: bXktY2x1c3Rlcg==
---
# Source: dataplane/templates/propeller/deployment-webhook.yaml
# Create an empty secret that the first propeller pod will populate
apiVersion: v1
kind: Secret
metadata:
  name: flyte-pod-webhook
  namespace: union
type: Opaque
---
# Source: dataplane/templates/clusterresourcesync/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: clusterresource-template
  namespace: union
  labels: 
    app.kubernetes.io/name: clusterresourcesync
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
data:
  a_namespace.yaml: | 
    apiVersion: v1
    kind: Namespace
    metadata:
      name: {{ namespace }}
      labels:
        union.ai/namespace-type: flyte
    spec:
      finalizers:
      - kubernetes
    
  b_default_service_account.yaml: | 
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: default
      namespace: {{ namespace }}
      annotations:
        {{ defaultUserRoleKey }}: {{ defaultUserRoleValue }}
    
  c_project_resource_quota.yaml: | 
    apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: project-quota
      namespace: {{ namespace }}
    spec:
      hard:
        limits.cpu: {{ projectQuotaCpu }}
        limits.memory: {{ projectQuotaMemory }}
        requests.nvidia.com/gpu: {{ projectQuotaNvidiaGpu }}
---
# Source: dataplane/templates/nodeexecutor/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: executor
  namespace: union
  labels:
    app: executor
data:
  task_logs.yaml: | 
    plugins:
      logs:
        cloudwatch-enabled: false
        kubernetes-enabled: true
  enabled_plugins.yaml: |
    tasks:
      task-plugins:
        default-for-task-types:
          actor: fast-task
          container: container
          container_array: k8s-array
          sidecar: sidecar
        enabled-plugins:
        - container
        - sidecar
        - k8s-array
        - agent-service
        - echo
        - fast-task
  config.yaml: |
    executor:
      cluster: 'my-cluster'
      evaluatorCount: 64
      maxActions: 2000
      organization: 'union'
      unionAuth:
        injectSecret: true
        secretName: EAGER_API_KEY
      workerName: worker1
      task_resources:
        defaults:
          cpu: 100m
          memory: 500Mi
        limits:
          cpu: 2
          gpu: 1
          memory: 1Gi
    namespace_mapping:
      template: 'union'
    union:
      connection:
        host: dns:///union.us-west-2.union.ai
      auth:
        authorizationMetadataKey: flyte-authorization
        clientId: 'my-client-id'
        clientSecretLocation: /etc/union/secret/client_secret
        tokenRefreshWindow: 5m
        type: ClientSecret
    admin:
      clientId: 'my-client-id'
      clientSecretLocation: /etc/union/secret/client_secret
      endpoint: dns:///union.us-west-2.union.ai
      insecure: false
    authorizer:
      type: noop
    catalog-cache:
      cache-endpoint: dns:///union.us-west-2.union.ai
      endpoint: dns:///union.us-west-2.union.ai
      insecure: false
      type: fallback
      use-admin-auth: true
    logger:
      level: 4
      show-source: true
    sharedService:
      metrics:
        scope: 'executor:'
      security:
        allowCors: true
        allowLocalhostAccess: true
        allowedHeaders:
        - Content-Type
        allowedOrigins:
        - '*'
        secure: false
        useAuth: false
    propeller:
      limit-namespace: 'union'
      node-config:
        disable-input-file-writes: true
    plugins:
      fasttask:
        additional-worker-args:
        - --last-ack-grace-period-seconds
        - "120"
        callback-uri: http://unionai-dataplane-executor.union.svc.cluster.local:15605
        grace-period-status-not-found: 2m
      ioutils:
        remoteFileOutputPaths:
          deckFilename: report.html
      k8s:
        disable-inject-owner-references: true
        default-cpus: 100m
        default-env-vars:
        - FLYTE_AWS_ENDPOINT: http://localstack.aws.svc.cluster.local:4566
        - FLYTE_AWS_ACCESS_KEY_ID: test123
        - FLYTE_AWS_SECRET_ACCESS_KEY: test
        default-memory: 100Mi
        co-pilot:
          image: 'cr.flyte.org/flyteorg/flytecopilot:v1.14.1'
          name: flyte-copilot-
          start-timeout: 30s
    storage:
      container: bucket
      type: stow
      stow:
        kind: s3
        config:
          auth_type: accesskey
          access_key_id: test123
          secret_key: test
          disable_ssl: false
          endpoint: http://localstack.aws.svc.cluster.local:4566
          region: us-west-2
      enable-multicontainer: false
      limits:
        maxDownloadMBs: 1024
      cache:
        max_size_mbs: 0
        target_gc_percent: 70
---
# Source: dataplane/templates/operator/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: union-operator
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
data:
  k8s.yaml: | 
    plugins:
      k8s:
        default-cpus: 100m
        default-env-vars:
        - FLYTE_AWS_ENDPOINT: http://localstack.aws.svc.cluster.local:4566
        - FLYTE_AWS_ACCESS_KEY_ID: test123
        - FLYTE_AWS_SECRET_ACCESS_KEY: test
        default-memory: 100Mi
  config.yaml: |
    union:
      connection: 
        host: dns:///union.us-west-2.union.ai
      auth: 
        authorizationMetadataKey: flyte-authorization
        clientId: 'my-client-id'
        clientSecretLocation: /etc/union/secret/client_secret
        tokenRefreshWindow: 5m
        type: ClientSecret
    sharedService: 
      features:
        gatewayV2: true
      port: 8081
    authorizer: 
      type: noop
    operator:
      enabled: true
      enableTunnelService: true
      tunnel:
        enableDirectToAppIngress: false
        deploymentToRestart: union-operator-proxy
      limitNamespace: 'union'
      disableClusterPermissions: true
      apps: 
        enabled: 'false'
      syncClusterConfig: 
        enabled: false
      clusterId: 
        organization: 'union'
      clusterData:
        appId: 'my-client-id'
        bucketName: 'bucket'
        bucketRegion: 'us-west-2'
        cloudHostName: 'union.us-west-2.union.ai'
        gcpProjectId: ''
        metadataBucketPrefix: s3://bucket
        userRole: 'arn:aws:iam::ACCOUNT_ID:role/flyte_project_role'
        userRoleKey: 'eks.amazonaws.com/role-arn'
      collectUsages: 
        enabled: false
      billableUsageCollector: 
        enabled: false
      dependenciesHeartbeat: 
        propeller:
          endpoint: 'http://flytepropeller:10254'
        proxy:
          endpoint: 'http://union-operator-proxy:10254'
    proxy: 
      imageBuilderConfig:
        authenticationType: 'noop'
        defaultRepository: ''
      persistedLogs:
        objectStore:
          pathTemplate: namespace-{{.KubernetesNamespace}}.pod-{{.KubernetesPodName}}.cont-{{.KubernetesContainerName}}
          prefix: persisted-logs
        sourceType: ObjectStore
      smConfig:
        enabled: 'true'
        k8sConfig:
          namespace: 'union'
        type: 'K8s'
  logger.yaml: |
    logger: 
      level: 4
      show-source: true
  config-overrides.yaml: | 
    cache:
      identity:
        enabled: false
  storage.yaml: | 
    storage:
      container: bucket
      type: stow
      stow:
        kind: s3
        config:
          auth_type: accesskey
          access_key_id: test123
          secret_key: test
          disable_ssl: false
          endpoint: http://localstack.aws.svc.cluster.local:4566
          region: us-west-2
      enable-multicontainer: false
      limits:
        maxDownloadMBs: 1024
      cache:
        max_size_mbs: 0
        target_gc_percent: 70
  fast_registration_storage.yaml: | 
    fastRegistrationStorage:
      container: "bucket"
      type: stow
      stow:
        kind: s3
        config:
          auth_type: accesskey
          access_key_id: test123
          secret_key: test
          disable_ssl: false
          endpoint: http://localstack.aws.svc.cluster.local:4566
          region: us-west-2
---
# Source: dataplane/templates/propeller/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: flyte-propeller-config
  namespace: union
data:
  admin.yaml: | 
    admin:
      clientId: 'my-client-id'
      clientSecretLocation: /etc/union/secret/client_secret
      endpoint: dns:///union.us-west-2.union.ai
      insecure: false
    event:
      capacity: 1000
      rate: 500
      type: admin
  catalog.yaml: | 
    catalog-cache:
      cache-endpoint: dns:///union.us-west-2.union.ai
      endpoint: dns:///union.us-west-2.union.ai
      insecure: false
      type: fallback
      use-admin-auth: true
  copilot.yaml: | 
    plugins:
      k8s:
        co-pilot:
          image: 'cr.flyte.org/flyteorg/flytecopilot:v1.14.1'
          name: flyte-copilot-
          start-timeout: 30s
  core.yaml: | 
    propeller:
      downstream-eval-duration: 30s
      enable-admin-launcher: true
      leader-election:
        enabled: true
        lease-duration: 15s
        lock-config-map:
          name: propeller-leader
          namespace: 'union'
        renew-deadline: 10s
        retry-period: 2s
      limit-namespace: 'union'
      literal-offloading-config:
        enabled: true
      max-workflow-retries: 30
      metadata-prefix: metadata/propeller
      metrics-prefix: flyte
      prof-port: 10254
      queue:
        batch-size: -1
        batching-interval: 2s
        queue:
          base-delay: 5s
          capacity: 1000
          max-delay: 120s
          rate: 100
          type: maxof
        sub-queue:
          capacity: 100
          rate: 10
          type: bucket
        type: batch
      rawoutput-prefix: s3://bucket
      workers: 4
      workflow-reeval-duration: 30s
    webhook:
      certDir: /etc/webhook/certs
      disableCreateMutatingWebhookConfig: true
      embeddedSecretManagerConfig:
        imagePullSecrets:
          enabled: true
        k8sConfig:
          namespace: 'union'
        type: 'K8s'
      secretManagerTypes:
      - Embedded
      - K8s
      serviceName: flyte-pod-webhook
  enabled_plugins.yaml: | 
    tasks:
      task-plugins:
        default-for-task-types:
          actor: fast-task
          container: container
          container_array: k8s-array
          sidecar: sidecar
        enabled-plugins:
        - container
        - sidecar
        - k8s-array
        - agent-service
        - echo
        - fast-task
  k8s.yaml: | 
    plugins:
      k8s:
        default-cpus: 100m
        default-env-vars:
        - FLYTE_AWS_ENDPOINT: http://localstack.aws.svc.cluster.local:4566
        - FLYTE_AWS_ACCESS_KEY_ID: test123
        - FLYTE_AWS_SECRET_ACCESS_KEY: test
        default-memory: 100Mi
  logger.yaml: |
    logger: 
      level: 4
      show-source: true
  resource_manager.yaml: | 
    propeller:
      resourcemanager:
        type: noop
  task_logs.yaml: | 
    plugins:
      logs:
        cloudwatch-enabled: false
        dynamic-log-links:
        - vscode:
            displayName: VS Code Debugger
            templateUris:
            - /dataplane/pod/v1/generated_name/task/{{.executionProject}}/{{.executionDomain}}/{{.executionName}}/{{.nodeID}}/{{.taskRetryAttempt}}/{{.taskProject}}/{{.taskDomain}}/{{.taskID}}/{{.taskVersion}}/
        kubernetes-enabled: false
        templates:
        - displayName: Task Logs
          scheme: TaskExecution
          templateUris:
          - /console/projects/{{.executionProject}}/domains/{{.executionDomain}}/executions/{{.executionName}}/nodeId/{{.nodeID}}/taskId/{{.taskID}}/attempt/{{.taskRetryAttempt}}/view/logs?duration=all&fromExecutionNav=true
  storage.yaml: | 
    storage:
      container: bucket
      type: stow
      stow:
        kind: s3
        config:
          auth_type: accesskey
          access_key_id: test123
          secret_key: test
          disable_ssl: false
          endpoint: http://localstack.aws.svc.cluster.local:4566
          region: us-west-2
      enable-multicontainer: false
      limits:
        maxDownloadMBs: 1024
      cache:
        max_size_mbs: 0
        target_gc_percent: 70
---
# Source: dataplane/templates/operator/serviceaccount-proxy-secret.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: proxy-system-secret
  namespace: union
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - '*'
    resources:
      - secrets
    verbs:
      - get
      - list
      - create
      - update
      - delete
---
# Source: dataplane/templates/operator/serviceaccount-proxy.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: proxy-system
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - '*'
    resources:
      - events
      - flyteworkflows
      - pods/log
      - pods
      - rayjobs
      - resourcequotas
    verbs:
      - get
      - list
      - watch
---
# Source: dataplane/templates/operator/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: operator-system
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - '*'
    resources:
      - secrets
      - deployments
    verbs:
      - get
      - list
      - watch
      - create
      - update
  - apiGroups:
      - flyte.lyft.com
    resources:
      - flyteworkflows
      - flyteworkflows/finalizers
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - patch
      - post
      - deletecollection
  - apiGroups:
      - '*'
    resources:
      - resourcequotas
      - pods
      - configmaps
      - podtemplates
      - secrets
      - namespaces
      - nodes
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
---
# Source: dataplane/templates/propeller/serviceaccount-webhook.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: flytepropeller-webhook-role
  namespace: union
rules:
  - apiGroups:
      - "*"
    resources:
      - mutatingwebhookconfigurations
      - secrets
      - pods
      - replicasets/finalizers
    verbs:
      - get
      - create
      - update
      - patch
---
# Source: dataplane/templates/propeller/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: flytepropeller-role
rules:
  # Allow RO access to PODS
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
  # Allow Event recording access
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - update
      - delete
      - patch
  # Allow Access All plugin objects
  - apiGroups:
      - '*'
    resources:
      - '*'
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - patch
  # Allow Access to CRD
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
      - create
      - delete
      - update
  # Allow Access to all resources under flyte.lyft.com
  - apiGroups:
      - flyte.lyft.com
    resources:
      - flyteworkflows
      - flyteworkflows/finalizers
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - patch
      - post
      - deletecollection
---
# Source: dataplane/templates/operator/serviceaccount-proxy-secret.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: proxy-system-secret
  namespace: union
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: proxy-system-secret
subjects:
  - kind: ServiceAccount
    name: proxy-system
    namespace: union
---
# Source: dataplane/templates/operator/serviceaccount-proxy.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: proxy-system
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: proxy-system
subjects:
  - kind: ServiceAccount
    name: proxy-system
    namespace: union
---
# Source: dataplane/templates/operator/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: operator-system
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: operator-system
subjects:
  - kind: ServiceAccount
    name: operator-system
    namespace: union
---
# Source: dataplane/templates/propeller/serviceaccount-webhook.yaml
# Create a binding from Role -> ServiceAccount
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: flytepropeller-webhook-binding
  namespace: union
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: flytepropeller-webhook-role
subjects:
  - kind: ServiceAccount
    name: flytepropeller-webhook-system
    namespace: union
---
# Source: dataplane/templates/propeller/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: flytepropeller-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: flytepropeller-role
subjects:
  - kind: ServiceAccount
    name: flytepropeller-system
    namespace: union
---
# Source: dataplane/templates/nodeexecutor/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-dataplane-executor
  labels:
    app: executor
spec:
  type: ClusterIP
  ports:
    - port: 15605
      targetPort: 15605
      protocol: TCP
      name: fasttask
  selector:
    app: executor
---
# Source: dataplane/templates/operator/service-proxy.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator-proxy
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
    - port: 10254
      targetPort: debug
      protocol: TCP
      name: debug
  selector:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
---
# Source: dataplane/templates/operator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: union-operator
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: debug
      protocol: TCP
      name: debug
  selector:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
---
# Source: dataplane/templates/propeller/service-webhook.yaml
apiVersion: v1
kind: Service
metadata:
  name: flyte-pod-webhook
  namespace: union
  labels: 
    app.kubernetes.io/name: flyte-pod-webhook
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
  annotations: 
    projectcontour.io/upstream-protocol.h2c: grpc
spec:
  selector: 
    app.kubernetes.io/name: flyte-pod-webhook
    app.kubernetes.io/instance: release-name
  ports:
    - name: https
      protocol: TCP
      port: 443
      targetPort: 9443
    - name: debug
      protocol: TCP
      port: 10254
      targetPort: 10254
---
# Source: dataplane/templates/propeller/service.yaml
apiVersion: v1
kind: Service
metadata:
  namespace: union
  name: flytepropeller
  labels: 
    app.kubernetes.io/name: flytepropeller
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: debug
      protocol: TCP
      port: 10254
    - name: fasttask
      port: 15605
      protocol: TCP
      targetPort: 15605
  selector: 
    app.kubernetes.io/name: flytepropeller
    app.kubernetes.io/instance: release-name
---
# Source: dataplane/templates/nodeexecutor/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: executor
  namespace: union
  labels:
    app: executor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: executor
  template:
    metadata:
      labels:
        app: executor
    spec:
      securityContext:
        fsGroup: 1337
      serviceAccountName: flytepropeller-system # TODO: update `executor` ServiceAccount to have required permissions
      volumes:
        - name: config-volume
          configMap:
            name: executor
        - name: secret-volume
          secret:
            secretName: union-secret-auth
        - name: auth
          secret:
            secretName: union-secret-auth
      containers:
        - name: executor
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2025.11.1"
          imagePullPolicy: IfNotPresent
          command:
            - executor
            - serve
            - --config
            - /etc/config/*.yaml
          ports:
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: metrics
              containerPort: 10254
              protocol: TCP
          env:
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.memory
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  divisor: "1"
                  resource: limits.cpu
          resources:
            limits:
              cpu:    "4"
              memory: "8Gi"
            requests:
              cpu:    "1"
              memory: "1Gi"
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: secret-volume
              mountPath: /etc/union/secret
            - name: auth
              mountPath: /etc/secrets/
---
# Source: dataplane/templates/operator/deployment-proxy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: union-operator-proxy
  namespace: union
  labels:
    app.kubernetes.io/name: operator-proxy
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: operator-proxy
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        configChecksum: "250779afed88ceeb263fd1c2f11b55fff895d8b8e0df00ba32995fed6af679a"
        
      labels:
        app.kubernetes.io/name: operator-proxy
        app.kubernetes.io/instance: release-name
        platform.union.ai/service-group: release-name
        app.kubernetes.io/managed-by: Helm
    spec:
      volumes:
        - name: config-volume
          projected:
            sources:
            - configMap:
                name: union-operator
        - name: secret-volume
          secret:
            secretName: union-secret-auth
      priorityClassName: 
      serviceAccountName: proxy-system
      securityContext:
        {}
      containers:
        - name: operator-proxy
          securityContext:
            {}
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2025.11.1"
          imagePullPolicy: IfNotPresent
          terminationMessagePolicy: FallbackToLogsOnError
          resources:
            limits:
              cpu: "3"
              memory: 3Gi
            requests:
              cpu: 500m
              memory: 500Mi
          volumeMounts:
            - mountPath: /etc/union/config
              name: config-volume
            - mountPath: /etc/union/secret
              name: secret-volume
          args:
            - operator
            - proxy
            - --config
            - /etc/union/config/*.yaml
          ports:
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: connect
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 8081
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.cpu
          - name: CLUSTER_NAME
            valueFrom:
              secretKeyRef:
                name: operator-cluster-name
                key: cluster_name
          - name: DEPLOYMENT_NAME
            value: operator
          - name: PROXY_SERVICE_URL
            value: http://union-operator-proxy:8080
          - name: PROMETHEUS_SERVICE_URL
            value: http://union-operator-prometheus:80
          - name: KNATIVE_PROXY_SERVICE_URL
            value: http://kourier-internal
        - name: "tunnel"
          securityContext:
            {}
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2025.11.1"
          imagePullPolicy: IfNotPresent
          args:
            - cloudflared
            - tunnel
            - --no-autoupdate
            - run
            - --token
            - $(TUNNEL_TOKEN)
          env:
            - name: TUNNEL_TOKEN
              valueFrom:
                secretKeyRef:
                  name: union-secret-auth
                  key: tunnel_token
                  optional: true
---
# Source: dataplane/templates/operator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: union-operator
  labels:
    app.kubernetes.io/name: union-operator
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: union-operator
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        configChecksum: "250779afed88ceeb263fd1c2f11b55fff895d8b8e0df00ba32995fed6af679a"
        
      labels:
        
        app.kubernetes.io/name: union-operator
        app.kubernetes.io/instance: release-name
        platform.union.ai/service-group: release-name
        app.kubernetes.io/managed-by: Helm
    spec:
      priorityClassName: 
      serviceAccountName: operator-system
      securityContext:
        {}
      volumes:
        - name: config-volume
          configMap:
            name: union-operator
        - name: secret-volume
          secret:
            secretName: union-secret-auth
      containers:
        - name: operator
          securityContext:
            {}
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2025.11.1"
          imagePullPolicy: IfNotPresent
          terminationMessagePolicy: FallbackToLogsOnError
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: "1"
              memory: 1Gi
          volumeMounts:
            - mountPath: /etc/union/config
              name: config-volume
            - mountPath: /etc/union/secret
              name: secret-volume
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.cpu
          - name: CLUSTER_NAME
            valueFrom:
              secretKeyRef:
                name: operator-cluster-name
                key: cluster_name
          - name: DEPLOYMENT_NAME
            value: operator
          - name: PROXY_SERVICE_URL
            value: http://union-operator-proxy:8080
          - name: PROMETHEUS_SERVICE_URL
            value: http://union-operator-prometheus:80
          - name: KNATIVE_PROXY_SERVICE_URL
            value: http://kourier-internal
          args:
            - operator
            - serve
            - --config
            - /etc/union/config/*.yaml
            - --operator.clusterId.name
            - "$(CLUSTER_NAME)"
            - --operator.tunnel.k8sSecretName
            - union-secret-auth
          ports:
            - name: grpc
              containerPort: 8080
              protocol: TCP
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
---
# Source: dataplane/templates/propeller/deployment-webhook.yaml
# Create the actual deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flytepropeller-webhook
  namespace: union
  labels:
    app.kubernetes.io/name: flyte-pod-webhook
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flyte-pod-webhook
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        
        app.kubernetes.io/name: flyte-pod-webhook
        app.kubernetes.io/instance: release-name
        platform.union.ai/service-group: release-name
        app.kubernetes.io/managed-by: Helm
      annotations:
        configChecksum: "199dbf88c79f06c7fc85d6642b930ea58c2f3aa3d2e4d5238ed217989bef2a5"
        
    spec:
      securityContext: 
        fsGroup: 65534
        fsGroupChangePolicy: Always
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions:
          type: spc_t
      serviceAccountName: flytepropeller-webhook-system
      initContainers:
        - name: generate-secrets
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2025.11.1"
          imagePullPolicy: "IfNotPresent"
          command:
            - flytepropeller
          args:
            - webhook
            - init-certs
            - --config
            - /etc/flyte/config/*.yaml
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.cpu
          - name: CLUSTER_NAME
            valueFrom:
              secretKeyRef:
                name: operator-cluster-name
                key: cluster_name
          - name: DEPLOYMENT_NAME
            value: operator
          - name: PROXY_SERVICE_URL
            value: http://union-operator-proxy:8080
          - name: PROMETHEUS_SERVICE_URL
            value: http://union-operator-prometheus:80
          - name: KNATIVE_PROXY_SERVICE_URL
            value: http://kourier-internal
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
          volumeMounts:
            - name: config-volume
              mountPath: /etc/flyte/config
      containers:
        - name: webhook
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2025.11.1"
          imagePullPolicy: "IfNotPresent"
          command:
            - flytepropeller
          args:
            - webhook
            - --config
            - /etc/flyte/config/*.yaml
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.cpu
          - name: CLUSTER_NAME
            valueFrom:
              secretKeyRef:
                name: operator-cluster-name
                key: cluster_name
          - name: DEPLOYMENT_NAME
            value: operator
          - name: PROXY_SERVICE_URL
            value: http://union-operator-proxy:8080
          - name: PROMETHEUS_SERVICE_URL
            value: http://union-operator-prometheus:80
          - name: KNATIVE_PROXY_SERVICE_URL
            value: http://kourier-internal
          ports:
            - containerPort: 9443
            - containerPort: 10254
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
          resources:
            requests:
              cpu: 200m
              ephemeral-storage: 500Mi
              memory: 500Mi
          volumeMounts:
            - name: config-volume
              mountPath: /etc/flyte/config
              readOnly: true
            - name: webhook-certs
              mountPath: /etc/webhook/certs
              readOnly: true
      volumes:
        - name: config-volume
          configMap:
            name: flyte-propeller-config
        - name: webhook-certs
          secret:
            secretName: flyte-pod-webhook
---
# Source: dataplane/templates/propeller/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: union
  name: flytepropeller
  labels:
    app.kubernetes.io/name: flytepropeller
    app.kubernetes.io/instance: release-name
    platform.union.ai/service-group: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flytepropeller
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        configChecksum: "199dbf88c79f06c7fc85d6642b930ea58c2f3aa3d2e4d5238ed217989bef2a5"
        
      labels:
        
        
        app.kubernetes.io/name: flytepropeller
        app.kubernetes.io/instance: release-name
        platform.union.ai/service-group: release-name
        app.kubernetes.io/managed-by: Helm
    spec:
      priorityClassName: system-cluster-critical
      containers:
        - command:
            - flytepropeller
            - --config
            - /etc/flyte/config/*.yaml
            - --propeller.cluster-id
            - my-cluster
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.memory
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                divisor: 1
                resource: limits.cpu
          - name: CLUSTER_NAME
            valueFrom:
              secretKeyRef:
                name: operator-cluster-name
                key: cluster_name
          - name: DEPLOYMENT_NAME
            value: operator
          - name: PROXY_SERVICE_URL
            value: http://union-operator-proxy:8080
          - name: PROMETHEUS_SERVICE_URL
            value: http://union-operator-prometheus:80
          - name: KNATIVE_PROXY_SERVICE_URL
            value: http://kourier-internal
          image: "public.ecr.aws/p0i0a9q8/unionoperator:2025.11.1"
          imagePullPolicy: "IfNotPresent"
          name: flytepropeller
          ports:
            - containerPort: 10254
          resources:
            limits:
              cpu: "3"
              memory: 3Gi
            requests:
              cpu: "1"
              memory: 1Gi
          volumeMounts:
            - name: config-volume
              mountPath: /etc/flyte/config
            - name: auth
              mountPath: /etc/union/secret
      serviceAccountName: flytepropeller-system
      volumes:
        - configMap:
            name: flyte-propeller-config
          name: config-volume
        - name: auth
          secret:
            secretName: union-secret-auth
---
# Source: dataplane/templates/monitoring/prometheusrule.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: union-opencost-rules
  namespace: union
  labels:
    release: release-name
spec:
  groups:
    - name: cost_calculations_15s
      interval: 15s
      rules:
        - record: pod_gpu_allocation
          expr: |
            sum by (namespace, pod) (DCGM_FI_DEV_GPU_UTIL >= bool 0) * on (namespace, pod) group_left() (max by (namespace, pod) (kube_pod_status_phase{phase=~"Running|Pending"} == 1))
        - record: execution_info # A join metric to look up execution-level info. Used to disambiguate workflow/task executions, apps, and workspaces.
          expr: |
            max by (label_domain, label_project, label_entity_name, label_execution_id, label_entity_id)(
              label_replace(
                label_replace(
                  label_replace(
                    label_replace(
                      label_replace(
                        flyte:propeller:all:round:execution_info{domain!="", project!="", workflow_name!="", execution_id!=""}, # filter for workflow/task executions
                        "label_entity_id", "$1", "execution_id", "(.*)" # join key
                      ), "label_entity_name", "$1", "workflow_name", "(.*)" # set label_entity_name to the workflow/task name from the workflow_execution_id
                    ),
                    "label_execution_id", "$1", "execution_id", "(.*)"
                  ),
                  "label_project", "$1", "project", "(.*)" # project
                ),
                "label_domain", "$1", "domain", "(.*)" # domain
              )
            )
        - record: app_info # A join metric to look up app-level info. Used to disambiguate workflow/task executions, apps, and workspaces.
          expr: |
            max by (label_domain, label_project, label_app_name, label_app_version, label_entity_id)(
              label_replace(
                label_replace(
                  label_replace(
                    kube_pod_labels{
                      label_domain!="",
                      label_project!="",
                      label_serving_unionai_dev_app_name!="",
                      label_serving_knative_dev_revision!=""
                    }, # this filters for apps
                    "label_app_name", "$1", "label_serving_unionai_dev_app_name", "(.*)" # rename to cleanup
                  ),
                  "label_app_version", "$1", "label_serving_knative_dev_revision", "(.*)" # the app_version is equivalent to an execution_id for workflows (lowest level of granularity)
                ),
                "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # join key
              )
            )
        - record: workspace_info # A join metric to look up workspace info. Used to disambiguate workflow/task executions, apps, and workspaces.
          expr: |
            max by (label_domain, label_project, label_workspace_name, label_entity_id)(
              label_replace(
                label_replace(
                  kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # filter for workspaces
                  "label_entity_id", "$1", "label_node_id", "(.*)" # join key
                ), "label_workspace_name", "$1", "label_node_id", "(.*)" # set label_workspace_name to the workspace name from the kube_pod_labels
              )
            )
        - record: entity_id:mem_usage_bytes_total_per_node:sum # Allocated memory (max(requested, consumed)) aggregated per node and entity, where entity is either a task/workflow execution or an app.
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, node) ( # aggregate up to entity
              # First, calculate the allocated memory for each pod
              max by (namespace, pod) ( # this is the case where consumed (the memory working set) exceeds requested memory
                (
                  sum by (namespace, pod) (
                    container_memory_working_set_bytes{namespace!="",pod!="",image!=""}
                  )
                  > sum by (namespace, pod) (
                    kube_pod_container_resource_requests{namespace!="", pod!="", node!="", resource="memory"}
                  )
                )
                or sum by (namespace, pod) ( # this is the case where memory requests are <= consumed memory
                  kube_pod_container_resource_requests{namespace!="", pod!="", node!="", resource="memory"} # needed to add node!="" to dedupe
                )
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but we do not want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
              # Now join in node identifiers which are used for subsequent overhead calculations
              * on (namespace, pod) group_left(node) (
                max by (namespace, pod, node) (kube_pod_info{node!=""}) # needed to add node!="" to dedupe
              )
            )
        - record: entity_id:cpu_usage_per_node:sum # Allocated cpu (max(requested, consumed)) aggregated per node and entity, where entity is either a task/workflow execution or an app.
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, node) (
              # First, calculate the allocated cpu for each pod
              max by (namespace, pod) ( # this is the case where consumed (the cpu usage seconds total) exceeds requested cpu
                (
                  sum by (namespace, pod) (
                    irate(container_cpu_usage_seconds_total{namespace!="",pod!="",image!=""}[5m])
                  )
                  > sum by (namespace, pod) (
                    kube_pod_container_resource_requests{namespace!="", pod!="", node!="", resource="cpu"}
                  )
                )
                or sum by (namespace, pod) ( # this is the case where cpu requests are <= consumed cpu
                    kube_pod_container_resource_requests{namespace!="", pod!="", node!="", resource="cpu"}
                )
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but I didn't want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
              # Now join in node identifiers which are used for subsequent overhead calculations
              * on (namespace, pod) group_left(node) (
                max by (namespace, pod, node) (kube_pod_info{node!=""}) # needed to add node!="" to dedupe
              )
            )
        - record: entity_id:gpu_usage_per_node:sum # Allocated gpu aggregated per node and entity, where entity is either a task/workflow execution or an app.
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, node) (
              # First, grab the allocated gpu for each pod (which is always either 1 or zero, since k8s can't split gpus the way it can with cpu/memory)
              max by (namespace, pod) (
                pod_gpu_allocation
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but I didn't want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
              # Now join in node identifiers which are used for subsequent overhead calculations
              * on (namespace, pod) group_left(node) (
                max by (namespace, pod, node) (kube_pod_info{node!=""}) # needed to add node!="" to dedupe
              )
            )
        - record: entity_id:used_mem_bytes:sum # the sum of used memory across all containers in an entity (numerator for aggregate utilization calculations)
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id) ( # aggregate up to entity
              # First, calculate the used memory for each pod
              sum by (namespace, pod) (
                container_memory_working_set_bytes{namespace!="",pod!="",image!=""}
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but we do not want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
            )
        - record: entity_id:allocated_mem_bytes:sum # the sum of allocated memory across all containers in an entity (denominator for aggregate utilization calculations)
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id) ( # aggregate up to entity (remove node)
              entity_id:mem_usage_bytes_total_per_node:sum
            )
        - record: entity_id:used_cpu:sum # the sum of used cpu across all containers in an entity (numerator for aggregate utilization calculations)
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id) (
              sum by (namespace, pod) (
                irate(container_cpu_usage_seconds_total{namespace!="",pod!="",image!=""}[5m])
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but I didn't want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
            )
        - record: entity_id:allocated_cpu:sum # the sum of allocated cpu across all containers in an entity (denominator for aggregate utilization calculations)
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id) ( # aggregate up to entity (remove node)
              entity_id:cpu_usage_per_node:sum
            )
        - record: entity_id:sm_occupancy:avg # the simple average of SM occupancy (a good generic measure of GPU utilization) per entity
          expr: |
            avg by (label_entity_type, label_domain, label_project, label_entity_id) (
              # First, grab the SM occupancy for each pod
              max by (namespace, pod) (
                DCGM_FI_PROF_SM_OCCUPANCY # SM occupancy is a good proxy for actual GPU usage
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but I didn't want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
            )
        - record: entity_id:gpu_count:sum # the count of running gpu pods per entity (need this to weight the gpu utilization when aggregating upwards - i.e. project-level)
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, node) (
              # First, grab the allocated gpu for each pod (which is always either 1 or zero, since k8s can't split gpus the way it can with cpu/memory)
              max by (namespace, pod) (
                pod_gpu_allocation
              )
              # Next, add labels to each pod that contain the relevant entity information (i.e. workflow/task or app). Note that this is repetitive but I didn't want to double the number of pod-level metrics we save
              * on (namespace, pod) group_left(label_entity_type, label_domain, label_project, label_entity_id) (
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workflow/task labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_workflow_name!="", label_execution_id!="", label_workspace=""}, # this filters for workflow and task executions only (no apps)
                          "label_entity_type", "workflow", "", "" # set label_entity_type to "workflow" (note that both workflow and single task executions will say "workflow")
                        ),
                        "label_entity_id", "$1", "label_execution_id", "(.*)" # set label_entity_id to the execution id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds app labels
                  label_replace(
                    label_replace(
                      kube_pod_labels{
                        label_domain!="",
                        label_project!="",
                        label_serving_unionai_dev_app_name!="",
                        label_serving_knative_dev_revision!=""
                        }, # this filters for apps only
                      "label_entity_type", "app", "", "" # set label_entity_type to "app"
                    ),
                    "label_entity_id", "$1", "label_serving_knative_dev_revision", "(.*)" # set label_entity_id to the app version (so we have label_entity_id with both execution ids and app versions)
                  )
                )
                or
                max by (label_entity_type, label_domain, label_project, label_entity_id, namespace, pod)( # adds workspace labels
                  label_replace(
                    label_replace(
                      label_replace(
                        label_replace(
                          kube_pod_labels{label_domain!="", label_project!="", label_node_id!="", label_workspace="true"}, # this filters for workspace executions only (no tasks, workflows, or apps)
                          "label_entity_type", "workspace", "", "" # set label_entity_type to "workspace"
                        ),
                        "label_entity_id", "$1", "label_node_id", "(.*)" # set label_entity_id to the label_node_id (join key)
                      ),
                      "label_domain", "$1", "label_domain", "(.*)"
                    ),
                    "label_project", "$1", "label_project", "(.*)"
                  )
                )
              )
              # Then filter for pods only in the "Running" or "Pending" phase
              * on (namespace, pod) group_left() (
                max by (namespace, pod) (
                  kube_pod_status_phase{phase=~"Running|Pending"} == 1
                )
              )
            )
        - record: entity_id:weighted_sm_occupancy:sum # product of SM occupancy and allocated GPU count (something like "used memory", numerator of weighted calcs)
          expr: |
            entity_id:sm_occupancy:avg
            * on (label_domain, label_project, label_entity_type, label_entity_id) entity_id:gpu_count:sum
        - record: entity_id:allocated_mem_cost:sum # Allocated cost of memory for each workflow/task execution and app.
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, type) (
              entity_id:mem_usage_bytes_total_per_node:sum / (1024 * 1024 * 1024) # convert bytes to GB
              * on (node) group_left(type) label_replace(avg by (node) (node_ram_hourly_cost * (15 / 3600)), "type", "mem", "", "") # convert hourly cost to 15-secondly cost and add type
            )
        - record: entity_id:allocated_cpu_cost:sum # Allocated cost of cpu for each workflow/task execution and app.
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, type)(
              entity_id:cpu_usage_per_node:sum
              * on (node) group_left(type) label_replace(avg by (node) (node_cpu_hourly_cost * (15 / 3600)), "type", "cpu", "", "") # convert hourly cost to 15-secondly cost and add type
            )
        - record: entity_id:allocated_gpu_cost:sum # Allocated cost of gpu for each workflow/task execution and app.
          expr: |
            sum by (label_entity_type, label_domain, label_project, label_entity_id, type)(
              entity_id:gpu_usage_per_node:sum
              * on (node) group_left(type) label_replace(avg by (node) (node_gpu_hourly_cost * (15 / 3600)), "type", "gpu", "", "") # convert hourly cost to 15-secondly cost and add type
            )
        - record: entity_id:allocated_cost:sum # Allocated cost of memory, cpu, and gpu for each workflow/task execution and app.
          expr: |
            label_replace(
              sum by (label_entity_type, label_domain, label_project, label_entity_id) ( # for the sum to work, the labels need to be different on each "or" element (type label)
                entity_id:allocated_mem_cost:sum
                or
                entity_id:allocated_cpu_cost:sum
                or
                entity_id:allocated_gpu_cost:sum
              ),
              "type", "allocated", "", "" # add type info
            )
        - record: entity_id:overhead_cost:sum # The amount of overhead costs (node costs that we can't allocate with container resources) to allocate to each entity (workflow/task execution or app)
          expr: |
            label_replace(
              sum by (label_entity_type, label_entity_id, label_domain, label_project)( # Aggregate the per-node metrics up to workflow/task execution or app (label_entity_id)
                # Start with each execution's and app's allocated cost per node
                sum by (label_entity_type, label_domain, label_project, label_entity_id, node) ( # for the sum to work, the labels need to be different on each "or" element (type label)
                  entity_id:mem_usage_bytes_total_per_node:sum / (1024 * 1024 * 1024) # convert bytes to GB
                  * on (node) group_left(type) label_replace(avg by (node) (node_ram_hourly_cost * (15 / 3600)), "type", "mem", "", "") # convert hourly cost to 15-secondly cost and add type
                  or
                  entity_id:cpu_usage_per_node:sum
                  * on (node) group_left(type) label_replace(avg by (node) (node_cpu_hourly_cost * (15 / 3600)), "type", "cpu", "", "") # convert hourly cost to 15-secondly cost and add type
                  or
                  entity_id:gpu_usage_per_node:sum
                  * on (node) group_left(type) label_replace(avg by (node) (node_gpu_hourly_cost * (15 / 3600)), "type", "gpu", "", "") # convert hourly cost to 15-secondly cost and add type
                )
                # Then divide out the total allocated cost per node to get the proportion of allocated cost associated with each entity
                / on (node) group_left()(
                  sum by (node) ( # for the sum to work, the labels need to be different on each "or" element (type label)
                    entity_id:mem_usage_bytes_total_per_node:sum / (1024 * 1024 * 1024) # convert bytes to GB
                    * on (node) group_left(type) label_replace(avg by (node) (node_ram_hourly_cost * (15 / 3600)), "type", "mem", "", "") # convert hourly cost to 15-secondly cost and add type
                    or
                    entity_id:cpu_usage_per_node:sum
                    * on (node) group_left(type) label_replace(avg by (node) (node_cpu_hourly_cost * (15 / 3600)), "type", "cpu", "", "") # convert hourly cost to 15-secondly cost and add type
                    or
                    entity_id:gpu_usage_per_node:sum
                    * on (node) group_left(type) label_replace(avg by (node) (node_gpu_hourly_cost * (15 / 3600)), "type", "gpu", "", "") # convert hourly cost to 15-secondly cost and add type
                  )
                  > 0 # need to avoid dividing by zero, or gaps in the data can cause NaNs to proliferate, borking all charts
                )
                # Then multiply by the overhead cost per node
                * on (node) group_left() (
                  # To calculate overhead, start with the true cost of running each node
                  avg by (node)(kube_node_labels{label_flyte_org_node_role="worker"}) # only look at worker nodes
                  * on (node) max by (node) (
                    node_total_hourly_cost{instance_type!=""} # sometimes, the instance_type can be null, causing an unlabeled label to show up in the Compute Costs dashboard charts
                  ) * (15 / 3600) # convert hourly cost to 15-secondly cost
                  # Then subtract out the total allocated cost on each node
                  - on (node) group_left()(
                    sum by (node) ( # for the sum to work, the labels need to be different on each "or" element (type label)
                      entity_id:mem_usage_bytes_total_per_node:sum / (1024 * 1024 * 1024) # convert bytes to GB
                      * on (node) group_left(type) label_replace(avg by (node) (node_ram_hourly_cost * (15 / 3600)), "type", "mem", "", "") # convert hourly cost to 15-secondly cost and add type
                      or
                      entity_id:cpu_usage_per_node:sum
                      * on (node) group_left(type) label_replace(avg by (node) (node_cpu_hourly_cost * (15 / 3600)), "type", "cpu", "", "") # convert hourly cost to 15-secondly cost and add type
                      or
                      entity_id:gpu_usage_per_node:sum
                      * on (node) group_left(type) label_replace(avg by (node) (node_gpu_hourly_cost * (15 / 3600)), "type", "gpu", "", "") # convert hourly cost to 15-secondly cost and add type
                    )
                  )
                )
              ),
              "type", "overhead", "", "" # add type info
            )
        - record: entity_id:total_cost:sum # Total cost of each entity (workflow/task execution or app), including allocated (from container resources) and overhead (proportion of unallocated node costs)
          expr: |
            label_replace(
              sum by (label_domain, label_project, label_entity_id, label_entity_type) (
                entity_id:allocated_cost:sum
                or
                entity_id:overhead_cost:sum
              ),
              "type", "total", "", "" # add type info
            )
        - record: node:total_cost:sum # Total cost of all nodes
          expr: |
            sum (
              avg by (node)(kube_node_labels{label_flyte_org_node_role="worker", label_node_kubernetes_io_instance_type!=""}) # only look at worker nodes
              * on (node) group_left() node_total_hourly_cost{instance_type!=""} * (15 / 3600) # convert hourly cost to 15-secondly cost
            )
        - record: node_type:total_cost:sum # Total cost of nodes grouped by node type
          expr: |
            sum by (node_type)(
              avg by (node)(kube_node_labels{label_flyte_org_node_role="worker", label_node_kubernetes_io_instance_type!=""}) # only look at worker nodes
              * on (node) group_left(node_type) label_replace(node_total_hourly_cost{instance_type!=""}, "node_type", "$1", "instance_type", "(.*)") * (15 / 3600) # convert hourly cost to 15-secondly cost and rename label
            )
        - record: node_type:uptime_hours:sum # Total uptime of nodes grouped by node type
          expr: |
            sum by (node_type)(
              avg by (node, node_type)( # dedupe
                label_replace(kube_node_labels{label_flyte_org_node_role="worker", label_node_kubernetes_io_instance_type!=""}, "node_type", "$1", "label_node_kubernetes_io_instance_type", "(.*)") # relabel
              )
            ) * (15 / 3600) # convert to number of hours per 15-second observation      # Aggregate the above into visible metrics
    - name: cost_rollup_15m
      interval: 15m
      rules:
        - record: execution_info15m
          expr: |
            max_over_time(execution_info[15m:15s])
        - record: app_info15m
          expr: |
            max_over_time(app_info[15m:15s])
        - record: workspace_info15m
          expr: |
            max_over_time(workspace_info[15m:15s])
        - record: entity_id:allocated_mem_bytes:sum15m
          expr: |
            sum_over_time(entity_id:allocated_mem_bytes:sum[15m:15s])
        - record: entity_id:used_mem_bytes:sum15m
          expr: |
            sum_over_time(entity_id:used_mem_bytes:sum[15m:15s])
        - record: entity_id:allocated_cpu:sum15m
          expr: |
            sum_over_time(entity_id:allocated_cpu:sum[15m:15s])
        - record: entity_id:used_cpu:sum15m
          expr: |
            sum_over_time(entity_id:used_cpu:sum[15m:15s])
        - record: entity_id:weighted_sm_occupancy:sum15m
          expr: |
            sum_over_time(entity_id:weighted_sm_occupancy:sum[15m:15s])
        - record: entity_id:gpu_count:sum15m
          expr: |
            sum_over_time(entity_id:gpu_count:sum[15m:15s])
        - record: entity_id:allocated_mem_cost:sum15m
          expr: |
            sum_over_time(entity_id:allocated_mem_cost:sum[15m:15s])
        - record: entity_id:allocated_cpu_cost:sum15m
          expr: |
            sum_over_time(entity_id:allocated_cpu_cost:sum[15m:15s])
        - record: entity_id:allocated_gpu_cost:sum15m
          expr: |
            sum_over_time(entity_id:allocated_gpu_cost:sum[15m:15s])
        - record: entity_id:allocated_cost:sum15m
          expr: |
            sum_over_time(entity_id:allocated_cost:sum[15m:15s])
        - record: entity_id:overhead_cost:sum15m
          expr: |
            sum_over_time(entity_id:overhead_cost:sum[15m:15s])
        - record: entity_id:total_cost:sum15m
          expr: |
            sum_over_time(entity_id:total_cost:sum[15m:15s])
        - record: node:total_cost:sum15m
          expr: |
            sum_over_time(node:total_cost:sum[15m:15s])
        - record: node_type:total_cost:sum15m
          expr: |
            sum_over_time(node_type:total_cost:sum[15m:15s])
        - record: node_type:uptime_hours:sum15m
          expr: |
            sum_over_time(node_type:uptime_hours:sum[15m:15s])
---
# Source: dataplane/templates/monitoring/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: cost
  namespace: union
  labels:
    release: release-name
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: opencost
  namespaceSelector:
    matchNames:
      - "union"
  endpoints:
    - port: http
      interval: 1m
      path: /metrics
      honorLabels: true
      metricRelabelings:
        - sourceLabels: [ "__name__" ]
          separator: ";"
          regex: "kube_node_labels|kube_pod_labels|node_total_hourly_cost|node_ram_hourly_cost|node_cpu_hourly_cost|node_gpu_hourly_cost"
          action: keep
---
# Source: dataplane/templates/monitoring/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: union-service-monitor
  namespace: union
  labels:
    release: release-name
spec:
  selector:
    matchLabels:
      platform.union.ai/service-group: release-name
  namespaceSelector:
    matchNames:
      - "union"
  endpoints:
    - port: debug
      interval: 1m
      path: /metrics
      honorLabels: true
