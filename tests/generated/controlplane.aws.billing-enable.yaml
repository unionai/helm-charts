---
# Source: controlplane/templates/flyte-core-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: flyteadmin
  namespace: union
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flyteadmin
---
# Source: controlplane/templates/flyte-core-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: datacatalog
  namespace: union
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: datacatalog
---
# Source: controlplane/templates/flyte-core-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: cacheservice
  namespace: union
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: cacheservice
---
# Source: controlplane/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: artifacts
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: artifacts
      app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: authorizer
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: authorizer
      app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: cluster
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: cluster
      app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: dataproxy
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: dataproxy
      app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: executions
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: executions
      app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: usage
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: usage
      app.kubernetes.io/instance: release-name
---
# Source: controlplane/charts/flyte/templates/admin/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flyteadmin
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.0-b2
    #app.kubernetes.io/managed-by: Helm
  annotations: 
    eks.amazonaws.com/role-arn: arn:aws:iam::<account-id>:role/adminflyterole
---
# Source: controlplane/charts/flyte/templates/datacatalog/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: datacatalog
  namespace: union
  labels: 
    app.kubernetes.io/name: datacatalog
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.0-b2
    app.kubernetes.io/managed-by: Helm
---
# Source: controlplane/charts/flyte/templates/flytescheduler/sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flytescheduler
  namespace: union
  labels: 
    app.kubernetes.io/name: flytescheduler
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.0-b2
    app.kubernetes.io/managed-by: Helm
---
# Source: controlplane/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: artifacts
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: artifacts
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
---
# Source: controlplane/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: authorizer
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: authorizer
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
---
# Source: controlplane/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cluster
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: cluster
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
---
# Source: controlplane/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: dataproxy
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: dataproxy
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
---
# Source: controlplane/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: executions
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: executions
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
---
# Source: controlplane/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: usage
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: usage
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
---
# Source: controlplane/charts/flyte/templates/admin/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: flyte-admin-secrets
  namespace: union
type: Opaque
stringData:
---
# Source: controlplane/charts/flyte/templates/common/secret-auth.yaml
apiVersion: v1
kind: Secret
metadata:
  name: flyte-secret-auth
  namespace: union
type: Opaque
stringData:
  client_secret: foobar
---
# Source: controlplane/charts/flyte/templates/common/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: db-pass
stringData:
  pass.txt: ''
type: Opaque
---
# Source: controlplane/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: artifacts
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: artifacts
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
type: Opaque
stringData:
---
# Source: controlplane/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: authorizer
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: authorizer
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
type: Opaque
stringData:
---
# Source: controlplane/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: cluster
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: cluster
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
type: Opaque
stringData:
---
# Source: controlplane/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: dataproxy
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: dataproxy
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
type: Opaque
stringData:
---
# Source: controlplane/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: executions
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: executions
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
type: Opaque
stringData:
---
# Source: controlplane/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: usage
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: usage
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
type: Opaque
stringData:
---
# Source: controlplane/charts/flyte/templates/admin/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: flyte-admin-clusters-config
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.0-b2
    #app.kubernetes.io/managed-by: Helm
data:
  clusters.yaml: |
    clusters:
      clusterConfigs: []
      labelClusterMap: {}
---
# Source: controlplane/charts/flyte/templates/admin/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: flyte-admin-base-config
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.0-b2
    #app.kubernetes.io/managed-by: Helm
data:
  db.yaml: | 
    database:
      connMaxLifeTime: 120s
      dbname: postgres
      host: ''
      maxIdleConnections: 10
      maxOpenConnections: 80
      passwordPath: /etc/db/pass.txt
      port: 5432
      username: ''
  domain.yaml: | 
    domains:
    - id: development
      name: development
    - id: staging
      name: staging
    - id: production
      name: production
  server.yaml: | 
    admin:
      authorizer:
        internalCommunicationConfig:
          enabled: false
      cacheserviceServer:
        authorizer:
          internalCommunicationConfig:
            enabled: false
        otel:
          otlpgrpc:
            endpoint: http://otel-collector.monitoring.svc.cluster.local:4317
          sampler:
            parentSampler: traceid
            traceIdRatio: 0.001
          type: otlpgrpc
        private:
          app:
            cacheProviderConfig:
              kind: bypass
      cloudEvents:
        enable: true
        eventsPublisher:
          eventTypes:
          - workflow
          - node
        type: aws
      domain: {}
      flyteadmin:
        metricsKeys:
        - phase
        useOffloadedInputs: true
        useOffloadedWorkflowClosure: true
      otel:
        otlpgrpc:
          endpoint: http://otel-collector.monitoring.svc.cluster.local:4317
        sampler:
          parentSampler: traceid
          traceIdRatio: 0.001
        type: otlpgrpc
      private:
        app:
          cacheProviderConfig:
            kind: bypass
        metrics:
          disable: true
      server:
        grpc:
          enableGrpcLatencyMetrics: true
          maxMessageSizeBytes: 16777216
      union:
        internalConnectionConfig:
          enabled: true
          urlPattern: '_SERVICE_.union.svc.cluster.local:80'
    auth:
      appAuth:
        thirdPartyConfig:
          flyteClient:
            clientId: flytectl
            redirectUri: http://localhost:53593/callback
            scopes:
            - offline
            - all
      authorizedUris:
      - https://localhost:30081
      - http://flyteadmin:80
      - http://flyteadmin.flyte.svc.cluster.local:80
      userAuth:
        openId:
          baseUrl: https://accounts.google.com
          clientId: 657465813211-6eog7ek7li5k7i7fvgv2921075063hpe.apps.googleusercontent.com
          scopes:
          - profile
          - openid
    authorizer:
      internalCommunicationConfig:
        enabled: false
      type: Noop
    cloudEvents:
      enable: false
    flyteadmin:
      eventVersion: 2
      metadataStoragePrefix:
      - metadata
      - admin
      metricsScope: 'flyte:'
      profilerPort: 10254
      roleNameKey: iam.amazonaws.com/role
      testing:
        host: http://flyteadmin
    private:
      app:
        cacheProviderConfig:
          kind: bypass
    server:
      grpc:
        port: 8089
      httpPort: 8088
      security:
        allowCors: true
        allowedHeaders:
        - Content-Type
        - flyte-authorization
        allowedOrigins:
        - '*'
        secure: false
        useAuth: false
    union:
      internalConnectionConfig:
        enabled: true
        urlPattern: '_SERVICE_.union.svc.cluster.local:80'
  remoteData.yaml: | 
    remoteData:
      region: us-east-1
      scheme: local
      signedUrls:
        durationMinutes: 3
  storage.yaml: | 
    storage:
      type: s3
      container: ""
      connection:
        auth-type: iam
        region: 
      enable-multicontainer: false
      limits:
        maxDownloadMBs: 10
      cache:
        max_size_mbs: 1024
        target_gc_percent: 70
  task_resource_defaults.yaml: | 
    task_resources:
      defaults:
        cpu: 100m
        memory: 500Mi
      limits:
        cpu: 2
        gpu: 1
        memory: 1Gi
---
# Source: controlplane/charts/flyte/templates/console/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: flyte-console-config
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteconsole
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.0-b2
    app.kubernetes.io/managed-by: Helm
data: 
  BASE_URL: /console
  CONFIG_DIR: /etc/flyte/config
---
# Source: controlplane/charts/flyte/templates/datacatalog/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: datacatalog-config
  namespace: union
  labels: 
    app.kubernetes.io/name: datacatalog
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.0-b2
    app.kubernetes.io/managed-by: Helm
data:
  db.yaml: | 
    database:
      connMaxLifeTime: 120s
      dbname: datacatalog
      host: ''
      maxIdleConnections: 10
      maxOpenConnections: 20
      passwordPath: /etc/db/pass.txt
      port: 5432
      username: ''
  server.yaml: | 
    application:
      grpcMaxRecvMsgSizeMBs: 6
      grpcPort: 8089
      grpcServerReflection: true
      httpPort: 8080
    datacatalog:
      heartbeat-grace-period-multiplier: 3
      max-reservation-heartbeat: 30s
      metrics-scope: datacatalog
      profiler-port: 10254
      storage-prefix: metadata/datacatalog
  storage.yaml: | 
    storage:
      type: s3
      container: ""
      connection:
        auth-type: iam
        region: 
      enable-multicontainer: false
      limits:
        maxDownloadMBs: 10
      cache:
        max_size_mbs: 1024
        target_gc_percent: 70
---
# Source: controlplane/charts/flyte/templates/flytescheduler/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: flyte-scheduler-config
  namespace: union
  labels: 
    app.kubernetes.io/name: flytescheduler
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.0-b2
    app.kubernetes.io/managed-by: Helm
data:
  admin.yaml: | 
    admin:
      clientId: 'flytepropeller'
      clientSecretLocation: /etc/secrets/client_secret
      endpoint: flyteadmin:81
      insecure: true
    event:
      capacity: 1000
      rate: 500
      type: admin
  db.yaml: | 
    database:
      connMaxLifeTime: 120s
      dbname: postgres
      host: ''
      maxIdleConnections: 10
      maxOpenConnections: 80
      passwordPath: /etc/db/pass.txt
      port: 5432
      username: ''
  server.yaml: | 
    scheduler:
      metricsScope: 'flyte:'
      profilerPort: 10254
---
# Source: controlplane/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: artifacts
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: artifacts
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
data:
  config.yaml: |
    artifactsConfig:
      app:
        adminClient:
          hackFlagUntilCellIsolation: true
        artifactBlobStoreConfig:
          container: 'artifactsBucketName'
          stow:
            config:
              auth_type: iam
              region: us-east-2
            kind: s3
          type: stow
        artifactDatabaseConfig:
          connMaxLifeTime: 1m
          maxIdleConnections: 20
          maxOpenConnections: 30
          postgres:
            dbname: 'dbName'
            host: 'db-instance-url'
            options: sslmode=disable
            passwordPath: /etc/db/pass.txt
            port: 5432
            readReplicaHost: 'db-instance-url'
            username: 'dbUser'
        artifactServerConfig:
          httpPort: 8089
          port: 8080
          respectUserOrgsForServerless: true
        artifactTriggerConfig:
          executionMaxRetryCount: 3
          executionSchedulerQuerySize: 20
          executionSchedulers: 1
          executionSchedulersWait: 10
          triggerProcessorQuerySize: 100
          triggerProcessors: 1
          triggerProcessorsWait: 10
    authorizer:
      internalCommunicationConfig:
        enabled: false
      type: Noop
    cache:
      identity:
        enabled: false
    connection:
      environment: staging
      region: us-east-2
      rootTenantURLPattern: dns:///fake-host.domain
    db:
      connectionPool:
        maxConnectionLifetime: 1m
        maxIdleConnections: 20
        maxOpenConnections: 20
      host: 'db-instance-url'
      passwordPath: /etc/db/pass.txt
      username: 'dbUser'
    logger:
      level: 6
    otel:
      otlpgrpc:
        endpoint: http://otel-collector.monitoring.svc.cluster.local:4317
      sampler:
        parentSampler: traceid
        traceIdRatio: 0.001
      type: otlpgrpc
    sharedService:
      metrics:
        scope: 'artifacts:'
    union:
      auth:
        enable: false
      internalConnectionConfig:
        enabled: true
        urlPattern: _SERVICE_.union.svc.cluster.local:80
---
# Source: controlplane/templates/configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: authorizer
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: authorizer
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
data:
  config.yaml: |
    authorizer:
      internalCommunicationConfig:
        enabled: false
      type: Noop
    cache:
      identity:
        enabled: false
    connection:
      environment: staging
      region: us-east-2
      rootTenantURLPattern: dns:///fake-host.domain
    logger:
      level: 6
    otel:
      otlpgrpc:
        endpoint: http://otel-collector.monitoring.svc.cluster.local:4317
      sampler:
        parentSampler: traceid
        traceIdRatio: 0.001
      type: otlpgrpc
    sharedService:
      metrics:
        scope: 'authorizer:'
    union:
      auth:
        enable: false
      internalConnectionConfig:
        enabled: true
        urlPattern: _SERVICE_.union.svc.cluster.local:80
---
# Source: controlplane/templates/configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: cluster
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
data:
  config.yaml: |
    authorizer:
      internalCommunicationConfig:
        enabled: false
      type: Noop
    cache:
      identity:
        enabled: false
    cloudProvider:
      provider: Mock
    cluster:
      cloudflare:
        active: false
    connection:
      environment: staging
      region: us-east-2
      rootTenantURLPattern: dns:///fake-host.domain
    db:
      connectionPool:
        maxConnectionLifetime: 1m
        maxIdleConnections: 20
        maxOpenConnections: 20
      host: 'db-instance-url'
      passwordPath: /etc/db/pass.txt
      username: 'dbUser'
    logger:
      level: 6
    otel:
      otlpgrpc:
        endpoint: http://otel-collector.monitoring.svc.cluster.local:4317
      sampler:
        parentSampler: traceid
        traceIdRatio: 0.001
      type: otlpgrpc
    sharedService:
      metrics:
        scope: 'cluster:'
    union:
      auth:
        enable: false
      internalConnectionConfig:
        enabled: true
        urlPattern: _SERVICE_.union.svc.cluster.local:80
---
# Source: controlplane/templates/configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: dataproxy
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: dataproxy
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
data:
  config.yaml: |
    authorizer:
      internalCommunicationConfig:
        enabled: false
      type: Noop
    cache:
      identity:
        enabled: false
    connection:
      environment: staging
      region: us-east-2
      rootTenantURLPattern: dns:///fake-host.domain
    dataproxy:
      clusterSelector:
        type: local
      secureTunnelTenantURLPattern: http://ingress-nginx-internal.ingress-nginx.svc.cluster.local:80
    logger:
      level: 6
    otel:
      otlpgrpc:
        endpoint: http://otel-collector.monitoring.svc.cluster.local:4317
      sampler:
        parentSampler: traceid
        traceIdRatio: 0.001
      type: otlpgrpc
    sharedService:
      metrics:
        scope: 'usage:'
    union:
      auth:
        enable: false
      internalConnectionConfig:
        enabled: true
        urlPattern: _SERVICE_.union.svc.cluster.local:80
---
# Source: controlplane/templates/configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: executions
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: executions
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
data:
  config.yaml: |
    authorizer:
      internalCommunicationConfig:
        enabled: false
      type: Noop
    cache:
      identity:
        enabled: false
    cloudEventsProcessor:
      cloudProvider: Local
    connection:
      environment: staging
      region: us-east-2
      rootTenantURLPattern: dns:///fake-host.domain
    db:
      connectionPool:
        maxConnectionLifetime: 1m
        maxIdleConnections: 20
        maxOpenConnections: 20
      host: 'db-instance-url'
      passwordPath: /etc/db/pass.txt
      username: 'dbUser'
    executions:
      apps:
        enrichIdentities: false
        publicURLPattern: https://%s.apps.%s.cloud-staging.union.ai
    logger:
      level: 6
    otel:
      otlpgrpc:
        endpoint: http://otel-collector.monitoring.svc.cluster.local:4317
      sampler:
        parentSampler: traceid
        traceIdRatio: 0.001
      type: otlpgrpc
    sharedService:
      metrics:
        scope: 'executions:'
    union:
      auth:
        enable: false
      internalConnectionConfig:
        enabled: true
        urlPattern: _SERVICE_.union.svc.cluster.local:80
    workspace:
      enable: false
---
# Source: controlplane/templates/configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: usage
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: usage
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
data:
  config.yaml: |
    authorizer:
      internalCommunicationConfig:
        enabled: false
      type: Noop
    billing:
      enable: true
    cache:
      identity:
        enabled: false
    cloudProvider:
      provider: Mock
    connection:
      environment: staging
      region: us-east-2
      rootTenantURLPattern: dns:///fake-host.domain
    logger:
      level: 6
    otel:
      otlpgrpc:
        endpoint: http://otel-collector.monitoring.svc.cluster.local:4317
      sampler:
        parentSampler: traceid
        traceIdRatio: 0.001
      type: otlpgrpc
    sharedService:
      metrics:
        scope: 'usage:'
    union:
      auth:
        enable: false
      internalConnectionConfig:
        enabled: true
        urlPattern: _SERVICE_.union.svc.cluster.local:80
---
# Source: controlplane/charts/flyte/templates/admin/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: union-flyteadmin
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.0-b2
    #app.kubernetes.io/managed-by: Helm
rules:
- apiGroups: 
  - ""
  - flyte.lyft.com
  - rbac.authorization.k8s.io
  resources: 
  - configmaps
  - flyteworkflows
  - namespaces
  - pods
  - resourcequotas
  - roles
  - rolebindings
  - secrets
  - services
  - serviceaccounts
  - spark-role
  - limitranges
  verbs: 
  - '*'
---
# Source: controlplane/charts/flyte/templates/admin/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: union-flyteadmin-binding
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.0-b2
    #app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: union-flyteadmin
subjects:
- kind: ServiceAccount
  name: flyteadmin
  namespace: union
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: flyteadmin
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: controlplane-2025.5.6
    #app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - ""
      - flyte.lyft.com
      - rbac.authorization.k8s.io
    resources:
      - configmaps
      - flyteworkflows
      - namespaces
      - pods
      - resourcequotas
      - roles
      - rolebindings
      - secrets
      - services
      - serviceaccounts
      - spark-role
    verbs:
      - '*'
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: flyteadmin-binding
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: controlplane-2025.5.6
    #app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: flyteadmin
subjects:
  - kind: ServiceAccount
    name: flyteadmin
    namespace: union
---
# Source: controlplane/charts/flyte/templates/admin/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: flyteadmin
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.0-b2
    #app.kubernetes.io/managed-by: Helm
  annotations: 
    projectcontour.io/upstream-protocol.h2c: grpc
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8088
    - name: grpc
      port: 81
      protocol: TCP
      # intentionally set to TCP instead of grpc
      targetPort: 8089
    - name: redoc
      protocol: TCP
      port: 87
      targetPort: 8087
    - name: http-metrics
      protocol: TCP
      port: 10254
  selector: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/charts/flyte/templates/console/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: flyteconsole
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteconsole
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.0-b2
    app.kubernetes.io/managed-by: Helm
  annotations: 
    external-dns.alpha.kubernetes.io/hostname: flyte.example.com
    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "600"
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
  selector: 
    app.kubernetes.io/name: flyteconsole
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/charts/flyte/templates/datacatalog/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: datacatalog
  namespace: union
  labels: 
    app.kubernetes.io/name: datacatalog
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.0-b2
    app.kubernetes.io/managed-by: Helm
  annotations: 
    projectcontour.io/upstream-protocol.h2c: grpc
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 88
    protocol: TCP
    targetPort: 8088
  - name: grpc
    port: 89
    protocol: TCP
    targetPort: 8089
  selector: 
    app.kubernetes.io/name: datacatalog
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: artifacts
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: artifacts
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: grpc
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: connect
      port:  83
      protocol: TCP
      targetPort: 8081
    - name: http
      port: 81
      protocol: TCP
      targetPort: 8089
    - name: debug
      port: 82
      protocol: TCP
      targetPort: 10254
  selector:
    app.kubernetes.io/name: artifacts
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: authorizer
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: authorizer
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: grpc
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: connect
      port:  83
      protocol: TCP
      targetPort: 8081
    - name: http
      port: 81
      protocol: TCP
      targetPort: 8089
    - name: debug
      port: 82
      protocol: TCP
      targetPort: 10254
  selector:
    app.kubernetes.io/name: authorizer
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: cluster
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: cluster
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: grpc
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: connect
      port:  83
      protocol: TCP
      targetPort: 8081
    - name: http
      port: 81
      protocol: TCP
      targetPort: 8089
    - name: debug
      port: 82
      protocol: TCP
      targetPort: 10254
  selector:
    app.kubernetes.io/name: cluster
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: dataproxy
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: dataproxy
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: grpc
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: connect
      port:  83
      protocol: TCP
      targetPort: 8081
    - name: http
      port: 81
      protocol: TCP
      targetPort: 8089
    - name: debug
      port: 82
      protocol: TCP
      targetPort: 10254
  selector:
    app.kubernetes.io/name: dataproxy
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: executions
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: executions
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: grpc
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: connect
      port:  83
      protocol: TCP
      targetPort: 8081
    - name: http
      port: 81
      protocol: TCP
      targetPort: 8089
    - name: debug
      port: 82
      protocol: TCP
      targetPort: 10254
  selector:
    app.kubernetes.io/name: executions
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: usage
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: usage
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: grpc
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: connect
      port:  83
      protocol: TCP
      targetPort: 8081
    - name: http
      port: 81
      protocol: TCP
      targetPort: 8089
    - name: debug
      port: 82
      protocol: TCP
      targetPort: 10254
  selector:
    app.kubernetes.io/name: usage
    app.kubernetes.io/instance: release-name
---
# Source: controlplane/charts/flyte/templates/admin/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flyteadmin
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteadmin
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.0-b2
    #app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels: 
      app.kubernetes.io/name: flyteadmin
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        configChecksum: "fff146907a786d52f6ae2299bf90d599a86fb10edfd30d0e9d8b3f9ecffbea9"
        kubectl.kubernetes.io/default-container: flyteadmin
      labels: 
        app.kubernetes.io/name: flyteadmin
        app.kubernetes.io/instance: release-name
        helm.sh/chart: flyte-v1.16.0-b2
        #app.kubernetes.io/managed-by: Helm
    spec:
      securityContext: 
        fsGroup: 65534
        fsGroupChangePolicy: Always
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions:
          type: spc_t
      initContainers:
        - command:
          - flyteadmin
          - --config
          - /etc/flyte/config/*.yaml
          - migrate
          - run
          image: ":"
          imagePullPolicy: "IfNotPresent"
          name: run-migrations
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
          volumeMounts:
          - mountPath: /etc/db
            name: db-pass
          - mountPath: /etc/flyte/config
            name: base-config-volume
        - command:
          - flyteadmin
          - --config
          - /etc/flyte/config/*.yaml
          - migrate
          - seed-projects
          - union-health-monitoring
          - flytesnacks
          image: ":"
          imagePullPolicy: "IfNotPresent"
          name: seed-projects
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
          volumeMounts:
          - mountPath: /etc/db
            name: db-pass
          - mountPath: /etc/flyte/config
            name: base-config-volume
        - name: generate-secrets
          image: ":"
          imagePullPolicy: "IfNotPresent"
          command: ["/bin/sh", "-c"]
          args:
            [
                "flyteadmin --config=/etc/flyte/config/*.yaml secrets init --localPath /etc/scratch/secrets && flyteadmin --config=/etc/flyte/config/*.yaml secrets create --name flyte-admin-secrets --fromPath /etc/scratch/secrets",
            ]
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
          volumeMounts:
            - mountPath: /etc/flyte/config
              name: base-config-volume
            - mountPath: /etc/scratch
              name: scratch
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
      containers:
      - command:
        - flyteadmin
        - --config
        - /etc/flyte/config/*.yaml
        - serve
        image: ":"
        imagePullPolicy: "IfNotPresent"
        name: flyteadmin
        ports:
        - containerPort: 8088
        - containerPort: 8089
        - containerPort: 10254
        readinessProbe:
          httpGet:
            path: /healthcheck
            port: 8088
          initialDelaySeconds: 15
          timeoutSeconds: 1
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 3
        livenessProbe:
          httpGet:
            path: /healthcheck
            port: 8088
          initialDelaySeconds: 20
          timeoutSeconds: 1
          periodSeconds: 5
          successThreshold: 1
          failureThreshold: 3
        resources:
          limits:
            cpu: 250m
            ephemeral-storage: 100Mi
            memory: 500Mi
          requests:
            cpu: 10m
            ephemeral-storage: 50Mi
            memory: 50Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
        volumeMounts:
        - mountPath: /etc/db
          name: db-pass
        - mountPath: /srv/flyte
          name: shared-data
        - mountPath: /etc/flyte/config
          name: clusters-config-volume
        - mountPath: /etc/secrets/
          name: admin-secrets
      serviceAccountName: flyteadmin
      volumes:
      - name: db-pass
        secret:
          secretName: db-pass
      - emptyDir: {}
        name: shared-data
      - emptyDir: {}
        name: scratch
      - projected:
          sources:
            - configMap:
                name: flyte-admin-base-config
        name: base-config-volume
      - projected:
          sources:
            - configMap:
                name: flyte-admin-base-config
            - configMap:
                name: flyte-admin-clusters-config
        name: clusters-config-volume
      - name: admin-secrets
        secret:
          secretName: flyte-admin-secrets
---
# Source: controlplane/charts/flyte/templates/console/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flyteconsole
  namespace: union
  labels: 
    app.kubernetes.io/name: flyteconsole
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.0-b2
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels: 
      app.kubernetes.io/name: flyteconsole
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        configChecksum: "be7e4a0346ddffa3e0ccb101b1fd4bc1a08b6aeb40efa15558e8fd16ab9044b"
        linkerd.io/inject: disabled
        prometheus.io/scrape: "false"
      labels: 
        app.kubernetes.io/name: flyteconsole
        app.kubernetes.io/instance: release-name
        helm.sh/chart: flyte-v1.16.0-b2
        app.kubernetes.io/managed-by: Helm
    spec:
      securityContext: 
        fsGroupChangePolicy: OnRootMismatch
        runAsNonRoot: true
        runAsUser: 1000
        seLinuxOptions:
          type: spc_t
      containers:
      - image: ":"
        imagePullPolicy: "IfNotPresent"
        name: flyteconsole
        envFrom:
        - configMapRef:
            name: flyte-console-config
        ports:
        - containerPort: 8080
        env:
        - name: ENABLE_GA
          value: "true"
        - name: GA_TRACKING_ID
          value: ""
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: http://otel-collector.monitoring.svc.cluster.local:4318
        - name: OTEL_EXPORTER_OTLP_PROTOCOL
          value: http/protobuf
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
        resources: 
          limits:
            cpu: 250m
            ephemeral-storage: 200Mi
            memory: 250Mi
          requests:
            cpu: 10m
            ephemeral-storage: 20Mi
            memory: 50Mi
        volumeMounts:
        - mountPath: /srv/flyte
          name: shared-data
      volumes:
      - emptyDir: {}
        name: shared-data
---
# Source: controlplane/charts/flyte/templates/datacatalog/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: datacatalog
  namespace: union
  labels: 
    app.kubernetes.io/name: datacatalog
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.0-b2
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels: 
      app.kubernetes.io/name: datacatalog
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        configChecksum: "267d73b002b5d6c031cd696508fab7886026f1a71d609a9c8f448b2fc0cb709"
      labels: 
        app.kubernetes.io/name: datacatalog
        app.kubernetes.io/instance: release-name
        helm.sh/chart: flyte-v1.16.0-b2
        app.kubernetes.io/managed-by: Helm
    spec:
      securityContext: 
        fsGroup: 1001
        fsGroupChangePolicy: OnRootMismatch
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions:
          type: spc_t
      initContainers:
      - command:
        - datacatalog
        - --config
        - /etc/datacatalog/config/*.yaml
        - migrate
        - run
        image: ":"
        imagePullPolicy: ""
        name: run-migrations
        volumeMounts:
        - mountPath: /etc/db
          name: db-pass
        - mountPath: /etc/datacatalog/config
          name: config-volume
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
      containers:
      - command:
        - datacatalog
        - --config
        - /etc/datacatalog/config/*.yaml
        - serve
        image: ":"
        imagePullPolicy: ""
        name: datacatalog
        ports:
        - containerPort: 8080
        - containerPort: 8089
        - containerPort: 10254
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
        resources:
          limits:
            cpu: 500m
            ephemeral-storage: 100Mi
            memory: 500Mi
          requests:
            cpu: 10m
            ephemeral-storage: 50Mi
            memory: 50Mi
        volumeMounts:
        - mountPath: /etc/db
          name: db-pass
        - mountPath: /etc/datacatalog/config
          name: config-volume
      serviceAccountName: datacatalog
      volumes:
      - name: db-pass
        secret:
          secretName: db-pass
      - emptyDir: {}
        name: shared-data
      - projected:
          sources:
            - configMap:
                name: datacatalog-config
        name: config-volume
---
# Source: controlplane/charts/flyte/templates/flytescheduler/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flytescheduler
  namespace: union
  labels: 
    app.kubernetes.io/name: flytescheduler
    app.kubernetes.io/instance: release-name
    helm.sh/chart: flyte-v1.16.0-b2
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels: 
      app.kubernetes.io/name: flytescheduler
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        configChecksum: "fff146907a786d52f6ae2299bf90d599a86fb10edfd30d0e9d8b3f9ecffbea9"
      labels: 
        app.kubernetes.io/name: flytescheduler
        app.kubernetes.io/instance: release-name
        helm.sh/chart: flyte-v1.16.0-b2
        app.kubernetes.io/managed-by: Helm
    spec:
      securityContext: 
        fsGroup: 65534
        fsGroupChangePolicy: Always
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions:
          type: spc_t
      initContainers:
      - command:
        - flytescheduler
        - precheck
        - --config
        - /etc/flyte/config/*.yaml
        image: "cr.flyte.org/flyteorg/flytescheduler-release:v1.16.0-b2"
        imagePullPolicy: "IfNotPresent"
        name: flytescheduler-check
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
        volumeMounts:
        - mountPath: /etc/db
          name: db-pass
        - mountPath: /etc/flyte/config
          name: config-volume
        - name: auth
          mountPath: /etc/secrets/
      containers:
      - command:
        - flytescheduler
        - run
        - --config
        - /etc/flyte/config/*.yaml
        image: "cr.flyte.org/flyteorg/flytescheduler-release:v1.16.0-b2"
        imagePullPolicy: "IfNotPresent"
        name: flytescheduler
        ports:
          - containerPort: 10254
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
        resources:
          limits:
            cpu: 250m
            ephemeral-storage: 100Mi
            memory: 500Mi
          requests:
            cpu: 10m
            ephemeral-storage: 50Mi
            memory: 50Mi
        volumeMounts:
        - mountPath: /etc/db
          name: db-pass
        - mountPath: /etc/flyte/config
          name: config-volume
        - name: auth
          mountPath: /etc/secrets/
      serviceAccountName: flytescheduler
      volumes:
      - name: db-pass
        secret:
          secretName: db-pass
      - emptyDir: {}
        name: shared-data
      - configMap:
          name: flyte-scheduler-config
        name: config-volume
      - name: auth
        secret:
          secretName: flyte-secret-auth
---
# Source: controlplane/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: artifacts
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: artifacts
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: artifacts
      app.kubernetes.io/instance: release-name
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: artifacts
        linkerd.io/inject: disabled
        prometheus.io/path: /metrics
        prometheus.io/port: "10254"
      labels:
        app.kubernetes.io/name: artifacts
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: artifacts
      volumes:
      - name: secrets
        secret:
          secretName: artifacts
      - name: db-pass
        secret:
          secretName: db-pass
      - name: config
        configMap:
          name: artifacts
      initContainers:
        - name: artifacts-migrate
          image: ""  # empty string to avoid template error
          imagePullPolicy: IfNotPresent
          args:
          - artifacts
          - migrate
          - --config
          - /etc/config/*.yaml
          volumeMounts:
          - name: db-pass
            mountPath: /etc/db
          - name: secrets
            mountPath: /etc/secrets/
          - name: config
            mountPath: /etc/config/
      containers:
        - name: artifacts
          image: ""  # empty string to avoid template error
          imagePullPolicy: IfNotPresent
          args:
            - artifacts
            - serve
            - --config
            - /etc/config/*.yaml
          ports:
            - name: grpc
              containerPort: 8080
              protocol: TCP
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
          volumeMounts:
            - name: db-pass
              mountPath: /etc/db
            - name: secrets
              mountPath: /etc/secrets/
            - name: config
              mountPath: /etc/config/
          env:
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  resource: limits.memory
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  resource: limits.cpu
          resources:
              limits:
                cpu: 500m
                memory: 512Mi
              requests:
                cpu: 250m
                memory: 250Mi
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: artifacts
                    app.kubernetes.io/instance: release-name
                topologyKey: "kubernetes.io/hostname"
---
# Source: controlplane/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: authorizer
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: authorizer
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: authorizer
      app.kubernetes.io/instance: release-name
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: authorizer
        linkerd.io/inject: disabled
        prometheus.io/path: /metrics
        prometheus.io/port: "10254"
      labels:
        app.kubernetes.io/name: authorizer
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: authorizer
      volumes:
      - name: secrets
        secret:
          secretName: authorizer
      - name: db-pass
        secret:
          secretName: db-pass
      - name: config
        configMap:
          name: authorizer
      containers:
        - name: authorizer
          image: ""  # empty string to avoid template error
          imagePullPolicy: IfNotPresent
          args:
            - authorizer
            - serve
            - --config
            - /etc/config/*.yaml
          ports:
            - name: grpc
              containerPort: 8080
              protocol: TCP
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
          volumeMounts:
            - name: db-pass
              mountPath: /etc/db
            - name: secrets
              mountPath: /etc/secrets/
            - name: config
              mountPath: /etc/config/
          env:
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  resource: limits.memory
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  resource: limits.cpu
          resources:
              limits:
                cpu: 500m
                memory: 512Mi
              requests:
                cpu: 250m
                memory: 250Mi
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: authorizer
                    app.kubernetes.io/instance: release-name
                topologyKey: "kubernetes.io/hostname"
---
# Source: controlplane/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: cluster
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: cluster
      app.kubernetes.io/instance: release-name
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: cluster
        linkerd.io/inject: disabled
        prometheus.io/path: /metrics
        prometheus.io/port: "10254"
      labels:
        app.kubernetes.io/name: cluster
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: cluster
      volumes:
      - name: secrets
        secret:
          secretName: cluster
      - name: db-pass
        secret:
          secretName: db-pass
      - name: config
        configMap:
          name: cluster
      containers:
        - name: cluster
          image: ""  # empty string to avoid template error
          imagePullPolicy: IfNotPresent
          args:
            - cloudcluster
            - serve
            - --config
            - /etc/config/*.yaml
          ports:
            - name: grpc
              containerPort: 8080
              protocol: TCP
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
          volumeMounts:
            - name: db-pass
              mountPath: /etc/db
            - name: secrets
              mountPath: /etc/secrets/
            - name: config
              mountPath: /etc/config/
          env:
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  resource: limits.memory
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  resource: limits.cpu
          resources:
              limits:
                cpu: 500m
                memory: 512Mi
              requests:
                cpu: 250m
                memory: 250Mi
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: cluster
                    app.kubernetes.io/instance: release-name
                topologyKey: "kubernetes.io/hostname"
---
# Source: controlplane/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dataproxy
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: dataproxy
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: dataproxy
      app.kubernetes.io/instance: release-name
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: dataproxy
        linkerd.io/inject: disabled
        prometheus.io/path: /metrics
        prometheus.io/port: "10254"
      labels:
        app.kubernetes.io/name: dataproxy
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: dataproxy
      volumes:
      - name: secrets
        secret:
          secretName: dataproxy
      - name: db-pass
        secret:
          secretName: db-pass
      - name: config
        configMap:
          name: dataproxy
      containers:
        - name: dataproxy
          image: ""  # empty string to avoid template error
          imagePullPolicy: IfNotPresent
          args:
            - dataproxy
            - serve
            - --config
            - /etc/config/*.yaml
          ports:
            - name: grpc
              containerPort: 8080
              protocol: TCP
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
          volumeMounts:
            - name: db-pass
              mountPath: /etc/db
            - name: secrets
              mountPath: /etc/secrets/
            - name: config
              mountPath: /etc/config/
          env:
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  resource: limits.memory
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  resource: limits.cpu
          resources:
              limits:
                cpu: 500m
                memory: 512Mi
              requests:
                cpu: 250m
                memory: 250Mi
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: dataproxy
                    app.kubernetes.io/instance: release-name
                topologyKey: "kubernetes.io/hostname"
---
# Source: controlplane/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: executions
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: executions
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: executions
      app.kubernetes.io/instance: release-name
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: executions
        linkerd.io/inject: disabled
        prometheus.io/path: /metrics
        prometheus.io/port: "10254"
      labels:
        app.kubernetes.io/name: executions
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: executions
      volumes:
      - name: secrets
        secret:
          secretName: executions
      - name: db-pass
        secret:
          secretName: db-pass
      - name: config
        configMap:
          name: executions
      containers:
        - name: executions
          image: ""  # empty string to avoid template error
          imagePullPolicy: IfNotPresent
          args:
            - cloudpropeller
            - serve
            - --config
            - /etc/config/*.yaml
          ports:
            - name: grpc
              containerPort: 8080
              protocol: TCP
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
          volumeMounts:
            - name: db-pass
              mountPath: /etc/db
            - name: secrets
              mountPath: /etc/secrets/
            - name: config
              mountPath: /etc/config/
          env:
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  resource: limits.memory
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  resource: limits.cpu
          resources:
              limits:
                cpu: 500m
                memory: 512Mi
              requests:
                cpu: 250m
                memory: 250Mi
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: executions
                    app.kubernetes.io/instance: release-name
                topologyKey: "kubernetes.io/hostname"
---
# Source: controlplane/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: usage
  labels:
    helm.sh/chart: controlplane-2025.5.6
    app.kubernetes.io/name: usage
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2025.5.6"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: usage
      app.kubernetes.io/instance: release-name
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: usage
        linkerd.io/inject: disabled
        prometheus.io/path: /metrics
        prometheus.io/port: "10254"
      labels:
        app.kubernetes.io/name: usage
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: usage
      volumes:
      - name: secrets
        secret:
          secretName: usage
      - name: db-pass
        secret:
          secretName: db-pass
      - name: config
        configMap:
          name: usage
      containers:
        - name: usage
          image: ""  # empty string to avoid template error
          imagePullPolicy: IfNotPresent
          args:
            - usage
            - serve
            - --config
            - /etc/config/*.yaml
          ports:
            - name: grpc
              containerPort: 8080
              protocol: TCP
            - name: http
              containerPort: 8089
              protocol: TCP
            - name: debug
              containerPort: 10254
              protocol: TCP
          volumeMounts:
            - name: db-pass
              mountPath: /etc/db
            - name: secrets
              mountPath: /etc/secrets/
            - name: config
              mountPath: /etc/config/
          env:
            - name: GOMEMLIMIT
              valueFrom:
                resourceFieldRef:
                  resource: limits.memory
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  resource: limits.cpu
          resources:
              limits:
                cpu: 500m
                memory: 512Mi
              requests:
                cpu: 250m
                memory: 250Mi
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: debug
            initialDelaySeconds: 3
            periodSeconds: 3
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: usage
                    app.kubernetes.io/instance: release-name
                topologyKey: "kubernetes.io/hostname"
---
# Source: controlplane/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: artifacts
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: artifacts
  minReplicas: 1
  maxReplicas: 1
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
---
# Source: controlplane/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: authorizer
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: authorizer
  minReplicas: 1
  maxReplicas: 1
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
---
# Source: controlplane/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: cluster
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: cluster
  minReplicas: 1
  maxReplicas: 1
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
---
# Source: controlplane/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: dataproxy
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: dataproxy
  minReplicas: 1
  maxReplicas: 1
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
---
# Source: controlplane/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: executions
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: executions
  minReplicas: 1
  maxReplicas: 1
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
---
# Source: controlplane/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: usage
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: usage
  minReplicas: 1
  maxReplicas: 1
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
---
# Source: controlplane/charts/flyte/templates/common/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: flyte
  namespace: union
  annotations: 
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/app-root: /console
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
spec:
  ingressClassName: 
  rules:
    - http:
        paths:
          # This is useful only for frontend development
          # NOTE: If you change this, you must update the BASE_URL value in flyteconsole.yaml
          - path: /console
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /console/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /api
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /api/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /healthcheck
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /v1/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /.well-known
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /.well-known/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /login
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /login/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /logout
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /logout/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /callback
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /callback/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /me
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /config
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /config/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /oauth2
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /oauth2/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
      host: fake-host.domain
  tls:
    - secretName: fake-host-tls-secret
      hosts:
        - fake-host.domain
  
# Certain ingress controllers like nginx cannot serve HTTP 1 and GRPC with a single ingress because GRPC can only
# enabled on the ingress object, not on backend services (GRPC annotation is set on the ingress, not on the services).
---
# Source: controlplane/charts/flyte/templates/common/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: flyte-grpc
  namespace: union
  annotations: 
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/app-root: /console
    nginx.ingress.kubernetes.io/backend-protocol: GRPC
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
spec:
  ingressClassName: 
  rules:
    - host: fake-host.domain
      http:
        paths:
          #
          # NOTE: Port 81 in flyteadmin is the GRPC server port for FlyteAdmin.
          - path: /grpc.health.v1.Health
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /grpc.health.v1.Health/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /flyteidl.service.AuthMetadataService
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /flyteidl.service.AuthMetadataService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
  tls:
    - secretName: fake-host-tls-secret
      hosts:
        - fake-host.domain
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane-dataproxy
  namespace: union
  annotations: 
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/app-root: /console
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.org/websocket-services: dataproxy-service
spec:
  tls:
  - hosts:
    - fake-host.domain
    secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          - path: /data/*
            pathType: Prefix
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /data
            pathType: Prefix
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane-usage-grpc
  namespace: union
  annotations: 
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/app-root: /console
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/backend-protocol: GRPC
spec:
  tls:
  - hosts:
    - fake-host.domain
    secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          
          - path: /cloudidl.usage.UsageService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: usage
                port:
                  number: 80
          - path: /cloudidl.usage.UsageService
            pathType: ImplementationSpecific
            backend:
              service:
                name: usage
                port:
                  number: 80
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane-usage
  namespace: union
  annotations: 
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/app-root: /console
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.org/websocket-services: dataproxy-service
spec:
  tls:
  - hosts:
    - fake-host.domain
    secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          
          - path: /usage/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: usage
                port:
                  number: 81
          - path: /usage
            pathType: ImplementationSpecific
            backend:
              service:
                name: usage
                port:
                  number: 81
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane-protected
  namespace: union
  annotations: 
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/app-root: /console
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.org/websocket-services: dataproxy-service
spec:
  tls:
  - hosts:
    - fake-host.domain
    secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          
          - path: /api
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /api/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /v1/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /cloudadmin
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /cloudadmin/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /actor
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /actor/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /agent
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /agent/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /dataplane
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /dataplane/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /spark-history-server
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /spark-history-server/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /api/v1/dataproxy
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /api/v1/dataproxy/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 81
          - path: /app
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 81
          - path: /app/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 81
          - path: /apps
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 81
          - path: /apps/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 81
          - path: /cluster
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 81
          - path: /cluster/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 81
          - path: /clusterpool
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 81
          - path: /clusterpool/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 81
          - path: /clusterconfig
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 81
          - path: /clusterconfig/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 81
          - path: /org
            pathType: ImplementationSpecific
            backend:
              service:
                name: organizations
                port:
                  number: 81
          - path: /org/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: organizations
                port:
                  number: 81
          - path: /managed_cluster
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 81
          - path: /managed_cluster/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 81
          - path: /authorizer
            pathType: ImplementationSpecific
            backend:
              service:
                name: authorizer
                port:
                  number: 81
          - path: /authorizer/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: authorizer
                port:
                  number: 81
          - path: /oauth_app
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /oauth_app/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /users
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /users/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /roles
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /roles/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /policies
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /policies/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /identities
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /identities/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 81
          - path: /echo
            pathType: ImplementationSpecific
            backend:
              service:
                name: execution
                port:
                  number: 81
          - path: /echo/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: execution
                port:
                  number: 81
          - path: /execution
            pathType: ImplementationSpecific
            backend:
              service:
                name: execution
                port:
                  number: 81
          - path: /execution/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: execution
                port:
                  number: 81
          - path: /workspace_registry
            pathType: ImplementationSpecific
            backend:
              service:
                name: execution
                port:
                  number: 81
          - path: /workspace_registry/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: execution
                port:
                  number: 81
          - path: /workspace_instance
            pathType: ImplementationSpecific
            backend:
              service:
                name: execution
                port:
                  number: 81
          - path: /workspace_instance/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: execution
                port:
                  number: 81
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane-protected-grpc
  namespace: union
  annotations: 
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/app-root: /console
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/backend-protocol: GRPC
spec:
  tls:
  - hosts:
    - fake-host.domain
    secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          - path: /cloudidl.execution.ExecutionService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.execution.ExecutionService
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.cluster.ClusterService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.cluster.ClusterService
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.apikey.APIKeyService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 83
          - path: /cloudidl.apikey.APIKeyService
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 83
          - path: /cloudidl.identity.AppsService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.AppsService
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.org.OrgService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: organizations
                port:
                  number: 80
          - path: /cloudidl.org.OrgService
            pathType: ImplementationSpecific
            backend:
              service:
                name: organizations
                port:
                  number: 80
          - path: /cloudidl.cloudaccounts.CloudAccountsService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.cloudaccounts.CloudAccountsService
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.cluster.ManagedClusterService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.cluster.ManagedClusterService
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.identity.UserService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.UserService
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.RoleService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.RoleService
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.PolicyService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.PolicyService
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.SelfServe/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.SelfServe
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.IdentityService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.identity.IdentityService
            pathType: ImplementationSpecific
            backend:
              service:
                name: identity
                port:
                  number: 80
          - path: /cloudidl.clusterpool.ClusterPoolService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.clusterpool.ClusterPoolService
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.clusterconfig.ClusterConfigService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.clusterconfig.ClusterConfigService
            pathType: ImplementationSpecific
            backend:
              service:
                name: cluster
                port:
                  number: 80
          - path: /cloudidl.authorizer.AuthorizerService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: authorizer
                port:
                  number: 80
          - path: /cloudidl.authorizer.AuthorizerService
            pathType: ImplementationSpecific
            backend:
              service:
                name: authorizer
                port:
                  number: 80
          - path: /datacatalog.DataCatalog/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: datacatalog
                port:
                  number: 89
          - path: /datacatalog.DataCatalog
            pathType: ImplementationSpecific
            backend:
              service:
                name: datacatalog
                port:
                  number: 89
          - path: /flyteidl.cacheservice.CacheService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: cacheservice
                port:
                  number: 89
          - path: /flyteidl.cacheservice.CacheService
            pathType: ImplementationSpecific
            backend:
              service:
                name: cacheservice
                port:
                  number: 89
          - path: /cloudidl.actor.ActorEnvironmentService
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.actor.ActorEnvironmentService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.agent.AgentService
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.agent.AgentService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.secret.SecretService
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.secret.SecretService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.clouddataproxy.CloudDataProxyService
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.clouddataproxy.CloudDataProxyService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /flyteidl.service.DataProxyService
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /flyteidl.service.DataProxyService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.logs.LogsService
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.logs.LogsService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.workspace.WorkspaceRegistryService
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workspace.WorkspaceRegistryService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workspace.WorkspaceInstanceService
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.workspace.WorkspaceInstanceService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.imagebuilder.ImageService
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.imagebuilder.ImageService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.app.AppService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.app.AppLogsService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.app.ReplicaService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane-protected-grpc-streaming
  namespace: union
  annotations: 
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/app-root: /console
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/backend-protocol: GRPC
spec:
  tls:
  - hosts:
    - fake-host.domain
    secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          - path: /flyteidl.service.AdminService
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /flyteidl.service.AdminService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          
          - path: /flyteidl.service.WatchService
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          
          - path: /flyteidl.service.WatchService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /cloudidl.cloudadmin.CloudAdminService
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /cloudidl.cloudadmin.CloudAdminService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /flyteidl.service.IdentityService
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /flyteidl.service.IdentityService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /cloudidl.echo.EchoService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.echo.EchoService
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /flyteidl.service.SignalService
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /flyteidl.service.SignalService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /cloudidl.actor.ActorEnvironmentService/Stream*
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.execution.ExecutionService/GetExecutionOperation
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.logs.LogsService/TailTaskExecutionLogs
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.workspace.WorkspaceInstanceService/WatchWorkspaceInstances
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.app.AppService/Watch
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.app.AppService/Lease
            pathType: ImplementationSpecific
            backend:
              service:
                name: executions
                port:
                  number: 80
          - path: /cloudidl.app.AppLogsService/TailLogs
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
          - path: /cloudidl.app.ReplicaService/WatchReplicas
            pathType: ImplementationSpecific
            backend:
              service:
                name: dataproxy
                port:
                  number: 80
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane
  namespace: union
  annotations: 
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/app-root: /console
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
spec:
  tls:
  - hosts:
    - fake-host.domain
    secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          
          # Port 87 in FlyteAdmin maps to the redoc container.
          - path: /openapi
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 87
          - path: /healthcheck
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /me
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          # Port 87 in FlyteAdmin maps to the redoc container.
          - path: /openapi/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 87
          - path: /.well-known
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /.well-known/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /login
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /login/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /logout
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /logout/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /callback
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /callback/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /config
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /config/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /oauth2
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /oauth2/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /auth
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /auth/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 80
          - path: /enqueue_metronome_request/v1
            pathType: ImplementationSpecific
            backend:
              service:
                name: usage
                port:
                  number: 81
          - path: /enqueue_metronome_request/v1/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: usage
                port:
                  number: 81
          - path: /enqueue_stripe_request/v1
            pathType: ImplementationSpecific
            backend:
              service:
                name: usage
                port:
                  number: 81
          - path: /enqueue_stripe_request/v1/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: usage
                port:
                  number: 81
---
# Source: controlplane/templates/flyte-core-app.yaml
# Certain ingress controllers like nginx cannot serve HTTP 1 and GRPC with a single ingress because GRPC can only
# enabled on the ingress object, not on backend services (GRPC annotation is set on the ingress, not on the services).
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane-grpc
  namespace: union
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/app-root: /console
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/backend-protocol: GRPC
spec:
  tls:
  - hosts:
    - fake-host.domain
    secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          # NOTE: Port 81 in flyteadmin is the GRPC server port for FlyteAdmin.
          - path: /grpc.health.v1.Health
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /grpc.health.v1.Health/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /flyteidl.service.AuthMetadataService
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
          - path: /flyteidl.service.AuthMetadataService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane-grpc-streaming
  namespace: union
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/app-root: /console
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/backend-protocol: GRPC
spec:
  tls:
  - hosts:
    - fake-host.domain
    secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          - path: /flyteidl.service.WatchService/WatchExecutionStatusUpdates
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteadmin
                port:
                  number: 81
---
# Source: controlplane/templates/flyte-core-app.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: controlplane-console-protected
  namespace: union
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/app-root: /console
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: 6m
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-cookie-domain: ~^ .$host
    nginx.ingress.kubernetes.io/server-snippet: |
      client_header_timeout 604800;
      client_body_timeout 604800;
      # Increasing the default configuration from
      #        client_header_buffer_size       1k;
      #        large_client_header_buffers     4 8k;
      # to default of 16k and 32k for large buffer sizes. These sizes are chosen as a short term mediation until we can collect data to reason
      # about expected header sizs (PE-1101).
      # Historically, we have seen is with the previous 8k max buffer size , the auth endpoint of /me would throw 400 Bad request and due to this ingress controller
      # threw a 500 as it doesn't expect this status code on auth request expected range :  200 <= authcall.status(i.e status of /me call) <=300
      # Code link for ref : https://github.com/nginx/nginx/blob/e734df6664e70f118ca3140bcef6d4f1750fa8fa/src/http/modules/ngx_http_auth_request_module.c#L170-L179
      # Now the main reason we have seen 400 bad request is large size of the cookies which contribute to the header size.
      # We should keep reducing the size of what headers are being sent meanwhile we increase this size to mitigate the long header issue.
      client_header_buffer_size 16k;
      large_client_header_buffers 64 32k;
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/auth-cache-key: $http_flyte_authorization$http_cookie
    nginx.org/websocket-services: dataproxy-service
spec:
  tls:
  - hosts:
    - fake-host.domain
    secretName: fake-host-tls-secret
  rules:
    - host: fake-host.domain
      http:
        paths:
          
          - path: /
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          # NOTE: If you change this, you must update the BASE_URL value in flyteconsole.yaml
          - path: /console
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /console/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /dashboard
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /dashboard/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /resources
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /resources/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /cost
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /cost/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /loading
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
          - path: /loading/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: flyteconsole
                port:
                  number: 80
