# helm-values: values-test-certs.yaml
# Intended to match and test charts/dataplane/values.aws.eks-automode.yaml
global:
  UNION_CONTROL_PLANE_HOST: "test-controlplane-host"
  CLUSTER_NAME: "test-cluster-name"
  ORG_NAME: "test-org-name"
  CLIENT_ID: "test-client-id"
  METADATA_BUCKET: "test-metadata-bucket"
  FAST_REGISTRATION_BUCKET: "test-fast-registration-bucket"
  AWS_REGION: "test-region"
  BACKEND_IAM_ROLE_ARN: "test-backend-iam-role-arn"
  WORKER_IAM_ROLE_ARN: "test-worker-iam-role-arn"

provider: aws

# ----------------------------------------------------------------------------
# SECTION 3: Object Storage Configuration (REQUIRED)
# ----------------------------------------------------------------------------
# Configure S3 buckets for workflow metadata and fast registration.
# These buckets must be created before deployment.

storage:
  provider: aws
  authType: iam  # Use IAM roles for pods (IRSA recommended)

  # AWS region for S3 buckets
  region: '{{ .Values.global.AWS_REGION }}'
  enableMultiContainer: true

# ----------------------------------------------------------------------------
# SECTION 4: IAM Roles (REQUIRED for AWS)
# ----------------------------------------------------------------------------
# Configure IRSA (IAM Roles for Service Accounts) for secure AWS access.
# These IAM roles must be created with appropriate trust policies and permissions.

# IAM role for Union backend services (operator, propeller, etc.)
additionalServiceAccountAnnotations:
  eks.amazonaws.com/role-arn: "{{ tpl .Values.global.BACKEND_IAM_ROLE_ARN . }}"

# IAM role for workflow execution pods
userRoleAnnotationKey: eks.amazonaws.com/role-arn
userRoleAnnotationValue: "{{ tpl .Values.global.WORKER_IAM_ROLE_ARN . }}"

# ----------------------------------------------------------------------------
# SECTION 5: Authentication Secrets (REQUIRED)
# ----------------------------------------------------------------------------
# Client credentials for authenticating with the Union control plane.
# These will be provided by the Union team for managed control planes.

secrets:
  admin:
    # Client ID for dataplane authentication
    clientId: '{{ .Values.global.CLIENT_ID }}'

    # Client secret for dataplane authentication (keep this secure!)
    clientSecret: "test-not-real-secret"

    # Whether to create the Kubernetes secret
    # Set to false if you manage secrets externally (e.g., via External Secrets Operator)
    create: true

# ----------------------------------------------------------------------------
# SECTION 6: Logging Configuration (REQUIRED)
# ----------------------------------------------------------------------------
# FluentBit collects and forwards logs to the control plane.

fluentbit:
  serviceAccount:
    name: fluentbit-system
    # Annotations may be required for IRSA
    annotations: {}

# ----------------------------------------------------------------------------
# SECTION 7: Task Level Monitoring
# ----------------------------------------------------------------------------

# DCGM Exporter
dcgm-exporter:
  enabled: true

  # Configure AWS specific affinity
  # Reference: http://github.com/aws-observability/helm-charts/blob/main/charts/amazon-cloudwatch-observability/values.yaml#L1413-L1545
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                  ## NVIDIA GPU instance types
                  - g3.4xlarge
                  - g3.8xlarge
                  - g3.16xlarge
                  - g3s.xlarge
                  - g4ad.2xlarge
                  - g4ad.4xlarge
                  - g4ad.8xlarge
                  - g4ad.16xlarge
                  - g4ad.xlarge
                  - g4dn.2xlarge
                  - g4dn.4xlarge
                  - g4dn.8xlarge
                  - g4dn.12xlarge
                  - g4dn.16xlarge
                  - g4dn.metal
                  - g4dn.xlarge
                  - g5.2xlarge
                  - g5.4xlarge
                  - g5.8xlarge
                  - g5.12xlarge
                  - g5.16xlarge
                  - g5.24xlarge
                  - g5.48xlarge
                  - g5.xlarge
                  - g5g.2xlarge
                  - g5g.4xlarge
                  - g5g.8xlarge
                  - g5g.16xlarge
                  - g5g.metal
                  - g5g.xlarge
                  - g6.2xlarge
                  - g6.4xlarge
                  - g6.8xlarge
                  - g6.12xlarge
                  - g6.16xlarge
                  - g6.24xlarge
                  - g6.48xlarge
                  - g6.xlarge
                  - g6e.2xlarge
                  - g6e.4xlarge
                  - g6e.8xlarge
                  - g6e.12xlarge
                  - g6e.16xlarge
                  - g6e.24xlarge
                  - g6e.48xlarge
                  - g6e.xlarge
                  - gr6.4xlarge
                  - gr6.8xlarge
                  - p2.8xlarge
                  - p2.16xlarge
                  - p2.xlarge
                  - p3.2xlarge
                  - p3.8xlarge
                  - p3.16xlarge
                  - p3dn.24xlarge
                  - p4d.24xlarge
                  - p4de.24xlarge
                  - p5.48xlarge
                  - p5e.48xlarge
                  - p5en.48xlarge
                  - ml.g3.4xlarge
                  - ml.g3.8xlarge
                  - ml.g3.16xlarge
                  - ml.g3s.xlarge
                  - ml.g4ad.2xlarge
                  - ml.g4ad.4xlarge
                  - ml.g4ad.8xlarge
                  - ml.g4ad.16xlarge
                  - ml.g4ad.xlarge
                  - ml.g4dn.2xlarge
                  - ml.g4dn.4xlarge
                  - ml.g4dn.8xlarge
                  - ml.g4dn.12xlarge
                  - ml.g4dn.16xlarge
                  - ml.g4dn.metal
                  - ml.g4dn.xlarge
                  - ml.g5.2xlarge
                  - ml.g5.4xlarge
                  - ml.g5.8xlarge
                  - ml.g5.12xlarge
                  - ml.g5.16xlarge
                  - ml.g5.24xlarge
                  - ml.g5.48xlarge
                  - ml.g5.xlarge
                  - ml.g5g.2xlarge
                  - ml.g5g.4xlarge
                  - ml.g5g.8xlarge
                  - ml.g5g.16xlarge
                  - ml.g5g.metal
                  - ml.g5g.xlarge
                  - ml.g6.2xlarge
                  - ml.g6.4xlarge
                  - ml.g6.8xlarge
                  - ml.g6.12xlarge
                  - ml.g6.16xlarge
                  - ml.g6.24xlarge
                  - ml.g6.48xlarge
                  - ml.g6.xlarge
                  - ml.g6e.2xlarge
                  - ml.g6e.4xlarge
                  - ml.g6e.8xlarge
                  - ml.g6e.12xlarge
                  - ml.g6e.16xlarge
                  - ml.g6e.24xlarge
                  - ml.g6e.48xlarge
                  - ml.g6e.xlarge
                  - ml.gr6.4xlarge
                  - ml.gr6.8xlarge
                  - ml.p2.8xlarge
                  - ml.p2.16xlarge
                  - ml.p2.xlarge
                  - ml.p3.2xlarge
                  - ml.p3.8xlarge
                  - ml.p3.16xlarge
                  - ml.p3dn.24xlarge
                  - ml.p4d.24xlarge
                  - ml.p4de.24xlarge
                  - ml.p5.48xlarge
                  - ml.p5e.48xlarge
                  - ml.p5en.48xlarge
              - key: eks.amazonaws.com/compute-type
                operator: NotIn
                values:
                  - fargate

  # -- It's common practice to taint accelerator nodes to ensure non accelerator workloads
  #
  #    tolerations to ensure it only runs on GPU nodes.
  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule

# ----------------------------------------------------------------------------
# SECTION 8: Union/Flyte Kubernetes specific Configuration
# ----------------------------------------------------------------------------

config:
  k8s:
    plugins:
      k8s:
        # AWS EKS Auto Mode Accelerator Configuration

        # This configuration maps specific GPU types to EKS Auto Mode compatible
        # eks.amazonaws.com/instance-gpu-name.
        #
        # flyte-sdk hardcodess certain common NVIDIA GPU types to specific names. This mapping is use
        # https://github.com/flyteorg/flyte-sdk/blob/f85f2f2733dcb6cf235bad0b34d4aa0482a5835d/src/flyte/_internal/runtime/resources_serde.py#L7-L25https://github.com/flyteorg/flyte-sdk/blob/4a440fd468044b85e26141ca9c73ff91eb2ff04c/src/flyte/_internal/runtime/resources_serde.py#L7-L25
        # The following block unmaps these GPU types to the corresponding EKS Auto Mode compatible names.
        accelerator-devices:
          # Flyte uppercases the GPU type names, so we need to map them to the lowercase names used by EKS Auto Mode.
          - NVIDIA-TESLA-T4: t4
          - NVIDIA-TESLA-V100: v100
          - NVIDIA-TESLA-A100: a100
          - NVIDIA-A10G: a10g
          - NVIDIA-TESLA-K80: k80
          - NVIDIA-H100: h100
          - NVIDIA-L4: l4
          - NVIDIA-L40S: l40s
        accelerator-device-classes:
          - NVIDIA_GPU:
              resource-name: nvidia.com/gpu
              device-node-label: eks.amazonaws.com/instance-gpu-name
              pod-template:
                template:
                  spec:
                    affinity:
                      nodeAffinity:
                        requiredDuringSchedulingIgnoredDuringExecution:
                          nodeSelectorTerms:
                            - matchExpressions:
                                - key: eks.amazonaws.com/instance-gpu-manufacturer
                                  operator: In
                                  values:
                                    - nvidia

# ----------------------------------------------------------------------------
# SECTION 9: Optional Config
# ----------------------------------------------------------------------------

# AWS EKS Auto Mode allows creating Nodepools through Kubernetes manifests.
# This section shows an example of creating a simple GPU accelerated nodepool.
#
extraObjects:
  # Example of a T4 specific node pool for GPU workloads. Adjust the instance types and labels as needed.
  - apiVersion: karpenter.sh/v1
    kind: NodePool
    metadata:
      name: gpu-t4
    spec:
      template:
        metadata:
          labels:
            # It's best practice to label nvidia accelerated nodepools to assist
            # Nvidia specific daemonsets targeting.
            nvidia.com/gpu: "true"
        spec:
          # Reference to the default eks.amazonaws.com NodeClass
          nodeClassRef:
            group: eks.amazonaws.com
            kind: NodeClass
            name: default

          # Ensure pods that don't require nvidia GPUs are not scheduled on these nodes by tainting them.
          taints:
            - key: nvidia.com/gpu
              value: "true"
              effect: NoSchedule

          # Instance requirements for g4dn family (NVIDIA T4 GPUs)
          # EKS Auto Mode automatically handles driver installation and readiness
          requirements:
            # Instance family - g4dn uses NVIDIA T4 GPUs
            - key: eks.amazonaws.com/instance-family
              operator: In
              values:
                - g4dn

            # Use on-demand instances for GPU reliability
            # Comment this out if you want to allow spot instances
            - key: karpenter.sh/capacity-type
              operator: In
              values:
                - on-demand

            - key: kubernetes.io/arch
              operator: In
              values:
                - amd64

            # Ensure nodes have GPUs
            - key: eks.amazonaws.com/instance-gpu-count
              operator: Gt
              values:
                - "0"

            # Ensure GPU type is T4
            - key: eks.amazonaws.com/instance-gpu-name
              operator: In
              values:
                - t4

      # Resource limits for this node pool
      # Adjust based on your budget and workload needs
      limits:
        cpu: "1000"           # Max total CPUs across all nodes in this pool
        memory: 4000Gi        # Max total memory across all nodes
        nvidia.com/gpu: "50"  # Max total GPUs (adjust as needed)

      # Disruption settings for node lifecycle management
      # Ref: https://karpenter.sh/docs/concepts/disruption/
      disruption:
        # Assume this nodepool is targetted exclusively for Union ephemeral workloads.
        # This means that we can assume that there will be no long-running workloads
        # on these nodes, and we can safely evict all pods when scaling down.
        consolidationPolicy: WhenEmpty

        # Wait 10 minutes to see if a new run will schedule before consolidating the node.
        consolidateAfter: 10m

        # Allow gradual rollouts and consolidation
        budgets:
          # Allow or more aggressive disruption budget for nodepools running Union workloads.
          # Our consolidation policy is "WhenEmpty", so we can allow up to 50% of the nodes
          # to be disrupted at a time.
          - nodes: "50%"
            reasons:
              - Underutilized
              - Empty
          # Don't allow any disruption if the node is not empty to avoid evicting non-Union workloads.
          # Assume each Union workload is short-lived and can be safely evicted when scaling down,
          # so we can allow 100% disruption for empty nodes.
          - nodes: "0"
            reasons:
              - Drifted

      # Priority weight (lower = less preferred)
      # GPU nodes are more expensive, so prefer general-purpose nodes first
      weight: 10
